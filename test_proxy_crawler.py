#!/usr/bin/env python3
"""ä»£ç†çˆ¬èŸ²ç³»çµ±æ¸¬è©¦è…³æœ¬

æ­¤è…³æœ¬ç”¨æ–¼æ¸¬è©¦ä»£ç†çˆ¬èŸ²ç³»çµ±çš„å„å€‹çµ„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
- å–®å€‹çˆ¬èŸ²æ¸¬è©¦
- çˆ¬èŸ²ç®¡ç†å™¨æ¸¬è©¦
- ä»£ç†é©—è­‰æ¸¬è©¦
- è¼¸å‡ºæ ¼å¼æ¸¬è©¦

ä½¿ç”¨æ–¹æ³•:
    python test_proxy_crawler.py
    python test_proxy_crawler.py --quick  # å¿«é€Ÿæ¸¬è©¦ï¼Œä¸é©—è­‰ä»£ç†
    python test_proxy_crawler.py --source sslproxies  # æ¸¬è©¦å–®å€‹æº
"""

import asyncio
import argparse
import sys
from pathlib import Path
from typing import Optional, List

# æ·»åŠ srcç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent / "src"))

from loguru import logger
from proxy_manager import (
    CrawlerManager, 
    SSLProxiesCrawler, 
    GeonodeCrawler, 
    FreeProxyListCrawler,
    ProxyValidator
)


def setup_logging(verbose: bool = False):
    """è¨­ç½®æ—¥èªŒè¨˜éŒ„"""
    logger.remove()
    
    if verbose:
        logger.add(
            sys.stderr,
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            level="DEBUG"
        )
    else:
        logger.add(
            sys.stderr,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
            level="INFO"
        )


async def test_single_crawler(crawler_name: str, limit: int = 10):
    """æ¸¬è©¦å–®å€‹çˆ¬èŸ²
    
    Args:
        crawler_name: çˆ¬èŸ²åç¨±
        limit: é™åˆ¶çˆ¬å–æ•¸é‡
    """
    logger.info(f"\n=== æ¸¬è©¦ {crawler_name} çˆ¬èŸ² ===")
    
    crawler_classes = {
        'sslproxies': SSLProxiesCrawler,
        'geonode': GeonodeCrawler,
        'free_proxy_list': FreeProxyListCrawler
    }
    
    if crawler_name not in crawler_classes:
        logger.error(f"æœªçŸ¥çš„çˆ¬èŸ²: {crawler_name}")
        return False
    
    try:
        crawler_class = crawler_classes[crawler_name]
        async with crawler_class() as crawler:
            logger.info(f"é–‹å§‹çˆ¬å– {crawler_name}...")
            proxies = await crawler.crawl()
            
            if proxies:
                logger.info(f"æˆåŠŸçˆ¬å– {len(proxies)} å€‹ä»£ç†")
                
                # é¡¯ç¤ºå‰å¹¾å€‹ä»£ç†ä½œç‚ºç¤ºä¾‹
                show_count = min(limit, len(proxies))
                logger.info(f"å‰ {show_count} å€‹ä»£ç†:")
                
                for i, proxy in enumerate(proxies[:show_count]):
                    logger.info(f"  {i+1}. {proxy.ip}:{proxy.port} ({proxy.protocol}) - {proxy.country}")
                
                return True
            else:
                logger.warning(f"æœªèƒ½å¾ {crawler_name} ç²å–åˆ°ä»£ç†")
                return False
                
    except Exception as e:
        logger.error(f"æ¸¬è©¦ {crawler_name} çˆ¬èŸ²æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return False


async def test_proxy_validator(proxies_to_test: int = 5):
    """æ¸¬è©¦ä»£ç†é©—è­‰å™¨
    
    Args:
        proxies_to_test: è¦æ¸¬è©¦çš„ä»£ç†æ•¸é‡
    """
    logger.info(f"\n=== æ¸¬è©¦ä»£ç†é©—è­‰å™¨ ===")
    
    try:
        # å…ˆç²å–ä¸€äº›ä»£ç†é€²è¡Œæ¸¬è©¦
        async with SSLProxiesCrawler() as crawler:
            proxies = await crawler.crawl()
            
        if not proxies:
            logger.warning("æ²’æœ‰ä»£ç†å¯ä¾›æ¸¬è©¦é©—è­‰å™¨")
            return False
        
        # å–å‰å¹¾å€‹ä»£ç†é€²è¡Œé©—è­‰
        test_proxies = proxies[:proxies_to_test]
        logger.info(f"å°‡é©—è­‰ {len(test_proxies)} å€‹ä»£ç†...")
        
        validator = ProxyValidator(timeout=5.0, max_concurrent=10)
        results = await validator.validate_proxies(
            test_proxies, 
            test_anonymity=True, 
            test_geo=False
        )
        
        working_proxies = validator.get_working_proxies(results)
        
        logger.info(f"é©—è­‰å®Œæˆ: {len(working_proxies)}/{len(test_proxies)} å€‹ä»£ç†å¯ç”¨")
        
        # é¡¯ç¤ºé©—è­‰çµæœ
        for result in results:
            status_icon = "âœ…" if result.status.value == "working" else "âŒ"
            response_time = f"{result.response_time:.2f}s" if result.response_time else "N/A"
            logger.info(
                f"  {status_icon} {result.proxy.ip}:{result.proxy.port} - "
                f"éŸ¿æ‡‰æ™‚é–“: {response_time}, åŒ¿åç­‰ç´š: {result.anonymity_level.value}"
            )
        
        return len(working_proxies) > 0
        
    except Exception as e:
        logger.error(f"æ¸¬è©¦ä»£ç†é©—è­‰å™¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return False


async def test_crawler_manager(args, enable_validation: bool = True, sources: Optional[List[str]] = None) -> bool:
    """æ¸¬è©¦çˆ¬èŸ²ç®¡ç†å™¨
    
    Args:
        args: å‘½ä»¤è¡Œåƒæ•¸
        enable_validation: æ˜¯å¦å•Ÿç”¨é©—è­‰
        sources: è¦æ¸¬è©¦çš„æºåˆ—è¡¨
    """
    logger.info(f"\n=== æ¸¬è©¦çˆ¬èŸ²ç®¡ç†å™¨ ===")
    
    try:
        # å‰µå»ºæ¸¬è©¦è¼¸å‡ºç›®éŒ„
        test_output_dir = Path("test_output")
        test_output_dir.mkdir(exist_ok=True)
        
        # å‰µå»ºç®¡ç†å™¨
        manager = CrawlerManager(
            output_dir=test_output_dir,
            enable_validation=enable_validation,
            validation_timeout=5.0,
            max_concurrent_validation=20,
            etl_mode=getattr(args, 'etl_mode', False)
        )
        
        # åˆ—å‡ºå¯ç”¨çˆ¬èŸ²
        available_crawlers = manager.list_available_crawlers()
        logger.info(f"å¯ç”¨çˆ¬èŸ²: {', '.join(available_crawlers)}")
        
        # ç¢ºå®šè¦æ¸¬è©¦çš„æº
        if sources is None:
            sources = available_crawlers
        
        logger.info(f"å°‡æ¸¬è©¦æº: {', '.join(sources)}")
        
        # é‹è¡Œçˆ¬èŸ²ç®¡ç†å™¨
        result = await manager.crawl_all_sources(
            sources=sources,
            validate_proxies=enable_validation,
            output_formats=['json', 'markdown']
        )
        
        # é¡¯ç¤ºçµæœ
        stats = result['stats']
        logger.info(f"\n=== ç®¡ç†å™¨æ¸¬è©¦çµæœ ===")
        logger.info(f"ç¸½çˆ¬å–ä»£ç†æ•¸: {stats['total_crawled']}")
        logger.info(f"å»é‡å¾Œä»£ç†æ•¸: {stats['total_unique']}")
        
        if enable_validation and stats['total_validated'] > 0:
            logger.info(f"é©—è­‰ä»£ç†æ•¸: {stats['total_validated']}")
            logger.info(f"å¯ç”¨ä»£ç†æ•¸: {stats['total_working']}")
            success_rate = (stats['total_working'] / stats['total_validated']) * 100
            logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        logger.info(f"ç¸½è€—æ™‚: {stats.get('end_time', 0) - stats.get('start_time', 0):.2f}s")
        
        # é¡¯ç¤ºæŒ‰æºçµ±è¨ˆ
        if stats['by_source']:
            logger.info(f"\næŒ‰æºçµ±è¨ˆ:")
            for source, info in stats['by_source'].items():
                if 'error' in info:
                    logger.warning(f"  {source}: éŒ¯èª¤ - {info['error']}")
                else:
                    logger.info(f"  {source}: {info['count']} å€‹ä»£ç†")
        
        # é¡¯ç¤ºè¼¸å‡ºæ–‡ä»¶
        if result['output_files']:
            logger.info(f"\nè¼¸å‡ºæ–‡ä»¶:")
            for format_type, file_path in result['output_files'].items():
                logger.info(f"  {format_type.upper()}: {file_path}")
        
        return stats['total_unique'] > 0
        
    except Exception as e:
        logger.error(f"æ¸¬è©¦çˆ¬èŸ²ç®¡ç†å™¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return False


async def run_tests(args):
    """é‹è¡Œæ‰€æœ‰æ¸¬è©¦
    
    Args:
        args: å‘½ä»¤è¡Œåƒæ•¸
    """
    logger.info("é–‹å§‹ä»£ç†çˆ¬èŸ²ç³»çµ±æ¸¬è©¦...")
    
    test_results = []
    
    # æ¸¬è©¦å–®å€‹çˆ¬èŸ²
    if args.source:
        # åªæ¸¬è©¦æŒ‡å®šçš„æº
        result = await test_single_crawler(args.source, limit=5)
        test_results.append((f"å–®å€‹çˆ¬èŸ²æ¸¬è©¦ ({args.source})", result))
    else:
        # æ¸¬è©¦æ‰€æœ‰çˆ¬èŸ²
        for crawler_name in ['sslproxies', 'geonode', 'free_proxy_list']:
            result = await test_single_crawler(crawler_name, limit=3)
            test_results.append((f"å–®å€‹çˆ¬èŸ²æ¸¬è©¦ ({crawler_name})", result))
    
    # æ¸¬è©¦ä»£ç†é©—è­‰å™¨ï¼ˆé™¤éæ˜¯å¿«é€Ÿæ¨¡å¼ï¼‰
    if not args.quick:
        result = await test_proxy_validator(proxies_to_test=3)
        test_results.append(("ä»£ç†é©—è­‰å™¨æ¸¬è©¦", result))
    
    # æ¸¬è©¦çˆ¬èŸ²ç®¡ç†å™¨
    sources = [args.source] if args.source else None
    result = await test_crawler_manager(
        args,
        enable_validation=not args.quick,
        sources=sources
    )
    test_results.append(("çˆ¬èŸ²ç®¡ç†å™¨æ¸¬è©¦", result))
    
    # é¡¯ç¤ºæ¸¬è©¦ç¸½çµ
    logger.info(f"\n=== æ¸¬è©¦ç¸½çµ ===")
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        logger.info(f"{test_name}: {status}")
        if result:
            passed += 1
    
    logger.info(f"\nç¸½è¨ˆ: {passed}/{total} å€‹æ¸¬è©¦é€šé")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰æ¸¬è©¦éƒ½é€šéäº†ï¼ä»£ç†çˆ¬èŸ²ç³»çµ±é‹è¡Œæ­£å¸¸ã€‚")
        return 0
    else:
        logger.warning(f"âš ï¸  æœ‰ {total - passed} å€‹æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç³»çµ±é…ç½®ã€‚")
        return 1


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="ä»£ç†çˆ¬èŸ²ç³»çµ±æ¸¬è©¦å·¥å…·")
    
    parser.add_argument(
        "--quick",
        action="store_true",
        help="å¿«é€Ÿæ¸¬è©¦æ¨¡å¼ï¼Œè·³éä»£ç†é©—è­‰"
    )
    
    parser.add_argument(
        "--source",
        choices=['sslproxies', 'geonode', 'free_proxy_list'],
        help="åªæ¸¬è©¦æŒ‡å®šçš„ä»£ç†æº"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="å•Ÿç”¨è©³ç´°æ—¥èªŒè¼¸å‡º"
    )
    
    parser.add_argument(
        "--etl-mode",
        action="store_true",
        help="å•Ÿç”¨ETLæ¨¡å¼ï¼ŒæŒ‰ç…§ETLæµç¨‹è¦ç¯„å­˜æ”¾æª”æ¡ˆ"
    )
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    setup_logging(args.verbose)
    
    # é‹è¡Œæ¸¬è©¦
    try:
        exit_code = asyncio.run(run_tests(args))
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.warning("\nç”¨æˆ¶ä¸­æ–·æ¸¬è©¦")
        sys.exit(130)
    except Exception as e:
        logger.error(f"æ¸¬è©¦é‹è¡Œå¤±æ•—: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()