# 網路爬蟲進階技術與反檢測策略報告

## 摘要

本報告詳細介紹現代網路爬蟲面臨的檢測機制及相應的反檢測策略，包含 User-Agent 輪換、代理IP池管理、TLS指紋模擬、Robots.txt 合規分析等關鍵技術，並提供完整的實現代碼方案。

## 1. User-Agent 池管理系統

### 1.1 核心實現代碼

```python
from user_agent_switcher import UserAgentSwitcher
import random
import time
import threading


class UAManager:
    def __init__(self, change_after_requests=25):
        self.uas = UserAgentSwitcher()
        self.request_count = 0
        self.change_after = change_after_requests
        self.current_ua = self.uas.get_random_user_agent()
        self.lock = threading.Lock()

    def get_ua(self):
        with self.lock:
            self.request_count += 1
        if self.request_count >= self.change_after:
            self.current_ua = self.uas.get_random_user_agent()
            self.request_count = 0
            print(f"已更換 User-Agent: {self.current_ua}")
        return self.current_ua


class TimedUAManager:
    def __init__(self, change_interval=300):
        self.uas = UserAgentSwitcher()
        self.current_ua = self.uas.get_random_user_agent()
        self.change_interval = change_interval
        self.last_change = time.time()
        self.running = True
        self.thread = threading.Thread(target=self._auto_change)
        self.thread.daemon = True
        self.thread.start()

    def _auto_change(self):
        while self.running:
            time.sleep(self.change_interval)
            self.current_ua = self.uas.get_random_user_agent()
            self.last_change = time.time()
            print(f"定時更換 User-Agent: {self.current_ua}")

    def get_ua(self):
        return self.current_ua

    def stop(self):
        self.running = False
        self.thread.join()


# 使用示例
ua_manager = UAManager(change_after_requests=20)
headers = {"User-Agent": ua_manager.get_ua()}
```

### 1.2 智能動態調整策略

```python
class SmartUAManager:
    def __init__(self):
        self.uas = UserAgentSwitcher()
        self.current_ua = self.uas.get_random_user_agent()
        self.request_count = 0
        self.fail_count = 0
        self.change_threshold = 25

    def get_ua(self):
        self.request_count += 1
        # 動態調整策略
        if self.fail_count > 5:
            self.change_threshold = max(10, self.change_threshold - 5)
        elif self.fail_count < 2:
            self.change_threshold = min(50, self.change_threshold + 5)
        if self.request_count >= self.change_threshold:
            self._change_ua()
        return self.current_ua

    def _change_ua(self):
        self.current_ua = self.uas.get_random_user_agent()
        self.request_count = 0

    def record_failure(self):
        self.fail_count += 1
        self._change_ua()

    def record_success(self):
        self.fail_count = max(0, self.fail_count - 1)
```

## 2. 代理 IP 池管理系統

### 2.1 免費代理獲取與驗證

```python
import requests
from concurrent.futures import ThreadPoolExecutor
import time
import random


class FreeProxyManager:
    def __init__(self):
        self.working_proxies = []
        self.last_update = 0

    def fetch_free_proxies(self):
        """從多個來源獲取免費代理"""
        sources = [
            'https://api.proxyscrape.com/v2/?request=getproxies&protocol=http',
            'https://www.sslproxies.org',
            'https://geonode.com/free-proxy-list'
        ]
        all_proxies = []
        for url in sources:
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    proxies = response.text.strip().split('\n')
                    all_proxies.extend([p.strip() for p in proxies if p.strip()])
            except Exception:
                continue
        return list(set(all_proxies))

    def test_proxy(self, proxy):
        """測試代理可用性"""
        try:
            start_time = time.time()
            response = requests.get(
                'http://httpbin.org/ip',
                proxies={'http': f'http://{proxy}', 'https': f'http://{proxy}'},
                timeout=8
            )
            response_time = time.time() - start_time
            if response.status_code == 200:
                return {
                    'proxy': proxy,
                    'working': True,
                    'response_time': response_time
                }
        except Exception:
            pass
        return {'proxy': proxy, 'working': False}

    def update_proxies(self):
        """更新代理池"""
        print("開始獲取免費代理...")
        raw_proxies = self.fetch_free_proxies()
        working_proxies = []
        with ThreadPoolExecutor(max_workers=20) as executor:
            results = executor.map(self.test_proxy, raw_proxies)
            for result in results:
                if result['working'] and result['response_time'] < 5.0:
                    working_proxies.append(result['proxy'])
        self.working_proxies = working_proxies
        self.last_update = time.time()
        print(f"找到 {len(working_proxies)} 個可用代理")

    def get_random_proxy(self):
        if time.time() - self.last_update > 1800 or not self.working_proxies:
            self.update_proxies()
        return random.choice(self.working_proxies) if self.working_proxies else None
```

### 2.2 CrowdSec 惡意 IP 過濾集成

```python
class SafeProxyManager(FreeProxyManager):
    def __init__(self):
        super().__init__()
        self.crowdsec_url = "http://localhost:8080/v1/decisions"

    def is_malicious_ip(self, ip):
        """使用 CrowdSec 檢查惡意 IP"""
        try:
            response = requests.get(f"{self.crowdsec_url}?ip={ip}", timeout=2)
            if response.status_code == 200:
                return len(response.json()) > 0
        except Exception:
            return True  # 謹慎起見，API失敗時視為惡意
        return False

    def filter_with_crowdsec(self, proxies):
        """過濾惡意代理"""
        clean_proxies = []
        for proxy in proxies:
            ip = proxy.split(':')[0]
            if not self.is_malicious_ip(ip):
                clean_proxies.append(proxy)
        return clean_proxies

    def update_proxies(self):
        raw_proxies = self.fetch_free_proxies()
        clean_proxies = self.filter_with_crowdsec(raw_proxies)
        # 驗證代理有效性
        working_proxies = []
        with ThreadPoolExecutor(max_workers=15) as executor:
            results = executor.map(self.test_proxy, clean_proxies)
            for result in results:
                if result['working']:
                    working_proxies.append(result['proxy'])
        self.working_proxies = working_proxies
        self.last_update = time.time()
        print(f"安全代理: {len(working_proxies)}/{len(raw_proxies)}")
```

## 3. TLS 指紋模擬系統

### 3.1 Python-Tls-Client 集成

```python
import tls_client
import random


class TLSFingerprintManager:
    def __init__(self):
        self.fingerprints = {
            'chrome': [
                'chrome_110', 'chrome_109', 'chrome_108',
                'chrome_107', 'chrome_106', 'chrome_105'
            ],
            'firefox': [
                'firefox_109', 'firefox_108', 'firefox_107',
                'firefox_106', 'firefox_105', 'firefox_104'
            ],
            'safari': [
                'safari_16_0', 'safari_15_6', 'safari_15_5'
            ]
        }

    def get_random_fingerprint(self, browser_type=None):
        if browser_type and browser_type in self.fingerprints:
            return random.choice(self.fingerprints[browser_type])
        all_fps = [fp for fps in self.fingerprints.values() for fp in fps]
        return random.choice(all_fps)

    def create_session(self, fingerprint=None, **kwargs):
        if not fingerprint:
            fingerprint = self.get_random_fingerprint()
        return tls_client.Session(
            client_identifier=fingerprint,
            random_tls_extension_order=True,
            **kwargs
        )


# 使用示例
tls_manager = TLSFingerprintManager()
session = tls_manager.create_session('chrome_110')
response = session.get('https://httpbin.org/headers')
```

## 4. 網站合規性分析系統

### 4.1 Robots.txt 分析器

```python
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin
import requests


class RobotsAnalyzer:
    def __init__(self, base_url):
        self.base_url = base_url
        self.robots_url = urljoin(base_url, '/robots.txt')
        self.rp = RobotFileParser()

    def analyze(self):
        try:
            self.rp.set_url(self.robots_url)
            self.rp.read()
            analysis = {
                'exists': True,
                'user_agents': {},
                'sitemaps': [],
                'disallow_rules': [],
                'allow_rules': []
            }
            # 解析詳細規則
            response = requests.get(self.robots_url, timeout=10)
            if response.status_code == 200:
                lines = response.text.split('\n')
                current_ua = '*'
                for line in lines:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    if line.lower().startswith('user-agent:'):
                        current_ua = line.split(':', 1)[1].strip()
                        analysis['user_agents'][current_ua] = []
                    elif line.lower().startswith('disallow:'):
                        path = line.split(':', 1)[1].strip()
                        analysis['disallow_rules'].append({'agent': current_ua, 'path': path})
                    elif line.lower().startswith('allow:'):
                        path = line.split(':', 1)[1].strip()
                        analysis['allow_rules'].append({'agent': current_ua, 'path': path})
                    elif line.lower().startswith('sitemap:'):
                        sitemap_url = line.split(':', 1)[1].strip()
                        analysis['sitemaps'].append(sitemap_url)
            return analysis
        except Exception as e:
            return {'exists': False, 'error': str(e), 'sitemaps': []}
```

### 4.2 Sitemap 分析器

```python
import xml.etree.ElementTree as ET


class SitemapAnalyzer:
    def __init__(self, sitemap_url):
        self.sitemap_url = sitemap_url

    def parse_sitemap(self):
        try:
            response = requests.get(self.sitemap_url, timeout=15)
            if response.status_code != 200:
                return {'error': f'HTTP {response.status_code}'}
            root = ET.fromstring(response.content)
            urls = []
            # 檢查是否是站點地圖索引
            if root.tag.endswith('sitemapindex'):
                sitemap_urls = []
                for sitemap in root.findall('.//{*}sitemap/{*}loc'):
                    sitemap_urls.append(sitemap.text)
                # 遞歸解析子站點地圖
                all_urls = []
                for sub_sitemap in sitemap_urls:
                    sub_analyzer = SitemapAnalyzer(sub_sitemap)
                    result = sub_analyzer.parse_sitemap()
                    if 'urls' in result:
                        all_urls.extend(result['urls'])
                return {'type': 'index', 'sitemaps': sitemap_urls, 'urls': all_urls}
            else:
                # 解析普通站點地圖
                for url_elem in root.findall('.//{*}url'):
                    url_data = {
                        'loc': None,
                        'lastmod': None,
                        'changefreq': None,
                        'priority': None
                    }
                    for field in ['loc', 'lastmod', 'changefreq', 'priority']:
                        elem = url_elem.find(f'{{*}}{field}')
                        if elem is not None:
                            url_data[field] = elem.text
                    if url_data['loc']:
                        urls.append(url_data)
                return {'type': 'sitemap', 'urls': urls, 'count': len(urls)}
        except Exception as e:
            return {'error': str(e)}
```

## 5. 完整爬蟲系統集成

### 5.1 綜合爬蟲類

```python
class AdvancedCrawler:
    def __init__(self):
        self.ua_manager = SmartUAManager()
        self.proxy_manager = SafeProxyManager()
        self.tls_manager = TLSFingerprintManager()
        self.robots_analyzer = None
        self.session = None

    def initialize_session(self):
        """初始化會話"""
        fingerprint = self.tls_manager.get_random_fingerprint()
        self.session = self.tls_manager.create_session(fingerprint)

    def analyze_target(self, target_url):
        """分析目標網站"""
        self.robots_analyzer = RobotsAnalyzer(target_url)
        analysis = self.robots_analyzer.analyze()
        if analysis['exists']:
            print("✅ Robots.txt 存在")
            print(f"禁止規則: {len(analysis['disallow_rules'])} 條")
            print(f"站點地圖: {len(analysis['sitemaps'])} 個")
        else:
            print("⚠️ 沒有robots.txt，請謹慎爬取")
        return analysis

    def make_request(self, url, use_proxy=True):
        """發送請求"""
        if not self.session:
            self.initialize_session()
        headers = {
            'User-Agent': self.ua_manager.get_ua(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
        }
        proxies = None
        if use_proxy:
            proxy = self.proxy_manager.get_random_proxy()
            if proxy:
                proxies = {'http': proxy, 'https': proxy}
        try:
            response = self.session.get(
                url,
                headers=headers,
                proxies=proxies,
                timeout=15
            )
            if response.status_code == 200:
                self.ua_manager.record_success()
                return response
            else:
                self.ua_manager.record_failure()
                return None
        except Exception as e:
            self.ua_manager.record_failure()
            print(f"請求失敗: {e}")
            return None

    def crawl_from_sitemap(self, sitemap_url):
        """從站點地圖開始爬取"""
        analyzer = SitemapAnalyzer(sitemap_url)
        result = analyzer.parse_sitemap()
        if 'urls' in result:
            for url_info in result['urls']:
                response = self.make_request(url_info['loc'])
                if response:
                    yield response, url_info
                time.sleep(random.uniform(1, 3))  # 禮貌延遲
```

```python
# 使用示例
def main():
    crawler = AdvancedCrawler()
    # 分析目標網站
    target_url = "https://example.com"
    analysis = crawler.analyze_target(target_url)
    if analysis['sitemaps']:
        # 從站點地圖爬取
        for response, url_info in crawler.crawl_from_sitemap(analysis['sitemaps'][0]):
            # 處理響應內容
            print(f"成功爬取: {url_info['loc']}")
    else:
        # 常規爬取
        urls_to_crawl = ['https://example.com/page1', 'https://example.com/page2']
        for url in urls_to_crawl:
            response = crawler.make_request(url)
            if response:
                print(f"成功爬取: {url}")


if __name__ == "__main__":
    main()
```

## 6. 最佳實踐與注意事項

### 6.1 配置建議

```python
# settings.py 範例配置
CRAWLER_CONFIG = {
    'request_delay': (1, 3),  # 隨機延遲範圍
    'max_retries': 3,
    'timeout': 15,
    'max_concurrent': 5,
    'user_agent_rotation': 25,  # 每25次請求更換UA
    'proxy_rotation': 10,       # 每10次請求更換代理
    'respect_robots': True,
    'use_sitemap': True,
    'max_pages': 1000,         # 最大爬取頁面數
}
```

### 6.2 錯誤處理與日誌

```python
import logging
from scrapy.utils.log import configure_logging

# 配置日誌
configure_logging(install_root_handler=False)
logging.basicConfig(
    filename='crawler.log',
    format='%(asctime)s [%(name)s] %(levelname)s: %(message)s',
    level=logging.INFO
)


class ErrorHandler:
    def __init__(self):
        self.error_count = 0
        self.max_errors = 50

    def handle_error(self, error, url):
        self.error_count += 1
        logging.error(f"爬取 {url} 時出錯: {error}")
        if self.error_count > self.max_errors:
            logging.critical("錯誤過多，停止爬取")
            raise SystemExit("Too many errors")
```

## 7. 結論與建議

### 7.1 技術總結

1. 多層次偽裝：結合 UA輪換、代理IP、TLS指紋模擬
2. 合規爬取：尊重 robots.txt，優先使用 sitemap
3. 智能調度：根據成功率動態調整策略
4. 安全過濾：使用 CrowdSec 過濾惡意代理

### 7.2 資源建議

* 免費資源：僅用於測試和學習
* 生產環境：使用付費代理服務（Bright Data、Oxylabs等）
* 合規性：確保遵守目標網站條款和當地法律法規

### 7.3 性能優化

* 會話復用減少 TLS 握手開銷
* 連接池管理提高效率
* 異步處理實現高並發

這份完整方案提供了從基礎到進階的爬蟲技術實現，能夠有效對抗現代網站的反爬蟲機制，同時保持合規性和效率。
