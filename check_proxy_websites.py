#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»£ç†ç¶²ç«™å¯è¨ªå•æ€§æª¢æŸ¥å·¥å…·

æª¢æŸ¥PRDæ–‡æª”ä¸­åˆ—å‡ºçš„2å€‹ä»£ç†ç¶²ç«™çš„ç•¶å‰ç‹€æ…‹ï¼Œ
è­˜åˆ¥å“ªäº›ç¶²ç«™ä»ç„¶å¯ç”¨ï¼Œå“ªäº›å·²ç¶“ç„¡æ³•è¨ªå•æˆ–é‡è¤‡ã€‚
"""

import asyncio
import aiohttp
import time
from typing import Dict, List, Tuple
from dataclasses import dataclass
from urllib.parse import urlparse


@dataclass
class WebsiteStatus:
    """ç¶²ç«™ç‹€æ…‹è³‡æ–™é¡"""
    url: str
    name: str
    status_code: int = 0
    response_time: float = 0.0
    is_accessible: bool = False
    error_message: str = ""
    content_length: int = 0
    final_url: str = ""  # é‡å®šå‘å¾Œçš„æœ€çµ‚URL


class ProxyWebsiteChecker:
    """ä»£ç†ç¶²ç«™æª¢æŸ¥å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æª¢æŸ¥å™¨"""
        self.websites = [
            # ç•¶å‰å°ˆæ¡ˆä½¿ç”¨çš„ä»£ç†ä¾†æº
            ("https://www.sslproxies.org/", "SSL Proxies"),
            ("https://geonode.com/free-proxy-list/", "Geonode Free Proxy"),
            # GitHub ä»£ç†å°ˆæ¡ˆ
            ("https://github.com/monosans/proxy-scraper-checker", "Proxy Scraper Checker (Rust)"),
            ("https://github.com/roosterkid/openproxylist", "Open Proxy List"),
            ("https://github.com/proxifly/free-proxy-list", "Proxifly Free Proxy List"),
            # ProxyScraper ç›¸é—œ
            ("https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt", "ProxyScraper HTTP List"),
            ("https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks4.txt", "ProxyScraper SOCKS4 List"),
            ("https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks5.txt", "ProxyScraper SOCKS5 List")
        ]
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    async def check_website(self, session: aiohttp.ClientSession, url: str, name: str) -> WebsiteStatus:
        """æª¢æŸ¥å–®å€‹ç¶²ç«™çš„å¯è¨ªå•æ€§
        
        Args:
            session: aiohttpæœƒè©±å°è±¡
            url: ç¶²ç«™URL
            name: ç¶²ç«™åç¨±
            
        Returns:
            WebsiteStatus: ç¶²ç«™ç‹€æ…‹ä¿¡æ¯
        """
        status = WebsiteStatus(url=url, name=name)
        start_time = time.time()
        
        try:
            async with session.get(
                url, 
                headers=self.headers,
                timeout=aiohttp.ClientTimeout(total=15),
                allow_redirects=True
            ) as response:
                status.status_code = response.status
                status.response_time = time.time() - start_time
                status.final_url = str(response.url)
                
                # è®€å–å…§å®¹é•·åº¦
                content = await response.read()
                status.content_length = len(content)
                
                # åˆ¤æ–·æ˜¯å¦å¯è¨ªå•ï¼ˆç‹€æ…‹ç¢¼200-299ç‚ºæˆåŠŸï¼‰
                status.is_accessible = 200 <= response.status < 300
                
                if not status.is_accessible:
                    status.error_message = f"HTTP {response.status}"
                    
        except asyncio.TimeoutError:
            status.response_time = time.time() - start_time
            status.error_message = "è«‹æ±‚è¶…æ™‚"
        except aiohttp.ClientError as e:
            status.response_time = time.time() - start_time
            status.error_message = f"é€£æ¥éŒ¯èª¤: {str(e)}"
        except Exception as e:
            status.response_time = time.time() - start_time
            status.error_message = f"æœªçŸ¥éŒ¯èª¤: {str(e)}"
        
        return status
    
    async def check_all_websites(self) -> List[WebsiteStatus]:
        """æª¢æŸ¥æ‰€æœ‰ç¶²ç«™çš„å¯è¨ªå•æ€§
        
        Returns:
            List[WebsiteStatus]: æ‰€æœ‰ç¶²ç«™çš„ç‹€æ…‹åˆ—è¡¨
        """
        connector = aiohttp.TCPConnector(
            limit=10,
            limit_per_host=5,
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [
                self.check_website(session, url, name) 
                for url, name in self.websites
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è™•ç†ç•°å¸¸çµæœ
            statuses = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    url, name = self.websites[i]
                    status = WebsiteStatus(url=url, name=name)
                    status.error_message = f"ä»»å‹™ç•°å¸¸: {str(result)}"
                    statuses.append(status)
                else:
                    statuses.append(result)
            
            return statuses
    
    def analyze_results(self, statuses: List[WebsiteStatus]) -> Dict:
        """åˆ†ææª¢æŸ¥çµæœ
        
        Args:
            statuses: ç¶²ç«™ç‹€æ…‹åˆ—è¡¨
            
        Returns:
            Dict: åˆ†æçµæœ
        """
        accessible = [s for s in statuses if s.is_accessible]
        inaccessible = [s for s in statuses if not s.is_accessible]
        
        # æª¢æŸ¥é‡è¤‡çš„åŸŸå
        domains = {}
        for status in statuses:
            domain = urlparse(status.url).netloc
            if domain in domains:
                domains[domain].append(status)
            else:
                domains[domain] = [status]
        
        duplicates = {domain: sites for domain, sites in domains.items() if len(sites) > 1}
        
        return {
            'total': len(statuses),
            'accessible': len(accessible),
            'inaccessible': len(inaccessible),
            'accessible_sites': accessible,
            'inaccessible_sites': inaccessible,
            'duplicates': duplicates,
            'success_rate': len(accessible) / len(statuses) * 100 if statuses else 0
        }
    
    def print_results(self, analysis: Dict):
        """æ‰“å°æª¢æŸ¥çµæœ
        
        Args:
            analysis: åˆ†æçµæœå­—å…¸
        """
        print("\n" + "="*80)
        print("ä»£ç†ç¶²ç«™å¯è¨ªå•æ€§æª¢æŸ¥å ±å‘Š")
        print("="*80)
        
        print(f"\nğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        print(f"   ç¸½ç¶²ç«™æ•¸: {analysis['total']}")
        print(f"   å¯è¨ªå•: {analysis['accessible']} ({analysis['success_rate']:.1f}%)")
        print(f"   ä¸å¯è¨ªå•: {analysis['inaccessible']}")
        
        print(f"\nâœ… å¯è¨ªå•çš„ç¶²ç«™ ({len(analysis['accessible_sites'])}å€‹):")
        for status in analysis['accessible_sites']:
            print(f"   â€¢ {status.name}")
            print(f"     URL: {status.url}")
            print(f"     ç‹€æ…‹: HTTP {status.status_code} | éŸ¿æ‡‰æ™‚é–“: {status.response_time:.2f}s | å…§å®¹å¤§å°: {status.content_length:,} bytes")
            if status.final_url != status.url:
                print(f"     é‡å®šå‘è‡³: {status.final_url}")
            print()
        
        print(f"\nâŒ ä¸å¯è¨ªå•çš„ç¶²ç«™ ({len(analysis['inaccessible_sites'])}å€‹):")
        for status in analysis['inaccessible_sites']:
            print(f"   â€¢ {status.name}")
            print(f"     URL: {status.url}")
            print(f"     éŒ¯èª¤: {status.error_message}")
            if status.status_code > 0:
                print(f"     ç‹€æ…‹ç¢¼: {status.status_code} | éŸ¿æ‡‰æ™‚é–“: {status.response_time:.2f}s")
            print()
        
        if analysis['duplicates']:
            print(f"\nğŸ”„ é‡è¤‡åŸŸåæª¢æŸ¥:")
            for domain, sites in analysis['duplicates'].items():
                print(f"   åŸŸå: {domain}")
                for site in sites:
                    print(f"     - {site.name}: {site.url}")
                print()
        else:
            print(f"\nâœ¨ æ²’æœ‰ç™¼ç¾é‡è¤‡çš„åŸŸå")
        
        print("\n" + "="*80)
        print("æª¢æŸ¥å®Œæˆ")
        print("="*80)


async def main():
    """ä¸»å‡½æ•¸"""
    print("é–‹å§‹æª¢æŸ¥ä»£ç†ç¶²ç«™å¯è¨ªå•æ€§...")
    
    checker = ProxyWebsiteChecker()
    statuses = await checker.check_all_websites()
    analysis = checker.analyze_results(statuses)
    checker.print_results(analysis)


if __name__ == "__main__":
    asyncio.run(main())