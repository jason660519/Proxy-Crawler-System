"""é«˜ç´šä»£ç†ç²å–å™¨æ¨¡çµ„

å¯¦æ–½ä»£ç†æ•´åˆç¬¬ä¸€éšæ®µï¼šæ“´å±•æ•¸æ“šä¾†æº
- Shodan/Censys API æ•´åˆ
- ProxyScrape å¤šä¾†æºèšåˆ
- GitHub é–‹æºå°ˆæ¡ˆæ•´åˆ
- æ™ºèƒ½ä»£ç†ç™¼ç¾èˆ‡é©—è­‰
"""

import asyncio
import aiohttp
import json
import re
from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any, AsyncGenerator, Set
from datetime import datetime, timedelta
import logging
from pathlib import Path
import random
from urllib.parse import urljoin, urlparse

from .models import ProxyNode, ProxyProtocol, ProxyAnonymity, ProxyStatus
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


class ProxyFetcher(ABC):
    """ä»£ç†ç²å–å™¨æŠ½è±¡åŸºé¡ã€‚"""
    
    def __init__(self, name: str = None):
        """åˆå§‹åŒ–ä»£ç†ç²å–å™¨ã€‚
        
        Args:
            name: ç²å–å™¨åç¨±
        """
        self.name = name or self.__class__.__name__
        self.enabled = True  # æ·»åŠ  enabled å±¬æ€§
        self.stats = {
            'total_fetched': 0,
            'successful_fetches': 0,
            'failed_fetches': 0,
            'last_fetch_time': None
        }
    
    @abstractmethod
    async def fetch_proxies(self, limit: int = None) -> List[ProxyNode]:
        """ç²å–ä»£ç†åˆ—è¡¨ã€‚
        
        Args:
            limit: é™åˆ¶ç²å–çš„ä»£ç†æ•¸é‡
            
        Returns:
            ä»£ç†ç¯€é»åˆ—è¡¨
        """
        pass
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–ç²å–å™¨çµ±è¨ˆä¿¡æ¯ã€‚
        
        Returns:
            çµ±è¨ˆä¿¡æ¯å­—å…¸
        """
        return self.stats.copy()


class ProxyScrapeApiFetcher(ProxyFetcher):
    """ProxyScrape API ä»£ç†ç²å–å™¨
    
    æ”¯æ´å¤šç¨®æ ¼å¼å’Œå”è­°çš„ä»£ç†ç²å–
    """
    
    def __init__(self, api_key: Optional[str] = None):
        super().__init__("proxyscrape-api")
        self.api_key = api_key
        self.base_url = "https://api.proxyscrape.com/v2/"
        self.session: Optional[aiohttp.ClientSession] = None
        self.total_fetched = 0
        self.fetch_errors = 0
        self.last_fetch_time = None
        
        # æ”¯æ´çš„ä»£ç†é¡å‹é…ç½®
        self.proxy_configs = [
            {"request": "get", "protocol": "http", "timeout": 10000, "country": "all"},
            {"request": "get", "protocol": "socks4", "timeout": 10000, "country": "all"},
            {"request": "get", "protocol": "socks5", "timeout": 10000, "country": "all"},
        ]
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """ç²å–æˆ–å‰µå»º HTTP æœƒè©±"""
        if not self.session:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
        return self.session
    
    async def fetch_proxies(self, limit: Optional[int] = None) -> List[ProxyNode]:
        """å¾ ProxyScrape API ç²å–ä»£ç†"""
        all_proxies = []
        session = await self._get_session()
        
        for config in self.proxy_configs:
            try:
                proxies = await self._fetch_by_protocol(session, config)
                all_proxies.extend(proxies)
                
                if limit and len(all_proxies) >= limit:
                    break
                    
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                await asyncio.sleep(random.uniform(1, 2))
                
            except Exception as e:
                logger.error(f"âŒ ProxyScrape {config['protocol']} ç²å–å¤±æ•—: {e}")
                self.fetch_errors += 1
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_proxies = self._deduplicate_proxies(all_proxies)
        result = unique_proxies[:limit] if limit else unique_proxies
        
        self.last_fetch_time = datetime.now()
        self.total_fetched += len(result)
        
        logger.info(f"âœ… ProxyScrape API ç²å–åˆ° {len(result)} å€‹ä»£ç†")
        return result
    
    async def _fetch_by_protocol(self, session: aiohttp.ClientSession, config: Dict[str, Any]) -> List[ProxyNode]:
        """æŒ‰å”è­°ç²å–ä»£ç†"""
        params = {
            "request": config["request"],
            "protocol": config["protocol"],
            "timeout": config["timeout"],
            "country": config["country"],
            "format": "textplain"
        }
        
        if self.api_key:
            params["api_key"] = self.api_key
        
        async with session.get(self.base_url, params=params) as response:
            if response.status == 200:
                content = await response.text()
                return self._parse_proxy_list(content, config["protocol"])
            else:
                logger.warning(f"ProxyScrape API éŸ¿æ‡‰éŒ¯èª¤: {response.status}")
                return []
    
    def _parse_proxy_list(self, content: str, protocol: str) -> List[ProxyNode]:
        """è§£æä»£ç†åˆ—è¡¨"""
        proxies = []
        lines = content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or ':' not in line:
                continue
            
            try:
                host, port_str = line.split(':', 1)
                port = int(port_str)
                
                proxy_protocol = {
                    "http": ProxyProtocol.HTTP,
                    "https": ProxyProtocol.HTTPS,
                    "socks4": ProxyProtocol.SOCKS4,
                    "socks5": ProxyProtocol.SOCKS5
                }.get(protocol.lower(), ProxyProtocol.HTTP)
                
                proxy = ProxyNode(
                    host=host.strip(),
                    port=port,
                    protocol=proxy_protocol,
                    status=ProxyStatus.INACTIVE,
                    source=self.name,
                    tags=["proxyscrape", protocol]
                )
                
                proxies.append(proxy)
                
            except ValueError as e:
                logger.debug(f"è§£æä»£ç†è¡Œå¤±æ•— '{line}': {e}")
                continue
        
        return proxies
    
    def _deduplicate_proxies(self, proxies: List[ProxyNode]) -> List[ProxyNode]:
        """ä»£ç†å»é‡"""
        seen = set()
        unique_proxies = []
        
        for proxy in proxies:
            key = f"{proxy.host}:{proxy.port}"
            if key not in seen:
                seen.add(key)
                unique_proxies.append(proxy)
        
        return unique_proxies
    
    async def close(self):
        """é—œé–‰æœƒè©±"""
        if self.session:
            await self.session.close()
            self.session = None


class GitHubProxyFetcher(ProxyFetcher):
    """GitHub é–‹æºä»£ç†å°ˆæ¡ˆç²å–å™¨
    
    å¾å¤šå€‹ GitHub ä»£ç†å°ˆæ¡ˆç²å–ä»£ç†åˆ—è¡¨
    """
    
    def __init__(self, github_token: Optional[str] = None):
        super().__init__("github-proxy")
        self.github_token = github_token
        self.session: Optional[aiohttp.ClientSession] = None
        self.total_fetched = 0
        self.fetch_errors = 0
        self.last_fetch_time = None
        
        # GitHub ä»£ç†å°ˆæ¡ˆé…ç½®
        self.github_sources = [
            {
                "name": "proxifly/free-proxy-list",
                "files": ["proxies/http.txt", "proxies/https.txt", "proxies/socks4.txt", "proxies/socks5.txt"],
                "format": "txt"
            },
            {
                "name": "TheSpeedX/PROXY-List",
                "files": ["http.txt", "socks4.txt", "socks5.txt"],
                "format": "txt"
            },
            {
                "name": "ShiftyTR/Proxy-List",
                "files": ["http.txt", "https.txt", "socks4.txt", "socks5.txt"],
                "format": "txt"
            }
        ]
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """ç²å–æˆ–å‰µå»º HTTP æœƒè©±"""
        if not self.session:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/vnd.github.v3.raw'
            }
            
            if self.github_token:
                headers['Authorization'] = f'token {self.github_token}'
            
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
        return self.session
    
    async def fetch_proxies(self, limit: Optional[int] = None) -> List[ProxyNode]:
        """å¾ GitHub å°ˆæ¡ˆç²å–ä»£ç†"""
        all_proxies = []
        session = await self._get_session()
        
        for source in self.github_sources:
            try:
                proxies = await self._fetch_from_github_source(session, source)
                all_proxies.extend(proxies)
                
                if limit and len(all_proxies) >= limit:
                    break
                
                # é¿å…è§¸ç™¼ GitHub API é™åˆ¶
                await asyncio.sleep(random.uniform(2, 4))
                
            except Exception as e:
                logger.error(f"âŒ GitHub å°ˆæ¡ˆ {source['name']} ç²å–å¤±æ•—: {e}")
                self.fetch_errors += 1
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_proxies = self._deduplicate_proxies(all_proxies)
        result = unique_proxies[:limit] if limit else unique_proxies
        
        self.last_fetch_time = datetime.now()
        self.total_fetched += len(result)
        
        logger.info(f"âœ… GitHub å°ˆæ¡ˆç²å–åˆ° {len(result)} å€‹ä»£ç†")
        return result
    
    async def _fetch_from_github_source(self, session: aiohttp.ClientSession, source: Dict[str, Any]) -> List[ProxyNode]:
        """å¾å–®å€‹ GitHub å°ˆæ¡ˆç²å–ä»£ç†"""
        all_proxies = []
        
        for file_path in source["files"]:
            try:
                url = f"https://raw.githubusercontent.com/{source['name']}/main/{file_path}"
                
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        protocol = self._extract_protocol_from_filename(file_path)
                        proxies = self._parse_github_proxy_list(content, protocol, source["name"])
                        all_proxies.extend(proxies)
                    else:
                        logger.debug(f"GitHub æ–‡ä»¶ç²å–å¤±æ•— {url}: {response.status}")
                
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                await asyncio.sleep(random.uniform(0.5, 1))
                
            except Exception as e:
                logger.debug(f"GitHub æ–‡ä»¶è™•ç†å¤±æ•— {file_path}: {e}")
                continue
        
        return all_proxies
    
    def _extract_protocol_from_filename(self, filename: str) -> str:
        """å¾æ–‡ä»¶åæå–å”è­°é¡å‹"""
        filename_lower = filename.lower()
        if "socks5" in filename_lower:
            return "socks5"
        elif "socks4" in filename_lower:
            return "socks4"
        elif "https" in filename_lower:
            return "https"
        elif "http" in filename_lower:
            return "http"
        else:
            return "http"  # é»˜èª
    
    def _parse_github_proxy_list(self, content: str, protocol: str, source_name: str) -> List[ProxyNode]:
        """è§£æ GitHub ä»£ç†åˆ—è¡¨"""
        proxies = []
        lines = content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#') or ':' not in line:
                continue
            
            try:
                # æ”¯æ´å¤šç¨®æ ¼å¼ï¼šip:port æˆ– ip:port:user:pass
                parts = line.split(':')
                if len(parts) >= 2:
                    host = parts[0].strip()
                    port = int(parts[1].strip())
                    
                    proxy_protocol = {
                        "http": ProxyProtocol.HTTP,
                        "https": ProxyProtocol.HTTPS,
                        "socks4": ProxyProtocol.SOCKS4,
                        "socks5": ProxyProtocol.SOCKS5
                    }.get(protocol.lower(), ProxyProtocol.HTTP)
                    
                    proxy = ProxyNode(
                        host=host,
                        port=port,
                        protocol=proxy_protocol,
                        status=ProxyStatus.INACTIVE,
                        source=self.name,
                        tags=["github", protocol, source_name.split('/')[1]],
                        metadata={"github_source": source_name}
                    )
                    
                    proxies.append(proxy)
                
            except (ValueError, IndexError) as e:
                logger.debug(f"è§£æ GitHub ä»£ç†è¡Œå¤±æ•— '{line}': {e}")
                continue
        
        return proxies
    
    def _deduplicate_proxies(self, proxies: List[ProxyNode]) -> List[ProxyNode]:
        """ä»£ç†å»é‡"""
        seen = set()
        unique_proxies = []
        
        for proxy in proxies:
            key = f"{proxy.host}:{proxy.port}"
            if key not in seen:
                seen.add(key)
                unique_proxies.append(proxy)
        
        return unique_proxies
    
    async def close(self):
        """é—œé–‰æœƒè©±"""
        if self.session:
            await self.session.close()
            self.session = None


class ShodanProxyFetcher(ProxyFetcher):
    """Shodan API ä»£ç†ç™¼ç¾å™¨
    
    ä½¿ç”¨ Shodan æœå°‹å¼•æ“ç™¼ç¾é–‹æ”¾çš„ä»£ç†æœå‹™
    """
    
    def __init__(self, api_key: str):
        super().__init__("shodan-proxy")
        self.api_key = api_key
        self.base_url = "https://api.shodan.io"
        self.session: Optional[aiohttp.ClientSession] = None
        self.total_fetched = 0
        self.fetch_errors = 0
        self.last_fetch_time = None
        
        # Shodan æœå°‹æŸ¥è©¢
        self.search_queries = [
            "port:8080 proxy",
            "port:3128 proxy",
            "port:1080 socks",
            "\"HTTP/1.1 200\" \"Via:\" port:8080",
            "\"HTTP/1.1 200\" \"X-Forwarded-For\" port:3128"
        ]
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """ç²å–æˆ–å‰µå»º HTTP æœƒè©±"""
        if not self.session:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            timeout = aiohttp.ClientTimeout(total=60)
            self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
        return self.session
    
    async def fetch_proxies(self, limit: Optional[int] = None) -> List[ProxyNode]:
        """å¾ Shodan ç™¼ç¾ä»£ç†"""
        all_proxies = []
        session = await self._get_session()
        
        for query in self.search_queries:
            try:
                proxies = await self._search_shodan(session, query, limit=50)
                all_proxies.extend(proxies)
                
                if limit and len(all_proxies) >= limit:
                    break
                
                # Shodan API æœ‰é€Ÿç‡é™åˆ¶
                await asyncio.sleep(random.uniform(3, 5))
                
            except Exception as e:
                logger.error(f"âŒ Shodan æœå°‹å¤±æ•— '{query}': {e}")
                self.fetch_errors += 1
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_proxies = self._deduplicate_proxies(all_proxies)
        result = unique_proxies[:limit] if limit else unique_proxies
        
        self.last_fetch_time = datetime.now()
        self.total_fetched += len(result)
        
        logger.info(f"âœ… Shodan ç™¼ç¾ {len(result)} å€‹æ½›åœ¨ä»£ç†")
        return result
    
    async def _search_shodan(self, session: aiohttp.ClientSession, query: str, limit: int = 50) -> List[ProxyNode]:
        """åŸ·è¡Œ Shodan æœå°‹"""
        params = {
            "key": self.api_key,
            "query": query,
            "limit": limit
        }
        
        url = f"{self.base_url}/shodan/host/search"
        
        async with session.get(url, params=params) as response:
            if response.status == 200:
                data = await response.json()
                return self._parse_shodan_results(data, query)
            elif response.status == 401:
                logger.error("âŒ Shodan API é‡‘é‘°ç„¡æ•ˆ")
                return []
            else:
                logger.warning(f"Shodan API éŸ¿æ‡‰éŒ¯èª¤: {response.status}")
                return []
    
    def _parse_shodan_results(self, data: Dict[str, Any], query: str) -> List[ProxyNode]:
        """è§£æ Shodan æœå°‹çµæœ"""
        proxies = []
        
        for match in data.get("matches", []):
            try:
                host = match.get("ip_str")
                port = match.get("port")
                
                if not host or not port:
                    continue
                
                # æ ¹æ“šç«¯å£æ¨æ¸¬å”è­°
                protocol = self._guess_protocol_from_port(port)
                
                # æå–åœ°ç†ä½ç½®ä¿¡æ¯
                location = match.get("location", {})
                
                proxy = ProxyNode(
                    host=host,
                    port=port,
                    protocol=protocol,
                    status=ProxyStatus.INACTIVE,
                    country=location.get("country_name"),
                    region=location.get("region_code"),
                    city=location.get("city"),
                    isp=match.get("isp"),
                    source=self.name,
                    tags=["shodan", "discovered"],
                    metadata={
                        "shodan_query": query,
                        "shodan_data": {
                            "org": match.get("org"),
                            "asn": match.get("asn"),
                            "hostnames": match.get("hostnames", []),
                            "timestamp": match.get("timestamp")
                        }
                    }
                )
                
                proxies.append(proxy)
                
            except Exception as e:
                logger.debug(f"è§£æ Shodan çµæœå¤±æ•—: {e}")
                continue
        
        return proxies
    
    def _guess_protocol_from_port(self, port: int) -> ProxyProtocol:
        """æ ¹æ“šç«¯å£æ¨æ¸¬å”è­°é¡å‹"""
        if port in [1080, 1081]:
            return ProxyProtocol.SOCKS5
        elif port in [1085]:
            return ProxyProtocol.SOCKS4
        elif port in [443, 8443]:
            return ProxyProtocol.HTTPS
        else:
            return ProxyProtocol.HTTP
    
    def _deduplicate_proxies(self, proxies: List[ProxyNode]) -> List[ProxyNode]:
        """ä»£ç†å»é‡"""
        seen = set()
        unique_proxies = []
        
        for proxy in proxies:
            key = f"{proxy.host}:{proxy.port}"
            if key not in seen:
                seen.add(key)
                unique_proxies.append(proxy)
        
        return unique_proxies
    
    async def close(self):
        """é—œé–‰æœƒè©±"""
        if self.session:
            await self.session.close()
            self.session = None


class CensysProxyFetcher(ProxyFetcher):
    """Censys API ä»£ç†ç™¼ç¾å™¨
    
    ä½¿ç”¨ Censys æœå°‹å¼•æ“ç™¼ç¾é–‹æ”¾çš„ä»£ç†æœå‹™
    """
    
    def __init__(self, api_id: str, api_secret: str):
        super().__init__("censys-proxy")
        self.api_id = api_id
        self.api_secret = api_secret
        self.base_url = "https://search.censys.io/api/v1"
        self.session: Optional[aiohttp.ClientSession] = None
        self.total_fetched = 0
        self.fetch_errors = 0
        self.last_fetch_time = None
        
        # Censys æœå°‹æŸ¥è©¢
        self.search_queries = [
            "services.port:8080 AND services.service_name:HTTP",
            "services.port:3128 AND services.service_name:HTTP", 
            "services.port:1080 AND services.service_name:SOCKS",
            "services.port:1081 AND services.service_name:SOCKS",
            "services.port:9050 AND services.service_name:TOR"
        ]
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """ç²å–æˆ–å‰µå»º HTTP æœƒè©±"""
        if not self.session:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            timeout = aiohttp.ClientTimeout(total=60)
            self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
        return self.session
    
    async def fetch_proxies(self, limit: Optional[int] = None) -> List[ProxyNode]:
        """å¾ Censys ç™¼ç¾ä»£ç†"""
        all_proxies = []
        session = await self._get_session()
        
        for query in self.search_queries:
            try:
                proxies = await self._search_censys(session, query, limit=50)
                all_proxies.extend(proxies)
                
                if limit and len(all_proxies) >= limit:
                    break
                
                # Censys API æœ‰é€Ÿç‡é™åˆ¶
                await asyncio.sleep(random.uniform(2, 4))
                
            except Exception as e:
                logger.error(f"âŒ Censys æœå°‹å¤±æ•— '{query}': {e}")
                self.fetch_errors += 1
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_proxies = self._deduplicate_proxies(all_proxies)
        result = unique_proxies[:limit] if limit else unique_proxies
        
        self.last_fetch_time = datetime.now()
        self.total_fetched += len(result)
        
        logger.info(f"âœ… Censys ç™¼ç¾ {len(result)} å€‹æ½›åœ¨ä»£ç†")
        return result
    
    async def _search_censys(self, session: aiohttp.ClientSession, query: str, limit: int = 50) -> List[ProxyNode]:
        """åŸ·è¡Œ Censys æœå°‹"""
        # ä½¿ç”¨ Basic Auth
        auth = aiohttp.BasicAuth(self.api_id, self.api_secret)
        
        params = {
            "q": query,
            "per_page": min(limit, 100),  # Censys æ¯é æœ€å¤š 100 å€‹çµæœ
            "page": 1
        }
        
        url = f"{self.base_url}/hosts/search"
        
        async with session.get(url, params=params, auth=auth) as response:
            if response.status == 200:
                data = await response.json()
                return self._parse_censys_results(data, query)
            elif response.status == 401:
                logger.error("âŒ Censys API æ†‘è­‰ç„¡æ•ˆ")
                return []
            else:
                logger.warning(f"Censys API éŸ¿æ‡‰éŒ¯èª¤: {response.status}")
                return []
    
    def _parse_censys_results(self, data: Dict[str, Any], query: str) -> List[ProxyNode]:
        """è§£æ Censys æœå°‹çµæœ"""
        proxies = []
        
        for result in data.get("result", {}).get("hits", []):
            try:
                host = result.get("ip")
                services = result.get("services", [])
                
                if not host or not services:
                    continue
                
                # å¾æœå‹™ä¸­æå–ç«¯å£å’Œå”è­°ä¿¡æ¯
                for service in services:
                    port = service.get("port")
                    service_name = service.get("service_name", "").upper()
                    
                    if not port:
                        continue
                    
                    # æ ¹æ“šæœå‹™åç¨±æ¨æ¸¬å”è­°
                    protocol = self._guess_protocol_from_service(service_name, port)
                    
                    # æå–åœ°ç†ä½ç½®ä¿¡æ¯
                    location = result.get("location", {})
                    
                    proxy = ProxyNode(
                        host=host,
                        port=port,
                        protocol=protocol,
                        status=ProxyStatus.INACTIVE,
                        country=location.get("country"),
                        region=location.get("province"),
                        city=location.get("city"),
                        isp=result.get("autonomous_system", {}).get("name"),
                        source=self.name,
                        tags=["censys", "discovered"],
                        metadata={
                            "censys_query": query,
                            "censys_data": {
                                "asn": result.get("autonomous_system", {}).get("asn"),
                                "protocols": [s.get("service_name") for s in services],
                                "timestamp": result.get("timestamp")
                            }
                        }
                    )
                    
                    proxies.append(proxy)
                
            except Exception as e:
                logger.debug(f"è§£æ Censys çµæœå¤±æ•—: {e}")
                continue
        
        return proxies
    
    def _guess_protocol_from_service(self, service_name: str, port: int) -> ProxyProtocol:
        """æ ¹æ“šæœå‹™åç¨±å’Œç«¯å£æ¨æ¸¬å”è­°é¡å‹"""
        service_lower = service_name.lower()
        
        if "socks" in service_lower or port in [1080, 1081]:
            return ProxyProtocol.SOCKS5
        elif "tor" in service_lower or port in [9050, 9051]:
            return ProxyProtocol.SOCKS5
        elif "https" in service_lower or port in [443, 8443]:
            return ProxyProtocol.HTTPS
        else:
            return ProxyProtocol.HTTP
    
    def _deduplicate_proxies(self, proxies: List[ProxyNode]) -> List[ProxyNode]:
        """ä»£ç†å»é‡"""
        seen = set()
        unique_proxies = []
        
        for proxy in proxies:
            key = f"{proxy.host}:{proxy.port}"
            if key not in seen:
                seen.add(key)
                unique_proxies.append(proxy)
        
        return unique_proxies
    
    async def close(self):
        """é—œé–‰æœƒè©±"""
        if self.session:
            await self.session.close()
            self.session = None


class AdvancedProxyFetcherManager:
    """é«˜ç´šä»£ç†ç²å–å™¨ç®¡ç†å™¨
    
    æ•´åˆå¤šå€‹é«˜ç´šä»£ç†ä¾†æºï¼Œæä¾›çµ±ä¸€çš„ä»£ç†ç²å–ä»‹é¢
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.fetchers: List[ProxyFetcher] = []
        self._setup_advanced_fetchers()
    
    def _setup_advanced_fetchers(self):
        """è¨­ç½®é«˜ç´šç²å–å™¨"""
        # ProxyScrape API
        proxyscrape_key = self.config.get("proxyscrape_api_key")
        self.add_fetcher(ProxyScrapeApiFetcher(proxyscrape_key))
        
        # GitHub å°ˆæ¡ˆ
        github_token = self.config.get("github_token")
        self.add_fetcher(GitHubProxyFetcher(github_token))
        
        # Shodanï¼ˆéœ€è¦ API é‡‘é‘°ï¼‰
        shodan_key = self.config.get("shodan_api_key")
        if shodan_key:
            self.add_fetcher(ShodanProxyFetcher(shodan_key))
        else:
            logger.info("âš ï¸ æœªé…ç½® Shodan API é‡‘é‘°ï¼Œè·³é Shodan ä»£ç†ç™¼ç¾")
        
        # Censysï¼ˆéœ€è¦ API æ†‘è­‰ï¼‰
        censys_api_id = self.config.get("censys_api_id")
        censys_api_secret = self.config.get("censys_api_secret")
        if censys_api_id and censys_api_secret:
            self.add_fetcher(CensysProxyFetcher(censys_api_id, censys_api_secret))
        else:
            logger.info("âš ï¸ æœªé…ç½® Censys API æ†‘è­‰ï¼Œè·³é Censys ä»£ç†ç™¼ç¾")
    
    def add_fetcher(self, fetcher: ProxyFetcher):
        """æ·»åŠ ç²å–å™¨"""
        self.fetchers.append(fetcher)
        logger.info(f"âœ… æ·»åŠ é«˜ç´šä»£ç†ç²å–å™¨: {fetcher.name}")
    
    async def fetch_all_proxies(self, limit_per_fetcher: Optional[int] = None) -> List[ProxyNode]:
        """å¾æ‰€æœ‰å•Ÿç”¨çš„ç²å–å™¨ç²å–ä»£ç†"""
        all_proxies = []
        
        for fetcher in self.fetchers:
            if not fetcher.enabled:
                continue
            
            try:
                logger.info(f"ğŸ”„ é–‹å§‹å¾ {fetcher.name} ç²å–ä»£ç†...")
                proxies = await fetcher.fetch_proxies(limit_per_fetcher)
                all_proxies.extend(proxies)
                logger.info(f"âœ… {fetcher.name} ç²å–åˆ° {len(proxies)} å€‹ä»£ç†")
                
            except Exception as e:
                logger.error(f"âŒ {fetcher.name} ç²å–å¤±æ•—: {e}")
                fetcher.fetch_errors += 1
        
        # å…¨å±€å»é‡
        unique_proxies = self._global_deduplicate(all_proxies)
        
        logger.info(f"ğŸ¯ ç¸½å…±ç²å–åˆ° {len(unique_proxies)} å€‹å”¯ä¸€ä»£ç†")
        return unique_proxies
    
    def _global_deduplicate(self, proxies: List[ProxyNode]) -> List[ProxyNode]:
        """å…¨å±€ä»£ç†å»é‡"""
        seen = set()
        unique_proxies = []
        
        for proxy in proxies:
            key = f"{proxy.host}:{proxy.port}"
            if key not in seen:
                seen.add(key)
                unique_proxies.append(proxy)
        
        return unique_proxies
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """ç²å–ç¶œåˆçµ±è¨ˆä¿¡æ¯"""
        total_fetchers = len(self.fetchers)
        enabled_fetchers = len([f for f in self.fetchers if f.enabled])
        total_fetched = sum(f.total_fetched for f in self.fetchers)
        total_errors = sum(f.fetch_errors for f in self.fetchers)
        
        return {
            "total_fetchers": total_fetchers,
            "enabled_fetchers": enabled_fetchers,
            "total_proxies_fetched": total_fetched,
            "total_fetch_errors": total_errors,
            "success_rate": (total_fetched / (total_fetched + total_errors)) * 100 if (total_fetched + total_errors) > 0 else 0,
            "fetchers": [f.get_stats() for f in self.fetchers]
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        return {
            'total_fetchers': len(self.fetchers),
            'enabled_fetchers': len([f for f in self.fetchers if f.enabled]),
            'total_proxies_fetched': sum(f.total_fetched for f in self.fetchers),
            'total_fetch_errors': sum(f.fetch_errors for f in self.fetchers),
            'fetchers': [f.get_stats() for f in self.fetchers]
        }
    
    async def close(self):
        """é—œé–‰æ‰€æœ‰ç²å–å™¨"""
        for fetcher in self.fetchers:
            if hasattr(fetcher, 'close'):
                await fetcher.close()
        
        logger.info("âœ… æ‰€æœ‰é«˜ç´šä»£ç†ç²å–å™¨å·²é—œé–‰")