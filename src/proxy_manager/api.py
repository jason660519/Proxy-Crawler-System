"""ä»£ç†ç®¡ç†å™¨ FastAPI æœå‹™æ¥å£

æä¾› REST API æ¥å£ä¾›å¤–éƒ¨ä½¿ç”¨ï¼š
- ä»£ç†ç²å–æ¥å£
- çµ±è¨ˆä¿¡æ¯æ¥å£
- ç®¡ç†æ“ä½œæ¥å£
- å¥åº·æª¢æŸ¥æ¥å£
"""

import asyncio
import logging
from typing import List, Optional, Dict, Any, Union
from datetime import datetime, timedelta
from pathlib import Path

from fastapi import FastAPI, HTTPException, Depends, Query, BackgroundTasks, Request, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel, Field
import redis.asyncio as aioredis
from src.config.settings import settings
import uvicorn
from prometheus_client import Counter, Gauge, Histogram, generate_latest, CONTENT_TYPE_LATEST
from time import perf_counter
from collections import defaultdict, deque

# å°å…¥ ETL API
try:
    from ..etl.etl_api import etl_app
    ETL_AVAILABLE = True
except ImportError:
    ETL_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("âš ï¸ ETL API æ¨¡çµ„ä¸å¯ç”¨ï¼ŒæŸäº›åŠŸèƒ½å°‡è¢«ç¦ç”¨")

from .manager import ProxyManager, ProxyManagerConfig
from .models import ProxyNode, ProxyStatus, ProxyAnonymity, ProxyProtocol, ProxyFilter
from .pools import PoolType
from .validators import ProxyValidator, ValidationResult
from .database_service import get_database_service, cleanup_database_service
from database_config import DatabaseConfig

logger = logging.getLogger(__name__)

# Pydantic æ¨¡å‹å®šç¾©
class ProxyResponse(BaseModel):
    """ä»£ç†éŸ¿æ‡‰æ¨¡å‹"""
    success: bool
    message: str
    data: Optional[Any] = None


class ProxyNodeResponse(BaseModel):
    """å–®å€‹ä»£ç†ç¯€é»éŸ¿æ‡‰æ¨¡å‹"""
    host: str
    port: int
    protocol: str
    anonymity: str
    country: Optional[str] = None
    region: Optional[str] = None
    city: Optional[str] = None
    score: float
    response_time_ms: Optional[int] = None
    last_checked: Optional[datetime] = None
    
    @classmethod
    def from_proxy_node(cls, proxy: ProxyNode) -> "ProxyNodeResponse":
        return cls(
            host=proxy.host,
            port=proxy.port,
            protocol=proxy.protocol.value,
            anonymity=proxy.anonymity.value,
            country=proxy.country,
            region=proxy.region,
            city=proxy.city,
            score=proxy.score,
            response_time_ms=proxy.metrics.response_time_ms,
            last_checked=proxy.last_checked
        )


class ProxyFilterRequest(BaseModel):
    """ä»£ç†ç¯©é¸è«‹æ±‚æ¨¡å‹"""
    protocols: Optional[List[str]] = None
    anonymity_levels: Optional[List[str]] = None
    countries: Optional[List[str]] = None
    min_score: Optional[float] = None
    max_response_time: Optional[int] = None
    page: int = Field(default=1, ge=1, description="é ç¢¼")
    page_size: int = Field(default=50, ge=1, le=100, description="æ¯é å¤§å°")
    order_by: str = Field(default="score", description="æ’åºå­—æ®µ")
    order_desc: bool = Field(default=True, description="æ˜¯å¦é™åº")
    
    @property
    def filter(self) -> ProxyFilter:
        """ç²å– ProxyFilter å°è±¡"""
        protocols = None
        if self.protocols:
            protocols = [ProxyProtocol(p) for p in self.protocols if p in [e.value for e in ProxyProtocol]]
        
        anonymity_levels = None
        if self.anonymity_levels:
            anonymity_levels = [ProxyAnonymity(a) for a in self.anonymity_levels if a in [e.value for e in ProxyAnonymity]]
        
        return ProxyFilter(
            protocols=protocols,
            anonymity_levels=anonymity_levels,
            countries=self.countries,
            min_score=self.min_score,
            max_response_time=self.max_response_time
        )
    
    def to_proxy_filter(self) -> ProxyFilter:
        """è½‰æ›ç‚º ProxyFilter å°è±¡ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
        return self.filter


class StatsResponse(BaseModel):
    """çµ±è¨ˆä¿¡æ¯éŸ¿æ‡‰æ¨¡å‹"""
    total_proxies: int
    total_active_proxies: int
    pool_distribution: Dict[str, int]
    overall_success_rate: float
    last_updated: str
    manager_stats: Dict[str, Any]
    pool_details: Dict[str, Any]


class HealthResponse(BaseModel):
    """å¥åº·æª¢æŸ¥éŸ¿æ‡‰æ¨¡å‹"""
    status: str
    timestamp: datetime
    uptime_seconds: Optional[float] = None
    total_proxies: int
    active_proxies: int
    version: str = "1.0.0"


class FetchRequest(BaseModel):
    """ç²å–ä»£ç†è«‹æ±‚æ¨¡å‹"""
    sources: Optional[List[str]] = None
    validate: bool = True


class ExportRequest(BaseModel):
    """å°å‡ºè«‹æ±‚æ¨¡å‹"""
    format_type: str = Field(default="json", pattern="^(json|txt|csv)$")
    pool_types: Optional[List[str]] = None


class ImportRequest(BaseModel):
    """å°å…¥è«‹æ±‚æ¨¡å‹"""
    file_path: str = Field(description="è¦å°å…¥çš„æ–‡ä»¶è·¯å¾‘")
    validate_proxies: bool = Field(default=True, description="æ˜¯å¦é©—è­‰å°å…¥çš„ä»£ç†")


# Prometheus metrics
REQUEST_COUNT = Counter("proxy_api_requests_total", "Total API requests", ["endpoint", "method", "status"])
POOL_ACTIVE = Gauge("proxy_pool_active", "Active proxies in pool")
POOL_TOTAL = Gauge("proxy_pool_total", "Total proxies in pool")
REQUEST_LATENCY = Histogram(
    "proxy_api_request_duration_seconds",
    "Request duration in seconds",
    ["endpoint", "method", "status"],
    buckets=(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10)
)


# In-process lightweight rollup for API trends/summary
ROLLUP_LOCK = asyncio.Lock()
REQUEST_ROLLUP: Dict[str, Dict[str, Any]] = defaultdict(lambda: {"count": 0, "error": 0, "duration_sum": 0.0})
PER_MINUTE: Dict[datetime, Dict[str, float]] = {}
MINUTE_KEYS: deque = deque()
MAX_MINUTES: int = 24 * 60  # keep last 24h


# ç°¡å–®çš„ API Key é©—è­‰ä¾è³´
async def require_api_key(x_api_key: Optional[str] = Header(default=None)):
    if not settings.api_key_enabled:
        return
    if not x_api_key or x_api_key not in settings.api_keys:
        raise HTTPException(status_code=401, detail="Invalid or missing API key")


# å…¨å±€ä»£ç†ç®¡ç†å™¨å¯¦ä¾‹
proxy_manager: Optional[ProxyManager] = None


def get_proxy_manager() -> ProxyManager:
    """ç²å–ä»£ç†ç®¡ç†å™¨å¯¦ä¾‹"""
    if proxy_manager is None:
        raise HTTPException(status_code=503, detail="ä»£ç†ç®¡ç†å™¨æœªåˆå§‹åŒ–")
    return proxy_manager


# å‰µå»º FastAPI æ‡‰ç”¨
app = FastAPI(
    title="ä»£ç†ç®¡ç†å™¨ API",
    description="æä¾›ä»£ç†ç²å–ã€ç®¡ç†å’Œçµ±è¨ˆåŠŸèƒ½çš„ REST API æœå‹™",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)
# å…¨åŸŸä¸­ä»‹å±¤ï¼šè«‹æ±‚è¨ˆæ•¸èˆ‡å»¶é²
@app.middleware("http")
async def metrics_middleware(request, call_next):
    if request.url.path == "/metrics":
        return await call_next(request)
    start = perf_counter()
    status_code = "200"
    try:
        response = await call_next(request)
        status_code = str(getattr(response, "status_code", 200))
        return response
    except Exception:
        status_code = "500"
        raise
    finally:
        try:
            endpoint = request.url.path
            method = request.method
            REQUEST_COUNT.labels(endpoint=endpoint, method=method, status=status_code).inc()
            REQUEST_LATENCY.labels(endpoint=endpoint, method=method, status=status_code).observe(perf_counter() - start)
            # In-process aggregations
            duration = max(0.0, perf_counter() - start)
            is_error = 1 if int(status_code) >= 400 else 0
            key = f"{endpoint}|{method}"
            minute_key = datetime.now().replace(second=0, microsecond=0)
            # Update rollups
            if not ROLLUP_LOCK.locked():
                # Best-effort without awaiting lock to avoid latency; fallback on simple updates
                pass
            async def _update():
                REQUEST_ROLLUP[key]["count"] += 1
                REQUEST_ROLLUP[key]["error"] += is_error
                REQUEST_ROLLUP[key]["duration_sum"] += duration
                bucket = PER_MINUTE.get(minute_key)
                if not bucket:
                    PER_MINUTE[minute_key] = {"count": 0, "error": 0, "duration_sum": 0.0}
                    MINUTE_KEYS.append(minute_key)
                PER_MINUTE[minute_key]["count"] += 1
                PER_MINUTE[minute_key]["error"] += is_error
                PER_MINUTE[minute_key]["duration_sum"] += duration
                # Trim old minutes
                while len(MINUTE_KEYS) > MAX_MINUTES:
                    old = MINUTE_KEYS.popleft()
                    PER_MINUTE.pop(old, None)
            try:
                # Fire-and-forget; slight race is acceptable for metrics
                asyncio.create_task(_update())
            except Exception:
                pass
        except Exception:
            pass


# è¨­ç½®æ¨¡æ¿å’Œéœæ…‹æ–‡ä»¶
templates = Jinja2Templates(directory=str(Path(__file__).parent / "templates"))

# æ›è¼‰éœæ…‹æ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
# app.mount("/static", StaticFiles(directory="static"), name="static")

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_allow_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ›è¼‰ ETL API å­æ‡‰ç”¨
if ETL_AVAILABLE:
    app.mount("/etl", etl_app, name="etl")
    logger.info("âœ… ETL API å·²æ›è¼‰åˆ° /etl è·¯å¾‘")
else:
    logger.warning("âš ï¸ ETL API ä¸å¯ç”¨ï¼Œè·³éæ›è¼‰")

# æ›è¼‰ URL2Parquet è·¯ç”±
try:
    from src.url2parquet.api.router import router as url2parquet_router
    app.include_router(url2parquet_router)
    logger.info("âœ… URL2Parquet è·¯ç”±å·²æ›è¼‰åˆ° /api/url2parquet")
except Exception as e:
    logger.warning(f"âš ï¸ ç„¡æ³•æ›è¼‰ URL2Parquet è·¯ç”±: {e}")

# æ›è¼‰ä»£ç†ç®¡ç† API è·¯ç”±
try:
    from src.api.proxy_management_api import router as proxy_management_router
    app.include_router(proxy_management_router)
    logger.info("âœ… ä»£ç†ç®¡ç† API è·¯ç”±å·²æ›è¼‰åˆ° /api")
except Exception as e:
    logger.warning(f"âš ï¸ ç„¡æ³•æ›è¼‰ä»£ç†ç®¡ç† API è·¯ç”±: {e}")


@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶"""
    global proxy_manager
    
    print("[DEBUG] startup_event è¢«èª¿ç”¨")
    logger.info("ğŸš€ å•Ÿå‹•ä»£ç†ç®¡ç†å™¨ API æœå‹™...")
    
    try:
        print("[DEBUG] é–‹å§‹å‰µå»ºé…ç½®")
        # å‰µå»ºé…ç½®
        config = ProxyManagerConfig()
        print("[DEBUG] é…ç½®å‰µå»ºæˆåŠŸ")
        
        print("[DEBUG] é–‹å§‹å‰µå»ºä»£ç†ç®¡ç†å™¨")
        # å‰µå»ºä¸¦å•Ÿå‹•ä»£ç†ç®¡ç†å™¨
        proxy_manager = ProxyManager(config)
        print("[DEBUG] ä»£ç†ç®¡ç†å™¨å‰µå»ºæˆåŠŸï¼Œé–‹å§‹å•Ÿå‹•")
        
        await proxy_manager.start()
        print("[DEBUG] ä»£ç†ç®¡ç†å™¨å•Ÿå‹•æˆåŠŸ")
        
        logger.info("âœ… ä»£ç†ç®¡ç†å™¨ API æœå‹™å•Ÿå‹•æˆåŠŸ")
        
    except Exception as e:
        print(f"[DEBUG] å•Ÿå‹•éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        logger.error(f"âŒ å•Ÿå‹•å¤±æ•—: {e}")
        import traceback
        error_details = traceback.format_exc()
        print(f"[DEBUG] è©³ç´°éŒ¯èª¤ä¿¡æ¯: {error_details}")
        logger.error(f"è©³ç´°éŒ¯èª¤ä¿¡æ¯: {error_details}")
        # ä¸è¦ raiseï¼Œè®“æœå‹™ç¹¼çºŒé‹è¡Œ
        proxy_manager = None


@app.on_event("shutdown")
async def shutdown_event():
    """æ‡‰ç”¨é—œé–‰äº‹ä»¶"""
    global proxy_manager
    
    logger.info("ğŸ›‘ é—œé–‰ä»£ç†ç®¡ç†å™¨ API æœå‹™...")
    
    if proxy_manager:
        await proxy_manager.stop()
        proxy_manager = None
    
    # æ¸…ç†æ•¸æ“šåº«æœå‹™
    await cleanup_database_service()
    
    logger.info("âœ… ä»£ç†ç®¡ç†å™¨ API æœå‹™å·²é—œé–‰")


# API è·¯ç”±å®šç¾©

@app.get("/", response_class=HTMLResponse, summary="Webç®¡ç†ç•Œé¢")
async def web_interface(request: Request):
    """Webç®¡ç†ç•Œé¢"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/api", summary="APIæ ¹è·¯å¾‘")
async def api_root():
    """APIæ ¹è·¯å¾‘ï¼Œè¿”å› API åŸºæœ¬ä¿¡æ¯"""
    return {
        "name": "ä»£ç†ç®¡ç†å™¨ API",
        "version": "1.0.0",
        "description": "æä¾›ä»£ç†ç²å–ã€ç®¡ç†å’Œçµ±è¨ˆåŠŸèƒ½çš„ REST API æœå‹™",
        "docs_url": "/api/docs",
        "health_check": "/api/health",
        "web_interface": "/"
    }


@app.get("/health", response_model=HealthResponse, summary="å¥åº·æª¢æŸ¥")
async def health_check(manager: ProxyManager = Depends(get_proxy_manager)):
    """å¥åº·æª¢æŸ¥æ¥å£ï¼ˆåŒ…å« DB èˆ‡ Redis é€£ç·šæª¢æŸ¥ï¼‰"""
    stats = manager.get_stats()

    uptime = None
    if stats['manager_stats']['start_time']:
        start_time = stats['manager_stats']['start_time']
        uptime = (datetime.now() - start_time).total_seconds()

    # é©—è­‰è³‡æ–™åº«
    db_ok = False
    try:
        db_service = await get_database_service()
        db_ok = await db_service.ping()
    except Exception as e:
        logger.error(f"DB health check failed: {e}")
        db_ok = False

    # é©—è­‰ Redis
    redis_ok = False
    try:
        redis_url = DatabaseConfig().get_redis_url()
        redis_client = aioredis.from_url(redis_url, decode_responses=True)
        redis_ok = await redis_client.ping()
        await redis_client.close()
    except Exception as e:
        logger.error(f"Redis health check failed: {e}")
        redis_ok = False

    running = stats['status']['running']
    if running and db_ok and redis_ok:
        overall = "healthy"
    elif running or db_ok or redis_ok:
        overall = "degraded"
    else:
        overall = "unhealthy"

    # æ›´æ–° metrics
    try:
        stats_summary = stats['pool_summary']
        POOL_TOTAL.set(stats_summary['total_proxies'])
        POOL_ACTIVE.set(stats_summary['total_active_proxies'])
    except Exception:
        pass

    return HealthResponse(
        status=overall,
        timestamp=datetime.now(),
        uptime_seconds=uptime,
        total_proxies=stats['pool_summary']['total_proxies'],
        active_proxies=stats['pool_summary']['total_active_proxies']
    )


@app.get("/proxy", response_model=ProxyResponse, summary="ç²å–å–®å€‹ä»£ç†")
async def get_proxy(
    protocol: Optional[str] = Query(None, description="å”è­°é¡å‹ (http, https, socks4, socks5)"),
    anonymity: Optional[str] = Query(None, description="åŒ¿ååº¦ (transparent, anonymous, elite)"),
    country: Optional[str] = Query(None, description="åœ‹å®¶ä»£ç¢¼"),
    min_score: Optional[float] = Query(None, ge=0, le=1, description="æœ€ä½åˆ†æ•¸"),
    max_response_time: Optional[int] = Query(None, gt=0, description="æœ€å¤§éŸ¿æ‡‰æ™‚é–“(æ¯«ç§’)"),
    pool_preference: Optional[str] = Query("hot,warm,cold", description="æ± å„ªå…ˆç´š (é€—è™Ÿåˆ†éš”)"),
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """ç²å–å–®å€‹ä»£ç†"""
    start = perf_counter()
    status_code = "200"
    try:
        # æ§‹å»ºç¯©é¸æ¢ä»¶
        filter_criteria = None
        if any([protocol, anonymity, country, min_score, max_response_time]):
            protocols = [ProxyProtocol(protocol)] if protocol else None
            anonymity_levels = [ProxyAnonymity(anonymity)] if anonymity else None
            countries = [country] if country else None
            
            filter_criteria = ProxyFilter(
                protocols=protocols,
                anonymity_levels=anonymity_levels,
                countries=countries,
                min_score=min_score,
                max_response_time=max_response_time
            )
        
        # è§£ææ± å„ªå…ˆç´š
        pool_types = []
        if pool_preference:
            for pool_name in pool_preference.split(','):
                pool_name = pool_name.strip().lower()
                if pool_name == 'hot':
                    pool_types.append(PoolType.HOT)
                elif pool_name == 'warm':
                    pool_types.append(PoolType.WARM)
                elif pool_name == 'cold':
                    pool_types.append(PoolType.COLD)
        
        if not pool_types:
            pool_types = [PoolType.HOT, PoolType.WARM, PoolType.COLD]
        
        # ç²å–ä»£ç†
        proxy = await manager.get_proxy(filter_criteria, pool_types)
        
        if not proxy:
            raise HTTPException(status_code=404, detail="æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„ä»£ç†")
        
        return ProxyResponse.from_proxy_node(proxy)
        
    except ValueError as e:
        status_code = "400"
        raise HTTPException(status_code=400, detail=f"åƒæ•¸éŒ¯èª¤: {e}")
    except Exception as e:
        status_code = "500"
        logger.error(f"âŒ ç²å–ä»£ç†å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")
    finally:
        try:
            endpoint = "/api/proxy"
            REQUEST_COUNT.labels(endpoint=endpoint, method="GET", status=status_code).inc()
            REQUEST_LATENCY.labels(endpoint=endpoint, method="GET", status=status_code).observe(perf_counter() - start)
        except Exception:
            pass


@app.get("/proxies", response_model=List[ProxyResponse], summary="æ‰¹é‡ç²å–ä»£ç†")
async def get_proxies(
    count: int = Query(10, ge=1, le=100, description="ç²å–æ•¸é‡"),
    protocol: Optional[str] = Query(None, description="å”è­°é¡å‹"),
    anonymity: Optional[str] = Query(None, description="åŒ¿ååº¦"),
    country: Optional[str] = Query(None, description="åœ‹å®¶ä»£ç¢¼"),
    min_score: Optional[float] = Query(None, ge=0, le=1, description="æœ€ä½åˆ†æ•¸"),
    max_response_time: Optional[int] = Query(None, gt=0, description="æœ€å¤§éŸ¿æ‡‰æ™‚é–“(æ¯«ç§’)"),
    pool_preference: Optional[str] = Query("hot,warm,cold", description="æ± å„ªå…ˆç´š"),
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """æ‰¹é‡ç²å–ä»£ç†"""
    try:
        # æ§‹å»ºç¯©é¸æ¢ä»¶ï¼ˆèˆ‡å–®å€‹ä»£ç†æ¥å£ç›¸åŒé‚è¼¯ï¼‰
        filter_criteria = None
        if any([protocol, anonymity, country, min_score, max_response_time]):
            protocols = [ProxyProtocol(protocol)] if protocol else None
            anonymity_levels = [ProxyAnonymity(anonymity)] if anonymity else None
            countries = [country] if country else None
            
            filter_criteria = ProxyFilter(
                protocols=protocols,
                anonymity_levels=anonymity_levels,
                countries=countries,
                min_score=min_score,
                max_response_time=max_response_time
            )
        
        # è§£ææ± å„ªå…ˆç´š
        pool_types = []
        if pool_preference:
            for pool_name in pool_preference.split(','):
                pool_name = pool_name.strip().lower()
                if pool_name == 'hot':
                    pool_types.append(PoolType.HOT)
                elif pool_name == 'warm':
                    pool_types.append(PoolType.WARM)
                elif pool_name == 'cold':
                    pool_types.append(PoolType.COLD)
        
        if not pool_types:
            pool_types = [PoolType.HOT, PoolType.WARM, PoolType.COLD]
        
        # æ‰¹é‡ç²å–ä»£ç†
        proxies = await manager.get_proxies(count, filter_criteria, pool_types)
        
        return [ProxyResponse.from_proxy_node(proxy) for proxy in proxies]
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"åƒæ•¸éŒ¯èª¤: {e}")
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡ç²å–ä»£ç†å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")


@app.post("/proxies/filter", response_model=List[ProxyResponse], summary="ä½¿ç”¨è¤‡é›œæ¢ä»¶ç¯©é¸ä»£ç†")
async def filter_proxies(
    filter_request: ProxyFilterRequest,
    count: int = Query(10, ge=1, le=100, description="ç²å–æ•¸é‡"),
    pool_preference: Optional[str] = Query("hot,warm,cold", description="æ± å„ªå…ˆç´š"),
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """ä½¿ç”¨è¤‡é›œæ¢ä»¶ç¯©é¸ä»£ç†"""
    try:
        # è½‰æ›ç¯©é¸æ¢ä»¶
        filter_criteria = filter_request.to_proxy_filter()
        
        # è§£ææ± å„ªå…ˆç´š
        pool_types = []
        if pool_preference:
            for pool_name in pool_preference.split(','):
                pool_name = pool_name.strip().lower()
                if pool_name == 'hot':
                    pool_types.append(PoolType.HOT)
                elif pool_name == 'warm':
                    pool_types.append(PoolType.WARM)
                elif pool_name == 'cold':
                    pool_types.append(PoolType.COLD)
        
        if not pool_types:
            pool_types = [PoolType.HOT, PoolType.WARM, PoolType.COLD]
        
        # ç²å–ä»£ç†
        proxies = await manager.get_proxies(count, filter_criteria, pool_types)
        
        return [ProxyResponse.from_proxy_node(proxy) for proxy in proxies]
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"åƒæ•¸éŒ¯èª¤: {e}")
    except Exception as e:
        logger.error(f"âŒ ç¯©é¸ä»£ç†å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")


@app.get("/stats", response_model=StatsResponse, summary="ç²å–çµ±è¨ˆä¿¡æ¯")
async def get_stats(manager: ProxyManager = Depends(get_proxy_manager)):
    """ç²å–çµ±è¨ˆä¿¡æ¯"""
    try:
        stats = manager.get_stats()
        
        return StatsResponse(
            total_proxies=stats['pool_summary']['total_proxies'],
            total_active_proxies=stats['pool_summary']['total_active_proxies'],
            pool_distribution=stats['pool_summary']['pool_distribution'],
            overall_success_rate=stats['pool_summary']['overall_success_rate'],
            last_updated=stats['pool_summary']['last_updated'],
            manager_stats=stats['manager_stats'],
            pool_details=stats['pool_details']
        )
        
    except Exception as e:
        logger.error(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")


class MetricsSummaryResponse(BaseModel):
    total_requests: int
    error_rate: float
    avg_latency_ms: float
    per_endpoint: Dict[str, Dict[str, float]]
    pool: Dict[str, int]


# åˆªé™¤é‡è¤‡çš„ metrics/summary ç«¯é»å®šç¾©ï¼Œä¿ç•™æ›´è©³ç´°çš„ç‰ˆæœ¬


class MetricsTrendsPoint(BaseModel):
    timestamp: datetime
    count: int
    error: int
    avg_latency_ms: float


# åˆªé™¤é‡è¤‡çš„ metrics/trends ç«¯é»å®šç¾©ï¼Œä¿ç•™æ›´è©³ç´°çš„ç‰ˆæœ¬

@app.post("/fetch", summary="æ‰‹å‹•ç²å–ä»£ç†")
async def fetch_proxies(
    fetch_request: FetchRequest,
    background_tasks: BackgroundTasks,
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """æ‰‹å‹•è§¸ç™¼ä»£ç†ç²å–"""
    try:
        # åœ¨å¾Œå°åŸ·è¡Œç²å–ä»»å‹™
        background_tasks.add_task(
            manager.fetch_proxies,
            fetch_request.sources
        )
        
        return {
            "message": "ä»£ç†ç²å–ä»»å‹™å·²å•Ÿå‹•",
            "sources": fetch_request.sources,
            "validate": fetch_request.validate,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ å•Ÿå‹•ç²å–ä»»å‹™å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")


@app.post("/validate", summary="æ‰‹å‹•é©—è­‰ä»£ç†æ± ", dependencies=[Depends(require_api_key)])
async def validate_pools(
    background_tasks: BackgroundTasks,
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """æ‰‹å‹•è§¸ç™¼ä»£ç†æ± é©—è­‰"""
    try:
        # åœ¨å¾Œå°åŸ·è¡Œé©—è­‰ä»»å‹™
        background_tasks.add_task(manager.validate_pools)
        
        return {
            "message": "ä»£ç†æ± é©—è­‰ä»»å‹™å·²å•Ÿå‹•",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ å•Ÿå‹•é©—è­‰ä»»å‹™å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")


@app.post("/cleanup", summary="æ‰‹å‹•æ¸…ç†ä»£ç†æ± ", dependencies=[Depends(require_api_key)])
async def cleanup_pools(
    background_tasks: BackgroundTasks,
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """æ‰‹å‹•è§¸ç™¼ä»£ç†æ± æ¸…ç†"""
    try:
        # åœ¨å¾Œå°åŸ·è¡Œæ¸…ç†ä»»å‹™
        background_tasks.add_task(manager.cleanup_pools)
        
        return {
            "message": "ä»£ç†æ± æ¸…ç†ä»»å‹™å·²å•Ÿå‹•",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ å•Ÿå‹•æ¸…ç†ä»»å‹™å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")


@app.post("/export", summary="å°å‡ºä»£ç†", dependencies=[Depends(require_api_key)])
async def export_proxies(
    export_request: ExportRequest,
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """å°å‡ºä»£ç†åˆ°æ–‡ä»¶"""
    try:
        # è§£ææ± é¡å‹
        pool_types = []
        if export_request.pool_types:
            for pool_name in export_request.pool_types:
                pool_name = pool_name.strip().lower()
                if pool_name == 'hot':
                    pool_types.append(PoolType.HOT)
                elif pool_name == 'warm':
                    pool_types.append(PoolType.WARM)
                elif pool_name == 'cold':
                    pool_types.append(PoolType.COLD)
        
        if not pool_types:
            pool_types = [PoolType.HOT, PoolType.WARM, PoolType.COLD]
        
        # ç”Ÿæˆæ–‡ä»¶å
        if not export_request.filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"proxies_{timestamp}.{export_request.format_type}"
        else:
            filename = export_request.filename
        
        file_path = Path("data/exports") / filename
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å°å‡ºä»£ç†
        count = await manager.export_proxies(
            file_path,
            export_request.format_type,
            pool_types
        )
        
        return {
            "message": "ä»£ç†å°å‡ºæˆåŠŸ",
            "filename": filename,
            "format": export_request.format_type,
            "count": count,
            "download_url": f"/download/{filename}",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ å°å‡ºä»£ç†å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"å°å‡ºå¤±æ•—: {e}")


@app.post("/import", summary="å°å…¥ä»£ç†", dependencies=[Depends(require_api_key)])
async def import_proxies_endpoint(
    import_request: ImportRequest,
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """å¾æ–‡ä»¶å°å…¥ä»£ç†"""
    try:
        file_path = Path(import_request.file_path)
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not file_path.exists():
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {import_request.file_path}")
        
        # æª¢æŸ¥æ–‡ä»¶æ ¼å¼
        if file_path.suffix.lower() not in ['.json', '.txt', '.csv']:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œåƒ…æ”¯æŒ .json, .txt, .csv")
        
        # å°å…¥ä»£ç†
        count = await manager.import_proxies(
            file_path,
            validate=import_request.validate_proxies
        )
        
        return {
            "message": "ä»£ç†å°å…¥æˆåŠŸ",
            "file_path": str(file_path),
            "count": count,
            "validated": import_request.validate_proxies,
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å°å…¥ä»£ç†å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"å°å…¥å¤±æ•—: {e}")


@app.get("/download/{filename}", summary="ä¸‹è¼‰å°å‡ºæ–‡ä»¶")
async def download_file(filename: str):
    """ä¸‹è¼‰å°å‡ºçš„æ–‡ä»¶"""
    file_path = Path("data/exports") / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type='application/octet-stream'
    )


@app.get("/proxies/{proxy_id}", response_model=ProxyResponse, summary="ç²å–å–®å€‹ä»£ç†")
async def get_proxy_by_id(proxy_id: str):
    """æ ¹æ“šIDç²å–å–®å€‹ä»£ç†"""
    try:
        db_service = await get_database_service()
        proxy = await db_service.get_proxy_by_id(proxy_id)
        
        if proxy:
            return ProxyResponse(
                success=True,
                message="ä»£ç†ç²å–æˆåŠŸ",
                data=ProxyNodeResponse.from_proxy_node(proxy).dict()
            )
        else:
            return ProxyResponse(
                success=False,
                message="ä»£ç†ä¸å­˜åœ¨",
                data=None
            )
    except Exception as e:
        logger.error(f"ç²å–ä»£ç†å¤±æ•—: {e}")
        return ProxyResponse(
            success=False,
            message=f"ç²å–ä»£ç†å¤±æ•—: {str(e)}",
            data=None
        )


@app.post("/proxies/batch", response_model=ProxyResponse, summary="æ‰¹é‡ç²å–ä»£ç†")
async def get_proxies_batch(
    filter_request: ProxyFilterRequest,
    count: int = Query(10, ge=1, le=100, description="ç²å–æ•¸é‡"),
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """æ‰¹é‡ç²å–ä»£ç†"""
    try:
        # è½‰æ›ç¯©é¸æ¢ä»¶
        filter_criteria = filter_request.to_proxy_filter()
        
        # ç²å–ä»£ç†
        proxies = await manager.get_proxies(count, filter_criteria)
        
        return ProxyResponse(
            success=True,
            message=f"æˆåŠŸç²å– {len(proxies)} å€‹ä»£ç†",
            data=[ProxyNodeResponse.from_proxy_node(proxy).dict() for proxy in proxies]
        )
        
    except ValueError as e:
        return ProxyResponse(
            success=False,
            message=f"åƒæ•¸éŒ¯èª¤: {e}",
            data=None
        )
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡ç²å–ä»£ç†å¤±æ•—: {e}")
        return ProxyResponse(
            success=False,
            message="æ‰¹é‡ç²å–ä»£ç†å¤±æ•—",
            data=None
        )


@app.post("/database/proxies", response_model=ProxyResponse, summary="å¾æ•¸æ“šåº«ç²å–ä»£ç†")
async def get_database_proxies(
    page: int = Query(1, ge=1, description="é ç¢¼"),
    page_size: int = Query(20, ge=1, le=100, description="æ¯é æ•¸é‡"),
    protocol: Optional[str] = Query(None, description="å”è­°é¡å‹"),
    anonymity: Optional[str] = Query(None, description="åŒ¿ååº¦"),
    country: Optional[str] = Query(None, description="åœ‹å®¶ä»£ç¢¼"),
    min_score: Optional[float] = Query(None, ge=0, le=1, description="æœ€ä½åˆ†æ•¸"),
    max_response_time: Optional[int] = Query(None, gt=0, description="æœ€å¤§éŸ¿æ‡‰æ™‚é–“(æ¯«ç§’)"),
    order_by: str = Query("score", description="æ’åºå­—æ®µ"),
    order_desc: bool = Query(True, description="æ˜¯å¦é™åºæ’åˆ—")
):
    """ç›´æ¥å¾æ•¸æ“šåº«ç²å–ä»£ç†æ•¸æ“š"""
    try:
        db_service = await get_database_service()
        
        # æ§‹å»ºç¯©é¸æ¢ä»¶
        filter_criteria = None
        if any([protocol, anonymity, country, min_score, max_response_time]):
            protocols = [ProxyProtocol(protocol)] if protocol else None
            anonymity_levels = [ProxyAnonymity(anonymity)] if anonymity else None
            countries = [country] if country else None
            
            filter_criteria = ProxyFilter(
                protocols=protocols,
                anonymity_levels=anonymity_levels,
                countries=countries,
                min_score=min_score,
                max_response_time=max_response_time
            )
        
        # å¾æ•¸æ“šåº«ç²å–ä»£ç†
        result = await db_service.get_proxies(
            filter_criteria=filter_criteria,
            page=page,
            page_size=page_size,
            order_by=order_by,
            order_desc=order_desc
        )
        
        return ProxyResponse(
            success=True,
            message=f"æˆåŠŸç²å– {len(result.proxies)} å€‹ä»£ç†",
            data={
                "proxies": [ProxyNodeResponse.from_proxy_node(proxy).dict() for proxy in result.proxies],
                "pagination": {
                    "page": result.page,
                    "page_size": result.page_size,
                    "total_count": result.total_count,
                    "has_next": result.has_next,
                    "has_prev": result.has_prev
                }
            }
        )
        
    except ValueError as e:
        return ProxyResponse(
            success=False,
            message=f"åƒæ•¸éŒ¯èª¤: {e}",
            data=None
        )
    except Exception as e:
        logger.error(f"å¾æ•¸æ“šåº«ç²å–ä»£ç†å¤±æ•—: {e}")
        return ProxyResponse(
            success=False,
            message=f"å¾æ•¸æ“šåº«ç²å–ä»£ç†å¤±æ•—: {str(e)}",
            data=None
        )


@app.get("/stats/detailed", summary="ç²å–è©³ç´°çµ±è¨ˆä¿¡æ¯")
async def get_detailed_stats(manager: ProxyManager = Depends(get_proxy_manager)):
    """ç²å–è©³ç´°çµ±è¨ˆä¿¡æ¯"""
    try:
        stats = manager.get_stats()
        
        return {
            "success": True,
            "data": {
                "total_proxies": stats['pool_summary']['total_proxies'],
                "total_active_proxies": stats['pool_summary']['total_active_proxies'],
                "pool_distribution": stats['pool_summary']['pool_distribution'],
                "overall_success_rate": stats['pool_summary']['overall_success_rate'],
                "last_updated": stats['pool_summary']['last_updated'],
                "manager_stats": stats['manager_stats'],
                "pool_details": stats['pool_details']
            },
            "message": "çµ±è¨ˆä¿¡æ¯ç²å–æˆåŠŸ",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ç²å–è©³ç´°çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")


@app.get("/pools", summary="ç²å–æ± è©³ç´°ä¿¡æ¯")
async def get_pools_info(manager: ProxyManager = Depends(get_proxy_manager)):
    """ç²å–æ‰€æœ‰æ± çš„è©³ç´°ä¿¡æ¯"""
    try:
        stats = manager.get_stats()
        return {
            "success": True,
            "data": {
                "pools": stats['pool_details'],
                "summary": stats['pool_summary']
            },
            "message": "æ± ä¿¡æ¯ç²å–æˆåŠŸ",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ç²å–æ± ä¿¡æ¯å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail="å…§éƒ¨æœå‹™å™¨éŒ¯èª¤")


@app.post("/etl/sync", summary="åŒæ­¥ä»£ç†æ•¸æ“šåˆ° ETL ç³»çµ±", dependencies=[Depends(require_api_key)])
async def sync_to_etl(
    background_tasks: BackgroundTasks,
    pool_types: Optional[str] = Query("hot,warm,cold", description="è¦åŒæ­¥çš„æ± é¡å‹"),
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """å°‡ä»£ç†ç®¡ç†å™¨ä¸­çš„æ•¸æ“šåŒæ­¥åˆ° ETL ç³»çµ±"""
    if not ETL_AVAILABLE:
        raise HTTPException(status_code=503, detail="ETL ç³»çµ±ä¸å¯ç”¨")
    
    try:
        # è§£ææ± é¡å‹
        pool_list = []
        if pool_types:
            for pool_name in pool_types.split(','):
                pool_name = pool_name.strip().lower()
                if pool_name in ['hot', 'warm', 'cold']:
                    pool_list.append(pool_name)
        
        if not pool_list:
            pool_list = ['hot', 'warm', 'cold']
        
        # åœ¨èƒŒæ™¯åŸ·è¡ŒåŒæ­¥ä»»å‹™
        background_tasks.add_task(_sync_data_to_etl, manager, pool_list)
        
        return {
            "message": "æ•¸æ“šåŒæ­¥ä»»å‹™å·²å•Ÿå‹•",
            "pool_types": pool_list,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ å•Ÿå‹•æ•¸æ“šåŒæ­¥å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"å•Ÿå‹•æ•¸æ“šåŒæ­¥å¤±æ•—: {e}")


@app.get("/etl/status", summary="ç²å– ETL ç³»çµ±ç‹€æ…‹")
async def get_etl_status():
    """ç²å– ETL ç³»çµ±çš„ç‹€æ…‹ä¿¡æ¯"""
    if not ETL_AVAILABLE:
        return {
            "available": False,
            "message": "ETL ç³»çµ±ä¸å¯ç”¨",
            "timestamp": datetime.now().isoformat()
        }
    
    try:
        # é€™è£¡å¯ä»¥æ·»åŠ å° ETL ç³»çµ±å¥åº·ç‹€æ…‹çš„æª¢æŸ¥
        return {
            "available": True,
            "status": "operational",
            "endpoints": {
                "jobs": "/etl/api/etl/jobs",
                "validation": "/etl/api/etl/validate",
                "monitoring": "/etl/api/etl/monitoring/metrics",
                "health": "/etl/api/etl/health"
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ç²å– ETL ç‹€æ…‹å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å– ETL ç‹€æ…‹å¤±æ•—: {e}")


@app.get("/metrics/summary", summary="ç²å–ç³»çµ±æŒ‡æ¨™æ‘˜è¦")
async def get_metrics_summary(manager: ProxyManager = Depends(get_proxy_manager)):
    """ç²å–ç³»çµ±é—œéµæŒ‡æ¨™çš„æ‘˜è¦ä¿¡æ¯ï¼Œä¾›å‰ç«¯å„€è¡¨æ¿ä½¿ç”¨"""
    try:
        stats = manager.get_stats()
        pool_summary = stats['pool_summary']
        manager_stats = stats['manager_stats']
        
        # è¨ˆç®—æˆåŠŸç‡
        total_requests = manager_stats.get('total_requests', 0)
        successful_requests = manager_stats.get('successful_requests', 0)
        success_rate = (successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        # è¨ˆç®—å¹³å‡éŸ¿æ‡‰æ™‚é–“
        avg_response_time = manager_stats.get('avg_response_time', 0)
        
        # ç²å–å„æ± çš„ä»£ç†æ•¸é‡
        hot_proxies = pool_summary.get('hot_pool_count', 0)
        warm_proxies = pool_summary.get('warm_pool_count', 0)
        cold_proxies = pool_summary.get('cold_pool_count', 0)
        blacklisted_proxies = pool_summary.get('blacklist_count', 0)
        
        return {
            "timestamp": datetime.now().isoformat(),
            "system_health": {
                "status": "healthy" if stats['status']['running'] else "stopped",
                "uptime_seconds": manager_stats.get('uptime_seconds', 0),
                "last_update": manager_stats.get('last_update', datetime.now()).isoformat()
            },
            "proxy_metrics": {
                "total_proxies": pool_summary['total_proxies'],
                "active_proxies": pool_summary['total_active_proxies'],
                "hot_pool": hot_proxies,
                "warm_pool": warm_proxies,
                "cold_pool": cold_proxies,
                "blacklisted": blacklisted_proxies
            },
            "performance_metrics": {
                "success_rate": round(success_rate, 2),
                "avg_response_time_ms": round(avg_response_time * 1000, 2),
                "total_requests": total_requests,
                "successful_requests": successful_requests,
                "failed_requests": total_requests - successful_requests
            },
            "validation_metrics": {
                "total_validations": manager_stats.get('total_validations', 0),
                "successful_validations": manager_stats.get('successful_validations', 0),
                "validation_success_rate": round(
                    (manager_stats.get('successful_validations', 0) / 
                     max(manager_stats.get('total_validations', 1), 1)) * 100, 2
                )
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ç²å–æŒ‡æ¨™æ‘˜è¦å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–æŒ‡æ¨™æ‘˜è¦å¤±æ•—: {e}")


@app.get("/metrics/trends", summary="ç²å–ç³»çµ±æŒ‡æ¨™è¶¨å‹¢")
async def get_metrics_trends(
    hours: int = Query(24, description="æŸ¥è©¢æœ€è¿‘å¹¾å°æ™‚çš„è¶¨å‹¢æ•¸æ“š"),
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """ç²å–ç³»çµ±æŒ‡æ¨™çš„è¶¨å‹¢æ•¸æ“šï¼Œä¾›å‰ç«¯åœ–è¡¨ä½¿ç”¨"""
    try:
        # é€™è£¡å¯ä»¥å¾è³‡æ–™åº«æˆ–å¿«å–ä¸­ç²å–æ­·å²æ•¸æ“š
        # ç›®å‰å…ˆè¿”å›æ¨¡æ“¬æ•¸æ“šï¼Œå¾ŒçºŒå¯ä»¥é€£æ¥åˆ°çœŸå¯¦çš„æ™‚åºæ•¸æ“šåº«
        
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=hours)
        
        # ç”Ÿæˆæ¨¡æ“¬çš„è¶¨å‹¢æ•¸æ“šé»ï¼ˆæ¯å°æ™‚ä¸€å€‹æ•¸æ“šé»ï¼‰
        data_points = []
        current_time = start_time
        
        while current_time <= end_time:
            # æ¨¡æ“¬æ•¸æ“š - å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰è©²å¾è³‡æ–™åº«æŸ¥è©¢
            base_proxies = 1000
            time_factor = (current_time.hour % 24) / 24  # 0-1 ä¹‹é–“çš„å€¼
            
            data_points.append({
                "timestamp": current_time.isoformat(),
                "total_proxies": base_proxies + int(200 * time_factor),
                "active_proxies": base_proxies + int(150 * time_factor),
                "success_rate": 85 + int(10 * time_factor),
                "avg_response_time_ms": 500 + int(200 * (1 - time_factor)),
                "requests_per_hour": 1000 + int(500 * time_factor)
            })
            
            current_time += timedelta(hours=1)
        
        return {
            "period": {
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "hours": hours
            },
            "data_points": data_points,
            "summary": {
                "avg_total_proxies": sum(dp["total_proxies"] for dp in data_points) // len(data_points),
                "avg_active_proxies": sum(dp["active_proxies"] for dp in data_points) // len(data_points),
                "avg_success_rate": sum(dp["success_rate"] for dp in data_points) / len(data_points),
                "avg_response_time_ms": sum(dp["avg_response_time_ms"] for dp in data_points) / len(data_points)
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ç²å–æŒ‡æ¨™è¶¨å‹¢å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–æŒ‡æ¨™è¶¨å‹¢å¤±æ•—: {e}")


@app.post("/batch/validate", summary="æ‰¹é‡é©—è­‰ä»£ç†", dependencies=[Depends(require_api_key)])
async def batch_validate_proxies(
    background_tasks: BackgroundTasks,
    pool_types: Optional[str] = Query("hot,warm,cold", description="è¦é©—è­‰çš„æ± é¡å‹"),
    batch_size: int = Query(100, ge=10, le=1000, description="æ‰¹æ¬¡å¤§å°"),
    manager: ProxyManager = Depends(get_proxy_manager)
):
    """æ‰¹é‡é©—è­‰ä»£ç†çš„å¯ç”¨æ€§å’Œæ€§èƒ½"""
    try:
        # è§£ææ± é¡å‹
        pool_list = []
        if pool_types:
            for pool_name in pool_types.split(','):
                pool_name = pool_name.strip().lower()
                if pool_name in ['hot', 'warm', 'cold']:
                    pool_list.append(pool_name)
        
        if not pool_list:
            pool_list = ['hot', 'warm', 'cold']
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œæ‰¹é‡é©—è­‰
        background_tasks.add_task(_batch_validate_proxies, manager, pool_list, batch_size)
        
        return {
            "message": "æ‰¹é‡é©—è­‰ä»»å‹™å·²å•Ÿå‹•",
            "pool_types": pool_list,
            "batch_size": batch_size,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ å•Ÿå‹•æ‰¹é‡é©—è­‰å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"å•Ÿå‹•æ‰¹é‡é©—è­‰å¤±æ•—: {e}")


# éŒ¯èª¤è™•ç†
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    try:
        endpoint = str(request.url.path)
        method = request.method
        status = str(exc.status_code)
        REQUEST_COUNT.labels(endpoint=endpoint, method=method, status=status).inc()
        REQUEST_LATENCY.labels(endpoint=endpoint, method=method, status=status).observe(0)
    except Exception:
        pass
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "timestamp": datetime.now().isoformat(),
            "path": str(request.url)
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logger.error(f"âŒ æœªè™•ç†çš„ç•°å¸¸: {exc}")
    try:
        endpoint = str(request.url.path)
        method = request.method
        status = "500"
        REQUEST_COUNT.labels(endpoint=endpoint, method=method, status=status).inc()
        REQUEST_LATENCY.labels(endpoint=endpoint, method=method, status=status).observe(0)
    except Exception:
        pass
    return JSONResponse(
        status_code=500,
        content={
            "error": "å…§éƒ¨æœå‹™å™¨éŒ¯èª¤",
            "timestamp": datetime.now().isoformat(),
            "path": str(request.url)
        }
    )


# ===== èƒŒæ™¯ä»»å‹™å‡½æ•¸ =====

async def _sync_data_to_etl(manager: ProxyManager, pool_types: List[str]):
    """åŒæ­¥æ•¸æ“šåˆ° ETL ç³»çµ±çš„èƒŒæ™¯ä»»å‹™"""
    try:
        logger.info(f"ğŸ”„ é–‹å§‹åŒæ­¥æ•¸æ“šåˆ° ETL ç³»çµ±ï¼Œæ± é¡å‹: {pool_types}")
        
        # ç²å–æ‰€æœ‰ä»£ç†æ•¸æ“š
        all_proxies = []
        for pool_type in pool_types:
            # é€™è£¡æ‡‰è©²æ ¹æ“šå¯¦éš›çš„ ProxyManager API ç²å–ä»£ç†
            # proxies = await manager.get_proxies_from_pool(pool_type)
            # all_proxies.extend(proxies)
            pass
        
        # å°‡æ•¸æ“šç™¼é€åˆ° ETL ç³»çµ±
        # é€™è£¡æ‡‰è©²èª¿ç”¨ ETL API ä¾†è™•ç†æ•¸æ“š
        
        logger.info(f"âœ… æ•¸æ“šåŒæ­¥å®Œæˆï¼Œå…±è™•ç† {len(all_proxies)} å€‹ä»£ç†")
        
    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šåŒæ­¥å¤±æ•—: {e}")


async def _batch_validate_proxies(manager: ProxyManager, pool_types: List[str], batch_size: int):
    """æ‰¹é‡é©—è­‰ä»£ç†çš„èƒŒæ™¯ä»»å‹™"""
    try:
        logger.info(f"ğŸ” é–‹å§‹æ‰¹é‡é©—è­‰ä»£ç†ï¼Œæ± é¡å‹: {pool_types}ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        total_validated = 0
        total_valid = 0
        
        for pool_type in pool_types:
            # é€™è£¡æ‡‰è©²æ ¹æ“šå¯¦éš›çš„ ProxyManager API é€²è¡Œæ‰¹é‡é©—è­‰
            # result = await manager.batch_validate_pool(pool_type, batch_size)
            # total_validated += result.get('total', 0)
            # total_valid += result.get('valid', 0)
            pass
        
        success_rate = (total_valid / total_validated * 100) if total_validated > 0 else 0
        
        logger.info(f"âœ… æ‰¹é‡é©—è­‰å®Œæˆï¼Œé©—è­‰ {total_validated} å€‹ä»£ç†ï¼ŒæˆåŠŸç‡: {success_rate:.2f}%")
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡é©—è­‰å¤±æ•—: {e}")


# å•Ÿå‹•æœå‹™å™¨çš„å‡½æ•¸
def start_server(
    host: str = "0.0.0.0",
    port: int = 8000,
    reload: bool = False,
    log_level: str = "info"
):
    """å•Ÿå‹• FastAPI æœå‹™å™¨"""
    uvicorn.run(
        "proxy_manager.api:app",
        host=host,
        port=port,
        reload=reload,
        log_level=log_level,
        access_log=True
    )


@app.get("/metrics", include_in_schema=False)
async def metrics():
    return HTMLResponse(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)


if __name__ == "__main__":
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # å•Ÿå‹•æœå‹™å™¨
    start_server(reload=True)