"""ä»£ç†ç®¡ç†å™¨æ ¸å¿ƒæ¨¡çµ„

æ•´åˆæ‰€æœ‰åŠŸèƒ½æ¨¡çµ„ï¼Œæä¾›çµ±ä¸€çš„ä»£ç†ç®¡ç†æ¥å£ï¼š
- ä»£ç†ç²å–èˆ‡é©—è­‰
- æ± ç®¡ç†èˆ‡è‡ªå‹•å¹³è¡¡
- çµ±è¨ˆèˆ‡ç›£æ§
- æŒä¹…åŒ–å­˜å„²
"""

import asyncio
import json
import logging
from typing import List, Optional, Dict, Any, Union
from datetime import datetime, timedelta
from pathlib import Path
from dataclasses import dataclass
import aiofiles

from .models import ProxyNode, ProxyStatus, ProxyAnonymity, ProxyProtocol, ProxyFilter, ScanTarget, ScanResult, ScanConfig
from .fetchers import ProxyFetcherManager, JsonFileFetcher
from .advanced_fetchers import AdvancedProxyFetcherManager
from .scanner import ProxyScanner
from .enhanced_scanner import EnhancedProxyScanner
from .validators import ProxyValidator, BatchValidator
from .pools import ProxyPoolManager, PoolConfig, PoolType
from .config import ProxyManagerConfig as ConfigClass

logger = logging.getLogger(__name__)


# ä½¿ç”¨æ–°çš„é…ç½®é¡
ProxyManagerConfig = ConfigClass


class ProxyManager:
    """ä»£ç†ç®¡ç†å™¨ä¸»é¡"""
    
    def __init__(self, config: Optional[ProxyManagerConfig] = None):
        self.config = config or ProxyManagerConfig()
        
        # æ ¸å¿ƒçµ„ä»¶
        self.fetcher_manager = ProxyFetcherManager()
        # ç‚º AdvancedProxyFetcherManager æº–å‚™å­—å…¸æ ¼å¼çš„é…ç½®
        advanced_config = {
            "proxyscrape_api_key": getattr(self.config.api, 'proxyscrape_api_key', None),
            "github_token": getattr(self.config.api, 'github_token', None),
            "shodan_api_key": getattr(self.config.api, 'shodan_api_key', None)
        }
        self.advanced_fetcher_manager = AdvancedProxyFetcherManager(advanced_config)
        self.scanner = ProxyScanner(self.config.scanner)
        self.enhanced_scanner = EnhancedProxyScanner()
        self.pool_manager = ProxyPoolManager()
        self.validator: Optional[ProxyValidator] = None
        self.batch_validator: Optional[BatchValidator] = None
        
        # ç‹€æ…‹
        self._running = False
        self._tasks: List[asyncio.Task] = []
        
        # çµ±è¨ˆ
        self.stats = {
            'total_fetched': 0,
            'total_validated': 0,
            'total_active': 0,
            'last_fetch_time': None,
            'last_validation_time': None,
            'start_time': None
        }
    
    async def start(self):
        """å•Ÿå‹•ä»£ç†ç®¡ç†å™¨"""
        if self._running:
            logger.warning("âš ï¸ ä»£ç†ç®¡ç†å™¨å·²ç¶“åœ¨é‹è¡Œä¸­")
            return
        
        logger.info("ğŸš€ å•Ÿå‹•ä»£ç†ç®¡ç†å™¨...")
        
        try:
            # åˆå§‹åŒ–çµ„ä»¶
            await self._initialize_components()
            
            # è¼‰å…¥ç¾æœ‰æ•¸æ“š
            await self._load_existing_data()
            
            # å•Ÿå‹•è‡ªå‹•ä»»å‹™
            await self._start_auto_tasks()
            
            self._running = True
            self.stats['start_time'] = datetime.now()
            
            logger.info("âœ… ä»£ç†ç®¡ç†å™¨å•Ÿå‹•æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ ä»£ç†ç®¡ç†å™¨å•Ÿå‹•å¤±æ•—: {e}")
            await self.stop()
            raise
    
    async def stop(self):
        """åœæ­¢ä»£ç†ç®¡ç†å™¨"""
        if not self._running:
            return
        
        logger.info("ğŸ›‘ åœæ­¢ä»£ç†ç®¡ç†å™¨...")
        
        self._running = False
        
        # å–æ¶ˆæ‰€æœ‰ä»»å‹™
        for task in self._tasks:
            task.cancel()
        
        # ç­‰å¾…ä»»å‹™å®Œæˆ
        if self._tasks:
            await asyncio.gather(*self._tasks, return_exceptions=True)
        
        self._tasks.clear()
        
        # ä¿å­˜æ•¸æ“š
        await self._save_data()
        
        # åœæ­¢çµ„ä»¶
        await self.pool_manager.stop()
        
        if self.validator and hasattr(self.validator, 'close'):
            await self.validator.close()
        
        await self.advanced_fetcher_manager.close()
        await self.scanner.close()
        
        logger.info("âœ… ä»£ç†ç®¡ç†å™¨å·²åœæ­¢")
    
    async def _initialize_components(self):
        """åˆå§‹åŒ–çµ„ä»¶"""
        # åˆå§‹åŒ–ç²å–å™¨
        # FreeProxyFetcher å·²ç§»é™¤ï¼Œæ”¹ç”¨å…¶ä»–ä»£ç†ä¾†æº
        # if self.config.enable_free_proxy:
        #     self.fetcher_manager.add_fetcher(FreeProxyFetcher())
        
        if self.config.enable_json_file and self.config.json_file_path.exists():
            self.fetcher_manager.add_fetcher(
                JsonFileFetcher(str(self.config.json_file_path))
            )
        
        # åˆå§‹åŒ–é©—è­‰å™¨
        self.validator = ProxyValidator(self.config.validation)
        
        self.batch_validator = BatchValidator(
            self.config.validation,
            self.config.batch_validation_size
        )
        
        # å•Ÿå‹•æ± ç®¡ç†å™¨
        await self.pool_manager.start()
    
    async def _load_existing_data(self):
        """è¼‰å…¥ç¾æœ‰æ•¸æ“š"""
        data_file = self.config.data_dir / "proxy_pools.json"
        if data_file.exists():
            try:
                await self.pool_manager.load_from_file(data_file)
                logger.info("ğŸ“‚ å·²è¼‰å…¥ç¾æœ‰ä»£ç†æ•¸æ“š")
            except Exception as e:
                logger.error(f"âŒ è¼‰å…¥æ•¸æ“šå¤±æ•—: {e}")
    
    async def _start_auto_tasks(self):
        """å•Ÿå‹•è‡ªå‹•ä»»å‹™"""
        if self.config.auto_fetch_enabled:
            task = asyncio.create_task(self._auto_fetch_loop())
            self._tasks.append(task)
        
        if self.config.auto_cleanup_enabled:
            task = asyncio.create_task(self._auto_cleanup_loop())
            self._tasks.append(task)
        
        if self.config.auto_save_enabled:
            task = asyncio.create_task(self._auto_save_loop())
            self._tasks.append(task)
    
    async def fetch_proxies(self, sources: Optional[List[str]] = None) -> List[ProxyNode]:
        """ç²å–ä»£ç†"""
        logger.info("ğŸ” é–‹å§‹ç²å–ä»£ç†...")
        
        try:
            # ç²å–åŸå§‹ä»£ç†ï¼ˆå‚³çµ±ä¾†æºï¼‰
            raw_proxies = await self.fetcher_manager.fetch_all_proxies()
            
            # ç²å–é«˜ç´šä¾†æºä»£ç†
            advanced_proxies = await self.advanced_fetcher_manager.fetch_all_proxies()
            
            # åˆä½µä»£ç†åˆ—è¡¨
            all_proxies = raw_proxies + advanced_proxies
            
            if not all_proxies:
                logger.warning("âš ï¸ æ²’æœ‰ç²å–åˆ°ä»»ä½•ä»£ç†")
                return []
            
            logger.info(f"ğŸ“¥ ç²å–åˆ° {len(raw_proxies)} å€‹å‚³çµ±ä»£ç†ï¼Œ{len(advanced_proxies)} å€‹é«˜ç´šä»£ç†")
            
            # ä½¿ç”¨æƒæå™¨é€²è¡Œå¿«é€Ÿé ç¯©é¸ï¼ˆå¯é¸ï¼‰
            if hasattr(self.config, 'scanner') and hasattr(self.config.scanner, 'enable_fast_scan') and self.config.scanner.enable_fast_scan:
                logger.info("ğŸ” åŸ·è¡Œå¿«é€Ÿç«¯å£æƒæé ç¯©é¸...")
                scanned_proxies = await self.scanner.scan_proxy_list(all_proxies)
                all_proxies = scanned_proxies
                logger.info(f"ğŸ¯ æƒæå¾Œå‰©é¤˜ {len(all_proxies)} å€‹ä»£ç†")
            
            raw_proxies = all_proxies
            
            logger.info(f"ğŸ“¥ ç¸½å…±è™•ç† {len(raw_proxies)} å€‹ä»£ç†")
            
            # æ‰¹é‡é©—è­‰
            if self.batch_validator:
                validation_results = await self.batch_validator.validate_large_batch(raw_proxies)
                # æå–æœ‰æ•ˆä»£ç†
                valid_proxies = [result.proxy for result in validation_results if result.is_working]
            else:
                # å¦‚æœæ²’æœ‰æ‰¹é‡é©—è­‰å™¨ï¼Œè·³éé©—è­‰ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰ä»£ç†
                logger.warning("âš ï¸ æ‰¹é‡é©—è­‰å™¨æœªåˆå§‹åŒ–ï¼Œè·³éé©—è­‰")
                valid_proxies = raw_proxies
            
            logger.info(f"âœ… é©—è­‰å®Œæˆ: {len(valid_proxies)}/{len(raw_proxies)} å€‹ä»£ç†å¯ç”¨")
            
            # æ·»åŠ åˆ°æ± ä¸­
            await self.pool_manager.add_proxies(valid_proxies)
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats['total_fetched'] += len(raw_proxies)
            self.stats['total_validated'] += len(raw_proxies)
            self.stats['total_active'] = len(valid_proxies)
            self.stats['last_fetch_time'] = datetime.now()
            self.stats['last_validation_time'] = datetime.now()
            
            return valid_proxies
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ä»£ç†å¤±æ•—: {e}")
            raise
    
    async def get_proxy(self, 
                       filter_criteria: Optional[ProxyFilter] = None,
                       pool_preference: Optional[List[PoolType]] = None) -> Optional[ProxyNode]:
        """ç²å–ä»£ç†"""
        return await self.pool_manager.get_proxy(pool_preference, filter_criteria)
    
    async def get_proxies(self, 
                         count: int = 10,
                         filter_criteria: Optional[ProxyFilter] = None,
                         pool_preference: Optional[List[PoolType]] = None) -> List[ProxyNode]:
        """æ‰¹é‡ç²å–ä»£ç†"""
        proxies = []
        for _ in range(count):
            proxy = await self.get_proxy(filter_criteria, pool_preference)
            if proxy:
                proxies.append(proxy)
            else:
                break
        return proxies
    
    async def validate_pools(self):
        """é©—è­‰å’Œé‡æ–°å¹³è¡¡ä»£ç†æ± """
        logger.info("ğŸ”„ é–‹å§‹é©—è­‰ä»£ç†æ± ...")
        await self.pool_manager.validate_and_rebalance()
        self.stats['last_validation_time'] = datetime.now()
    
    async def cleanup_pools(self):
        """æ¸…ç†ä»£ç†æ± """
        logger.info("ğŸ—‘ï¸ é–‹å§‹æ¸…ç†ä»£ç†æ± ...")
        await self.pool_manager.cleanup_blacklist()
    
    async def _auto_fetch_loop(self):
        """è‡ªå‹•ç²å–å¾ªç’°"""
        while self._running:
            try:
                await asyncio.sleep(self.config.auto_fetch_interval_hours * 3600)
                if self._running:
                    await self.fetch_proxies()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ è‡ªå‹•ç²å–å‡ºéŒ¯: {e}")
                await asyncio.sleep(300)  # å‡ºéŒ¯å¾Œç­‰å¾…5åˆ†é˜
    
    async def _auto_cleanup_loop(self):
        """è‡ªå‹•æ¸…ç†å¾ªç’°"""
        while self._running:
            try:
                await asyncio.sleep(self.config.auto_cleanup_interval_hours * 3600)
                if self._running:
                    await self.cleanup_pools()
                    await self.validate_pools()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ è‡ªå‹•æ¸…ç†å‡ºéŒ¯: {e}")
                await asyncio.sleep(300)
    
    async def _auto_save_loop(self):
        """è‡ªå‹•ä¿å­˜å¾ªç’°"""
        while self._running:
            try:
                await asyncio.sleep(self.config.auto_save_interval_minutes * 60)
                if self._running:
                    await self._save_data()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ è‡ªå‹•ä¿å­˜å‡ºéŒ¯: {e}")
                await asyncio.sleep(60)
    
    async def _save_data(self):
        """ä¿å­˜æ•¸æ“š"""
        try:
            data_file = self.config.data_dir / "proxy_pools.json"
            await self.pool_manager.save_to_file(data_file)
            
            # å‰µå»ºå‚™ä»½
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.config.backup_dir / f"proxy_pools_{timestamp}.json"
            await self.pool_manager.save_to_file(backup_file)
            
            # æ¸…ç†èˆŠå‚™ä»½ï¼ˆä¿ç•™æœ€è¿‘10å€‹ï¼‰
            await self._cleanup_old_backups()
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•¸æ“šå¤±æ•—: {e}")
    
    async def _cleanup_old_backups(self):
        """æ¸…ç†èˆŠå‚™ä»½æ–‡ä»¶"""
        try:
            backup_files = list(self.config.backup_dir.glob("proxy_pools_*.json"))
            backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # ä¿ç•™æœ€è¿‘10å€‹å‚™ä»½
            for old_backup in backup_files[10:]:
                old_backup.unlink()
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†å‚™ä»½å¤±æ•—: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        pool_stats = self.pool_manager.get_all_stats()
        pool_summary = self.pool_manager.get_summary()
        
        return {
            'manager_stats': self.stats.copy(),
            'pool_summary': pool_summary,
            'pool_details': {k: v.__dict__ for k, v in pool_stats.items()},
            'fetcher_stats': self.fetcher_manager.get_stats(),
            'advanced_fetcher_stats': self.advanced_fetcher_manager.get_stats(),
            'config': {
                'auto_fetch_enabled': self.config.auto_fetch_enabled,
                'auto_fetch_interval_hours': self.config.auto_fetch_interval_hours,
                'auto_cleanup_enabled': self.config.auto_cleanup_enabled,
                'auto_save_enabled': self.config.auto_save_enabled
            },
            'status': {
                'running': self._running,
                'active_tasks': len(self._tasks)
            }
        }
    
    async def export_proxies(self, 
                           file_path: Path,
                           format_type: str = "json",
                           pool_types: Optional[List[PoolType]] = None) -> int:
        """å°å‡ºä»£ç†åˆ°æ–‡ä»¶"""
        if pool_types is None:
            pool_types = [PoolType.HOT, PoolType.WARM, PoolType.COLD]
        
        all_proxies = []
        for pool_type in pool_types:
            if pool_type in self.pool_manager.pools:
                pool = self.pool_manager.pools[pool_type]
                active_proxies = [
                    proxy for proxy in pool.proxies.values() 
                    if proxy.status == ProxyStatus.ACTIVE
                ]
                all_proxies.extend(active_proxies)
        
        if format_type.lower() == "json":
            data = {
                'timestamp': datetime.now().isoformat(),
                'total_count': len(all_proxies),
                'proxies': [proxy.to_dict() for proxy in all_proxies]
            }
            
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(data, ensure_ascii=False, indent=2, default=str))
        
        elif format_type.lower() == "txt":
            lines = []
            for proxy in all_proxies:
                lines.append(f"{proxy.host}:{proxy.port}")
            
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write('\n'.join(lines))
        
        elif format_type.lower() == "csv":
            import csv
            import io
            
            output = io.StringIO()
            writer = csv.writer(output)
            
            # å¯«å…¥æ¨™é¡Œ
            writer.writerow(['host', 'port', 'protocol', 'anonymity', 'country', 'score', 'response_time'])
            
            # å¯«å…¥æ•¸æ“š
            for proxy in all_proxies:
                writer.writerow([
                    proxy.host,
                    proxy.port,
                    proxy.protocol.value,
                    proxy.anonymity.value,
                    proxy.country or '',
                    proxy.score,
                    proxy.metrics.avg_response_time or 0
                ])
            
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(output.getvalue())
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")
        
        logger.info(f"ğŸ“¤ å·²å°å‡º {len(all_proxies)} å€‹ä»£ç†åˆ°: {file_path}")
        return len(all_proxies)
    
    async def import_proxies(self, file_path: Path, validate: bool = True) -> int:
        """å¾æ–‡ä»¶å°å…¥ä»£ç†"""
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        proxies = []
        
        if file_path.suffix.lower() == '.json':
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
                data = json.loads(content)
                
                if 'proxies' in data:
                    for proxy_dict in data['proxies']:
                        proxy = ProxyNode.from_dict(proxy_dict)
                        proxies.append(proxy)
                else:
                    # ç°¡å–®æ ¼å¼
                    for item in data:
                        if isinstance(item, dict) and 'host' in item and 'port' in item:
                            proxy = ProxyNode.from_dict(item)
                            proxies.append(proxy)
        
        elif file_path.suffix.lower() == '.txt':
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
                lines = content.strip().split('\n')
                
                for line in lines:
                    line = line.strip()
                    if ':' in line:
                        host, port = line.split(':', 1)
                        proxy = ProxyNode(
                            host=host.strip(),
                            port=int(port.strip()),
                            protocol=ProxyProtocol.HTTP
                        )
                        proxies.append(proxy)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        
        if validate and proxies:
            logger.info(f"ğŸ” é©—è­‰å°å…¥çš„ {len(proxies)} å€‹ä»£ç†...")
            validation_results = await self.batch_validator.validate_large_batch(proxies)
            valid_proxies = [result.proxy for result in validation_results if result.is_working]
            await self.pool_manager.add_proxies(valid_proxies)
            logger.info(f"âœ… å°å…¥å®Œæˆ: {len(valid_proxies)}/{len(proxies)} å€‹ä»£ç†å¯ç”¨")
            return len(valid_proxies)
        else:
            await self.pool_manager.add_proxies(proxies)
            logger.info(f"ğŸ“¥ å°å…¥ {len(proxies)} å€‹ä»£ç†ï¼ˆæœªé©—è­‰ï¼‰")
            return len(proxies)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    async def main():
        # å‰µå»ºé…ç½®
        config = ProxyManagerConfig(
            data_dir=Path("data/proxy_manager"),
            auto_fetch_enabled=True,
            auto_fetch_interval_hours=6
        )
        
        # å‰µå»ºç®¡ç†å™¨
        manager = ProxyManager(config)
        
        try:
            # å•Ÿå‹•ç®¡ç†å™¨
            await manager.start()
            
            # ç²å–ä»£ç†
            await manager.fetch_proxies()
            
            # ç²å–å–®å€‹ä»£ç†
            proxy = await manager.get_proxy()
            if proxy:
                print(f"ç²å–åˆ°ä»£ç†: {proxy.url}")
            
            # ç²å–çµ±è¨ˆä¿¡æ¯
            stats = manager.get_stats()
            print(f"çµ±è¨ˆä¿¡æ¯: {stats['pool_summary']}")
            
            # å°å‡ºä»£ç†
            await manager.export_proxies(Path("exported_proxies.json"))
            
        finally:
            await manager.stop()


# å¢å¼·æƒæåŠŸèƒ½
class EnhancedProxyManager(ProxyManager):
    """å¢å¼·ä»£ç†ç®¡ç†å™¨ï¼ŒåŒ…å«æƒæåŠŸèƒ½"""
    
    async def scan_ip_range(self, ip_range: str, ports: Optional[List[int]] = None) -> List[ScanResult]:
        """æƒæ IP ç¯„åœå°‹æ‰¾ä»£ç†
        
        Args:
            ip_range: CIDR æ ¼å¼çš„ IP ç¯„åœï¼Œå¦‚ "192.168.1.0/24"
            ports: è¦æƒæçš„ç«¯å£åˆ—è¡¨ï¼Œé»˜èªä½¿ç”¨é…ç½®ä¸­çš„ç«¯å£
            
        Returns:
            List[ScanResult]: æƒæçµæœåˆ—è¡¨
        """
        try:
            logger.info(f"é–‹å§‹æƒæ IP ç¯„åœ: {ip_range}")
            
            # ä½¿ç”¨å¢å¼·æƒæå™¨
            results = await self.enhanced_scanner.scan_ip_range(ip_range, ports)
            
            # å°‡æˆåŠŸçš„æƒæçµæœè½‰æ›ç‚ºä»£ç†ç¯€é»
            for result in results:
                if result.result.value == "success" and result.proxy_node:
                    # æ·»åŠ åˆ°ä»£ç†æ± 
                    await self.pool_manager.add_proxy(result.proxy_node)
            
            logger.info(f"IP ç¯„åœæƒæå®Œæˆï¼Œç™¼ç¾ {len([r for r in results if r.result.value == 'success'])} å€‹ä»£ç†")
            return results
            
        except Exception as e:
            logger.error(f"IP ç¯„åœæƒæå¤±æ•—: {e}")
            return []
    
    async def scan_single_target(self, host: str, port: int, protocols: Optional[List[str]] = None) -> List[ScanResult]:
        """æƒæå–®å€‹ç›®æ¨™
        
        Args:
            host: ç›®æ¨™ä¸»æ©Ÿ
            port: ç›®æ¨™ç«¯å£
            protocols: è¦æ¸¬è©¦çš„å”è­°åˆ—è¡¨
            
        Returns:
            List[ScanResult]: æƒæçµæœåˆ—è¡¨
        """
        try:
            logger.info(f"é–‹å§‹æƒæç›®æ¨™: {host}:{port}")
            
            # å‰µå»ºæƒæç›®æ¨™
            target = ScanTarget(host=host, port=port)
            if protocols:
                from .models import ScanProtocol
                target.protocols = [ScanProtocol(p) for p in protocols]
            
            # ä½¿ç”¨å¢å¼·æƒæå™¨æƒæå–®å€‹ç›®æ¨™
            results = []
            for protocol in target.protocols:
                result = await self.enhanced_scanner._scan_single_target(target)
                results.append(result)
            
            # å°‡æˆåŠŸçš„æƒæçµæœè½‰æ›ç‚ºä»£ç†ç¯€é»
            for result in results:
                if result.result.value == "success" and result.proxy_node:
                    # æ·»åŠ åˆ°ä»£ç†æ± 
                    await self.pool_manager.add_proxy(result.proxy_node)
            
            logger.info(f"ç›®æ¨™æƒæå®Œæˆï¼Œç™¼ç¾ {len([r for r in results if r.result.value == 'success'])} å€‹ä»£ç†")
            return results
            
        except Exception as e:
            logger.error(f"ç›®æ¨™æƒæå¤±æ•—: {e}")
            return []
    
    async def get_scan_statistics(self) -> Dict[str, Any]:
        """ç²å–æƒæçµ±è¨ˆä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æƒæçµ±è¨ˆä¿¡æ¯
        """
        try:
            return await self.enhanced_scanner.get_statistics()
        except Exception as e:
            logger.error(f"ç²å–æƒæçµ±è¨ˆå¤±æ•—: {e}")
            return {}


if __name__ == "__main__":
    asyncio.run(main())