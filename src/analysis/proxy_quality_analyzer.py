#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»£ç†å“è³ªåˆ†æå™¨

æä¾›ä»£ç†å“è³ªè©•ä¼°ã€æ€§èƒ½åˆ†æå’Œé æ¸¬åŠŸèƒ½
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from statistics import mean, median, stdev
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib

from ..proxy_manager.models import ProxyNode, ProxyStatus, ProxyAnonymity, ProxyProtocol

logger = logging.getLogger(__name__)


@dataclass
class QualityMetrics:
    """ä»£ç†å“è³ªæŒ‡æ¨™"""
    response_time: float  # éŸ¿æ‡‰æ™‚é–“ (ms)
    success_rate: float   # æˆåŠŸç‡ (0-1)
    uptime: float         # é‹è¡Œæ™‚é–“ (0-1)
    anonymity_level: int  # åŒ¿åç­‰ç´š (1-4)
    location_score: float # åœ°ç†ä½ç½®è©•åˆ† (0-1)
    protocol_score: float # å”è­°æ”¯æ´è©•åˆ† (0-1)
    stability: float      # ç©©å®šæ€§è©•åˆ† (0-1)
    security: float       # å®‰å…¨æ€§è©•åˆ† (0-1)


@dataclass
class QualityScore:
    """ä»£ç†å“è³ªè©•åˆ†"""
    proxy_id: str
    overall_score: float      # ç¸½é«”è©•åˆ† (0-100)
    performance_score: float  # æ€§èƒ½è©•åˆ† (0-100)
    reliability_score: float  # å¯é æ€§è©•åˆ† (0-100)
    security_score: float     # å®‰å…¨æ€§è©•åˆ† (0-100)
    location_score: float     # åœ°ç†ä½ç½®è©•åˆ† (0-100)
    metrics: QualityMetrics
    timestamp: datetime
    recommendations: List[str]  # æ”¹é€²å»ºè­°


@dataclass
class AnalysisReport:
    """åˆ†æå ±å‘Š"""
    total_proxies: int
    analyzed_proxies: int
    average_score: float
    score_distribution: Dict[str, int]  # è©•åˆ†åˆ†å¸ƒ
    top_proxies: List[QualityScore]
    worst_proxies: List[QualityScore]
    recommendations: List[str]
    generated_at: datetime


class ProxyQualityAnalyzer:
    """ä»£ç†å“è³ªåˆ†æå™¨"""
    
    def __init__(self, data_dir: str = "data/analysis"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
        self.performance_model = None
        self.reliability_model = None
        self.model_file = self.data_dir / "quality_models.joblib"
        
        # åˆ†æé…ç½®
        self.weights = {
            'response_time': 0.25,
            'success_rate': 0.30,
            'uptime': 0.20,
            'anonymity_level': 0.15,
            'location_score': 0.10
        }
        
        # è¼‰å…¥æˆ–åˆå§‹åŒ–æ¨¡å‹
        self._load_models()
    
    def _load_models(self):
        """è¼‰å…¥æ©Ÿå™¨å­¸ç¿’æ¨¡å‹"""
        try:
            if self.model_file.exists():
                models = joblib.load(self.model_file)
                self.performance_model = models.get('performance')
                self.reliability_model = models.get('reliability')
                logger.info("âœ… æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            else:
                logger.info("ğŸ“Š åˆå§‹åŒ–æ–°çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    
    def _save_models(self):
        """ä¿å­˜æ©Ÿå™¨å­¸ç¿’æ¨¡å‹"""
        try:
            models = {
                'performance': self.performance_model,
                'reliability': self.reliability_model
            }
            joblib.dump(models, self.model_file)
            logger.info("âœ… æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±æ•—: {e}")
    
    def calculate_quality_metrics(self, proxy: ProxyNode, 
                                response_times: List[float],
                                success_count: int,
                                total_requests: int) -> QualityMetrics:
        """è¨ˆç®—ä»£ç†å“è³ªæŒ‡æ¨™"""
        
        # éŸ¿æ‡‰æ™‚é–“æŒ‡æ¨™
        avg_response_time = mean(response_times) if response_times else 1000
        response_time_score = max(0, 1 - (avg_response_time / 5000))  # 5ç§’ç‚ºæœ€å·®
        
        # æˆåŠŸç‡
        success_rate = success_count / total_requests if total_requests > 0 else 0
        
        # é‹è¡Œæ™‚é–“ (åŸºæ–¼æ­·å²æ•¸æ“š)
        uptime = min(1.0, success_rate * 1.2)  # ç°¡åŒ–è¨ˆç®—
        
        # åŒ¿åç­‰ç´šè©•åˆ†
        anonymity_scores = {
            ProxyAnonymity.TRANSPARENT: 0.2,
            ProxyAnonymity.ANONYMOUS: 0.6,
            ProxyAnonymity.ELITE: 1.0
        }
        anonymity_score = anonymity_scores.get(proxy.anonymity, 0.4)
        
        # åœ°ç†ä½ç½®è©•åˆ† (åŸºæ–¼åœ‹å®¶)
        location_scores = {
            'United States': 0.9, 'Canada': 0.8, 'United Kingdom': 0.8, 'Germany': 0.8, 'France': 0.7,
            'Japan': 0.7, 'Australia': 0.7, 'Netherlands': 0.8, 'Singapore': 0.6, 'Hong Kong': 0.6
        }
        location_score = location_scores.get(proxy.country, 0.5)
        
        # å”è­°æ”¯æ´è©•åˆ†
        protocol_scores = {
            ProxyProtocol.HTTP: 0.6,
            ProxyProtocol.HTTPS: 0.8,
            ProxyProtocol.SOCKS4: 0.7,
            ProxyProtocol.SOCKS5: 0.9
        }
        protocol_score = protocol_scores.get(proxy.protocol, 0.5)
        
        # ç©©å®šæ€§è©•åˆ† (åŸºæ–¼éŸ¿æ‡‰æ™‚é–“è®Šç•°)
        stability = 1.0 - (stdev(response_times) / mean(response_times)) if len(response_times) > 1 else 0.5
        stability = max(0, min(1, stability))
        
        # å®‰å…¨æ€§è©•åˆ† (åŸºæ–¼åŒ¿åç­‰ç´šå’Œå”è­°)
        security = (anonymity_score + protocol_score) / 2
        
        return QualityMetrics(
            response_time=avg_response_time,
            success_rate=success_rate,
            uptime=uptime,
            anonymity_level=proxy.anonymity.value if hasattr(proxy.anonymity, 'value') else 1,
            location_score=location_score,
            protocol_score=protocol_score,
            stability=stability,
            security=security
        )
    
    def calculate_quality_score(self, proxy: ProxyNode, metrics: QualityMetrics) -> QualityScore:
        """è¨ˆç®—ä»£ç†å“è³ªè©•åˆ†"""
        
        # æ€§èƒ½è©•åˆ† (éŸ¿æ‡‰æ™‚é–“ + æˆåŠŸç‡)
        performance_score = (
            (1 - metrics.response_time / 5000) * 0.6 +  # éŸ¿æ‡‰æ™‚é–“æ¬Šé‡
            metrics.success_rate * 0.4  # æˆåŠŸç‡æ¬Šé‡
        ) * 100
        
        # å¯é æ€§è©•åˆ† (ç©©å®šæ€§ + é‹è¡Œæ™‚é–“)
        reliability_score = (
            metrics.stability * 0.6 +
            metrics.uptime * 0.4
        ) * 100
        
        # å®‰å…¨æ€§è©•åˆ† (åŒ¿åç­‰ç´š + å”è­°æ”¯æ´)
        security_score = (
            metrics.security * 0.7 +
            metrics.protocol_score * 0.3
        ) * 100
        
        # åœ°ç†ä½ç½®è©•åˆ†
        location_score = metrics.location_score * 100
        
        # ç¸½é«”è©•åˆ† (åŠ æ¬Šå¹³å‡)
        overall_score = (
            performance_score * self.weights['response_time'] +
            reliability_score * self.weights['success_rate'] +
            security_score * self.weights['anonymity_level'] +
            location_score * self.weights['location_score']
        )
        
        # ç”Ÿæˆæ”¹é€²å»ºè­°
        recommendations = self._generate_recommendations(metrics, overall_score)
        
        return QualityScore(
            proxy_id=f"{proxy.host}:{proxy.port}",
            overall_score=round(overall_score, 2),
            performance_score=round(performance_score, 2),
            reliability_score=round(reliability_score, 2),
            security_score=round(security_score, 2),
            location_score=round(location_score, 2),
            metrics=metrics,
            timestamp=datetime.now(),
            recommendations=recommendations
        )
    
    def _generate_recommendations(self, metrics: QualityMetrics, overall_score: float) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        if metrics.response_time > 3000:
            recommendations.append("éŸ¿æ‡‰æ™‚é–“éæ…¢ï¼Œå»ºè­°æª¢æŸ¥ç¶²è·¯é€£æ¥")
        
        if metrics.success_rate < 0.7:
            recommendations.append("æˆåŠŸç‡è¼ƒä½ï¼Œå»ºè­°æ›´æ›ä»£ç†æˆ–æª¢æŸ¥é…ç½®")
        
        if metrics.stability < 0.5:
            recommendations.append("ç©©å®šæ€§ä¸è¶³ï¼Œå»ºè­°ä½¿ç”¨æ›´ç©©å®šçš„ä»£ç†")
        
        if metrics.security < 0.6:
            recommendations.append("å®‰å…¨æ€§è¼ƒä½ï¼Œå»ºè­°ä½¿ç”¨é«˜åŒ¿åä»£ç†")
        
        if overall_score < 60:
            recommendations.append("æ•´é«”è©•åˆ†è¼ƒä½ï¼Œå»ºè­°è€ƒæ…®æ›´æ›ä»£ç†")
        
        return recommendations
    
    async def analyze_proxies(self, proxies: List[ProxyNode], 
                            historical_data: Optional[Dict[str, Any]] = None) -> AnalysisReport:
        """åˆ†æä»£ç†åˆ—è¡¨"""
        logger.info(f"ğŸ” é–‹å§‹åˆ†æ {len(proxies)} å€‹ä»£ç†çš„å“è³ª")
        
        quality_scores = []
        
        for proxy in proxies:
            try:
                # ç²å–æ­·å²æ•¸æ“š
                proxy_data = historical_data.get(f"{proxy.host}:{proxy.port}", {}) if historical_data else {}
                
                # æ¨¡æ“¬æ¸¬è©¦æ•¸æ“š (å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰è©²å¾æ•¸æ“šåº«ç²å–)
                response_times = proxy_data.get('response_times', [1000, 1200, 1100])
                success_count = proxy_data.get('success_count', 8)
                total_requests = proxy_data.get('total_requests', 10)
                
                # è¨ˆç®—å“è³ªæŒ‡æ¨™
                metrics = self.calculate_quality_metrics(proxy, response_times, success_count, total_requests)
                
                # è¨ˆç®—å“è³ªè©•åˆ†
                score = self.calculate_quality_score(proxy, metrics)
                quality_scores.append(score)
                
            except Exception as e:
                logger.error(f"âŒ åˆ†æä»£ç† {proxy.host}:{proxy.port} å¤±æ•—: {e}")
                continue
        
        # ç”Ÿæˆåˆ†æå ±å‘Š
        report = self._generate_analysis_report(quality_scores)
        
        # ä¿å­˜åˆ†æçµæœ
        await self._save_analysis_results(report)
        
        logger.info(f"âœ… ä»£ç†å“è³ªåˆ†æå®Œæˆï¼Œå¹³å‡è©•åˆ†: {report.average_score:.2f}")
        return report
    
    def _generate_analysis_report(self, quality_scores: List[QualityScore]) -> AnalysisReport:
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        if not quality_scores:
            return AnalysisReport(
                total_proxies=0,
                analyzed_proxies=0,
                average_score=0,
                score_distribution={},
                top_proxies=[],
                worst_proxies=[],
                recommendations=[],
                generated_at=datetime.now()
            )
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        scores = [score.overall_score for score in quality_scores]
        average_score = mean(scores)
        
        # è©•åˆ†åˆ†å¸ƒ
        score_distribution = {
            "å„ªç§€ (90-100)": len([s for s in scores if s >= 90]),
            "è‰¯å¥½ (80-89)": len([s for s in scores if 80 <= s < 90]),
            "ä¸€èˆ¬ (70-79)": len([s for s in scores if 70 <= s < 80]),
            "è¼ƒå·® (60-69)": len([s for s in scores if 60 <= s < 70]),
            "å¾ˆå·® (0-59)": len([s for s in scores if s < 60])
        }
        
        # æ’åº
        sorted_scores = sorted(quality_scores, key=lambda x: x.overall_score, reverse=True)
        
        # æœ€ä½³å’Œæœ€å·®ä»£ç†
        top_proxies = sorted_scores[:5]
        worst_proxies = sorted_scores[-5:]
        
        # ç”Ÿæˆå»ºè­°
        recommendations = self._generate_global_recommendations(quality_scores, average_score)
        
        return AnalysisReport(
            total_proxies=len(quality_scores),
            analyzed_proxies=len(quality_scores),
            average_score=round(average_score, 2),
            score_distribution=score_distribution,
            top_proxies=top_proxies,
            worst_proxies=worst_proxies,
            recommendations=recommendations,
            generated_at=datetime.now()
        )
    
    def _generate_global_recommendations(self, quality_scores: List[QualityScore], 
                                       average_score: float) -> List[str]:
        """ç”Ÿæˆå…¨å±€å»ºè­°"""
        recommendations = []
        
        if average_score < 70:
            recommendations.append("æ•´é«”ä»£ç†å“è³ªè¼ƒä½ï¼Œå»ºè­°æ›´æ–°ä»£ç†æ± ")
        
        # åˆ†æå„é …æŒ‡æ¨™
        performance_scores = [s.performance_score for s in quality_scores]
        reliability_scores = [s.reliability_score for s in quality_scores]
        security_scores = [s.security_score for s in quality_scores]
        
        if mean(performance_scores) < 70:
            recommendations.append("ä»£ç†æ€§èƒ½æ™®éè¼ƒå·®ï¼Œå»ºè­°å„ªåŒ–ç¶²è·¯é…ç½®")
        
        if mean(reliability_scores) < 70:
            recommendations.append("ä»£ç†å¯é æ€§ä¸è¶³ï¼Œå»ºè­°å¢åŠ ç©©å®šæ€§æª¢æŸ¥")
        
        if mean(security_scores) < 70:
            recommendations.append("ä»£ç†å®‰å…¨æ€§è¼ƒä½ï¼Œå»ºè­°ä½¿ç”¨é«˜åŒ¿åä»£ç†")
        
        return recommendations
    
    async def _save_analysis_results(self, report: AnalysisReport):
        """ä¿å­˜åˆ†æçµæœ"""
        try:
            # ä¿å­˜è©³ç´°å ±å‘Š
            report_file = self.data_dir / f"quality_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            report_data = {
                'report': asdict(report),
                'generated_at': report.generated_at.isoformat()
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æœ€æ–°å ±å‘Š
            latest_file = self.data_dir / "latest_quality_analysis.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… åˆ†æçµæœå·²ä¿å­˜åˆ° {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æçµæœå¤±æ•—: {e}")
    
    def get_quality_trends(self, days: int = 7) -> Dict[str, Any]:
        """ç²å–å“è³ªè¶¨å‹¢æ•¸æ“š"""
        try:
            # è®€å–æ­·å²åˆ†æçµæœ
            trend_data = []
            for file_path in self.data_dir.glob("quality_analysis_*.json"):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    trend_data.append({
                        'date': data['generated_at'][:10],
                        'average_score': data['report']['average_score'],
                        'total_proxies': data['report']['total_proxies']
                    })
            
            # æŒ‰æ—¥æœŸæ’åº
            trend_data.sort(key=lambda x: x['date'])
            
            return {
                'trends': trend_data[-days:],
                'period_days': days,
                'generated_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å“è³ªè¶¨å‹¢å¤±æ•—: {e}")
            return {'trends': [], 'period_days': days, 'generated_at': datetime.now().isoformat()}
    
    def predict_proxy_performance(self, proxy: ProxyNode) -> Dict[str, float]:
        """é æ¸¬ä»£ç†æ€§èƒ½"""
        if not self.performance_model:
            return {'predicted_score': 0, 'confidence': 0}
        
        try:
            # æº–å‚™ç‰¹å¾µæ•¸æ“š
            features = np.array([[
                proxy.port,  # ç«¯å£
                1 if proxy.protocol == ProxyProtocol.HTTPS else 0,  # æ˜¯å¦HTTPS
                1 if proxy.protocol == ProxyProtocol.SOCKS5 else 0,  # æ˜¯å¦SOCKS5
                proxy.anonymity.value if hasattr(proxy.anonymity, 'value') else 1,  # åŒ¿åç­‰ç´š
            ]])
            
            # é æ¸¬
            predicted_score = self.performance_model.predict(features)[0]
            confidence = 0.8  # ç°¡åŒ–çš„ç½®ä¿¡åº¦è¨ˆç®—
            
            return {
                'predicted_score': round(predicted_score, 2),
                'confidence': round(confidence, 2)
            }
            
        except Exception as e:
            logger.error(f"âŒ é æ¸¬ä»£ç†æ€§èƒ½å¤±æ•—: {e}")
            return {'predicted_score': 0, 'confidence': 0}
