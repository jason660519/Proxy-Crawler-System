#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æ API ç«¯é»

æä¾›ä»£ç†å“è³ªåˆ†æã€æ€§èƒ½é æ¸¬å’Œè¶¨å‹¢åˆ†æç­‰ API æ¥å£
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import asyncio
import logging

from .proxy_quality_analyzer import ProxyQualityAnalyzer, AnalysisReport, QualityScore
from ..proxy_manager.models import ProxyNode, ProxyStatus, ProxyAnonymity, ProxyProtocol

logger = logging.getLogger(__name__)

# å‰µå»ºåˆ†æ API æ‡‰ç”¨ç¨‹å¼
analysis_api = FastAPI(
    title="Proxy Analysis API",
    description="ä»£ç†å“è³ªåˆ†æå’Œæ€§èƒ½é æ¸¬ API",
    version="1.0.0"
)

# åˆå§‹åŒ–åˆ†æå™¨
analyzer = ProxyQualityAnalyzer()


@analysis_api.get("/health")
async def health_check() -> Dict[str, Any]:
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy",
        "service": "proxy-analysis-api",
        "timestamp": datetime.now().isoformat()
    }


@analysis_api.get("/quality/analyze")
async def analyze_proxy_quality(
    limit: int = 50,
    include_recommendations: bool = True
) -> Dict[str, Any]:
    """åˆ†æä»£ç†å“è³ª
    
    Args:
        limit: åˆ†æä»£ç†æ•¸é‡é™åˆ¶
        include_recommendations: æ˜¯å¦åŒ…å«æ”¹é€²å»ºè­°
    """
    try:
        # æ¨¡æ“¬ä»£ç†æ•¸æ“š (å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰è©²å¾æ•¸æ“šåº«ç²å–)
        sample_proxies = _generate_sample_proxies(limit)
        
        # æ¨¡æ“¬æ­·å²æ•¸æ“š
        historical_data = _generate_sample_historical_data(sample_proxies)
        
        # åŸ·è¡Œåˆ†æ
        report = await analyzer.analyze_proxies(sample_proxies, historical_data)
        
        # æº–å‚™éŸ¿æ‡‰æ•¸æ“š
        response_data = {
            "success": True,
            "analysis_report": {
                "total_proxies": report.total_proxies,
                "analyzed_proxies": report.analyzed_proxies,
                "average_score": report.average_score,
                "score_distribution": report.score_distribution,
                "generated_at": report.generated_at.isoformat()
            },
            "top_proxies": [
                {
                    "proxy_id": score.proxy_id,
                    "overall_score": score.overall_score,
                    "performance_score": score.performance_score,
                    "reliability_score": score.reliability_score,
                    "security_score": score.security_score,
                    "location_score": score.location_score,
                    "recommendations": score.recommendations if include_recommendations else []
                }
                for score in report.top_proxies
            ],
            "worst_proxies": [
                {
                    "proxy_id": score.proxy_id,
                    "overall_score": score.overall_score,
                    "performance_score": score.performance_score,
                    "reliability_score": score.reliability_score,
                    "security_score": score.security_score,
                    "location_score": score.location_score,
                    "recommendations": score.recommendations if include_recommendations else []
                }
                for score in report.worst_proxies
            ],
            "global_recommendations": report.recommendations if include_recommendations else []
        }
        
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ ä»£ç†å“è³ªåˆ†æå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±æ•—: {str(e)}")


@analysis_api.get("/quality/trends")
async def get_quality_trends(days: int = 7) -> Dict[str, Any]:
    """ç²å–å“è³ªè¶¨å‹¢æ•¸æ“š
    
    Args:
        days: è¶¨å‹¢åˆ†æå¤©æ•¸
    """
    try:
        trends = analyzer.get_quality_trends(days)
        return {
            "success": True,
            "trends": trends,
            "period_days": days
        }
        
    except Exception as e:
        logger.error(f"âŒ ç²å–å“è³ªè¶¨å‹¢å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–è¶¨å‹¢å¤±æ•—: {str(e)}")


@analysis_api.post("/quality/predict")
async def predict_proxy_performance(
    proxy_data: Dict[str, Any]
) -> Dict[str, Any]:
    """é æ¸¬ä»£ç†æ€§èƒ½
    
    Args:
        proxy_data: ä»£ç†æ•¸æ“š
            - host: ä»£ç†ä¸»æ©Ÿ
            - port: ä»£ç†ç«¯å£
            - protocol: ä»£ç†å”è­°
            - anonymity: åŒ¿åç­‰ç´š
    """
    try:
        # å‰µå»ºä»£ç†å°è±¡
        proxy = ProxyNode(
            host=proxy_data.get("host", "127.0.0.1"),
            port=proxy_data.get("port", 8080),
            protocol=ProxyProtocol(proxy_data.get("protocol", "HTTP")),
            anonymity=ProxyAnonymity(proxy_data.get("anonymity", "ANONYMOUS")),
            country=proxy_data.get("country", "United States"),
            status=ProxyStatus.ACTIVE
        )
        
        # é æ¸¬æ€§èƒ½
        prediction = analyzer.predict_proxy_performance(proxy)
        
        return {
            "success": True,
            "proxy_id": f"{proxy.host}:{proxy.port}",
            "prediction": prediction,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ä»£ç†æ€§èƒ½é æ¸¬å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"é æ¸¬å¤±æ•—: {str(e)}")


@analysis_api.get("/quality/statistics")
async def get_quality_statistics() -> Dict[str, Any]:
    """ç²å–å“è³ªçµ±è¨ˆæ•¸æ“š"""
    try:
        # æ¨¡æ“¬çµ±è¨ˆæ•¸æ“š
        stats = {
            "total_analyzed": 150,
            "average_score": 75.5,
            "score_distribution": {
                "å„ªç§€ (90-100)": 15,
                "è‰¯å¥½ (80-89)": 35,
                "ä¸€èˆ¬ (70-79)": 45,
                "è¼ƒå·® (60-69)": 30,
                "å¾ˆå·® (0-59)": 25
            },
            "performance_metrics": {
                "average_response_time": 1200,
                "average_success_rate": 0.85,
                "average_uptime": 0.92
            },
            "top_countries": [
                {"country": "US", "count": 45, "avg_score": 78.5},
                {"country": "DE", "count": 32, "avg_score": 82.1},
                {"country": "GB", "count": 28, "avg_score": 79.8},
                {"country": "CA", "count": 25, "avg_score": 76.2},
                {"country": "FR", "count": 20, "avg_score": 74.9}
            ],
            "protocol_distribution": {
                "HTTP": 60,
                "HTTPS": 35,
                "SOCKS4": 3,
                "SOCKS5": 2
            },
            "anonymity_distribution": {
                "TRANSPARENT": 10,
                "ANONYMOUS": 70,
                "ELITE": 20
            }
        }
        
        return {
            "success": True,
            "statistics": stats,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ç²å–å“è³ªçµ±è¨ˆå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–çµ±è¨ˆå¤±æ•—: {str(e)}")


@analysis_api.post("/quality/batch-analyze")
async def batch_analyze_proxies(
    proxy_list: List[Dict[str, Any]],
    background_tasks: BackgroundTasks
) -> Dict[str, Any]:
    """æ‰¹é‡åˆ†æä»£ç†å“è³ª
    
    Args:
        proxy_list: ä»£ç†åˆ—è¡¨
        background_tasks: å¾Œå°ä»»å‹™
    """
    try:
        # å‰µå»ºä»£ç†å°è±¡åˆ—è¡¨
        proxies = []
        for proxy_data in proxy_list:
            proxy = ProxyNode(
                host=proxy_data.get("host", "127.0.0.1"),
                port=proxy_data.get("port", 8080),
                protocol=ProxyProtocol(proxy_data.get("protocol", "HTTP")),
                anonymity=ProxyAnonymity(proxy_data.get("anonymity", "ANONYMOUS")),
                country=proxy_data.get("country", "United States"),
                status=ProxyStatus.ACTIVE
            )
            proxies.append(proxy)
        
        # æ·»åŠ å¾Œå°ä»»å‹™
        background_tasks.add_task(_run_batch_analysis, proxies)
        
        return {
            "success": True,
            "message": f"å·²é–‹å§‹æ‰¹é‡åˆ†æ {len(proxies)} å€‹ä»£ç†",
            "task_id": f"batch_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡åˆ†æå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ†æå¤±æ•—: {str(e)}")


async def _run_batch_analysis(proxies: List[ProxyNode]):
    """åŸ·è¡Œæ‰¹é‡åˆ†æ (å¾Œå°ä»»å‹™)"""
    try:
        logger.info(f"ğŸ”„ é–‹å§‹å¾Œå°æ‰¹é‡åˆ†æ {len(proxies)} å€‹ä»£ç†")
        
        # æ¨¡æ“¬æ­·å²æ•¸æ“š
        historical_data = _generate_sample_historical_data(proxies)
        
        # åŸ·è¡Œåˆ†æ
        report = await analyzer.analyze_proxies(proxies, historical_data)
        
        logger.info(f"âœ… å¾Œå°æ‰¹é‡åˆ†æå®Œæˆï¼Œå¹³å‡è©•åˆ†: {report.average_score:.2f}")
        
    except Exception as e:
        logger.error(f"âŒ å¾Œå°æ‰¹é‡åˆ†æå¤±æ•—: {e}")


def _generate_sample_proxies(count: int) -> List[ProxyNode]:
    """ç”Ÿæˆæ¨£æœ¬ä»£ç†æ•¸æ“š"""
    import random
    
    countries = ['US', 'DE', 'GB', 'CA', 'FR', 'JP', 'AU', 'NL', 'SG', 'HK']
    protocols = [ProxyProtocol.HTTP, ProxyProtocol.HTTPS, ProxyProtocol.SOCKS4, ProxyProtocol.SOCKS5]
    anonymity_levels = [ProxyAnonymity.TRANSPARENT, ProxyAnonymity.ANONYMOUS, ProxyAnonymity.ELITE]
    
    proxies = []
    for i in range(count):
        proxy = ProxyNode(
            host=f"192.168.1.{i % 255}",
            port=8080 + (i % 1000),
            protocol=random.choice(protocols),
            anonymity=random.choice(anonymity_levels),
            country=random.choice(countries),
            status=ProxyStatus.ACTIVE
        )
        proxies.append(proxy)
    
    return proxies


def _generate_sample_historical_data(proxies: List[ProxyNode]) -> Dict[str, Any]:
    """ç”Ÿæˆæ¨£æœ¬æ­·å²æ•¸æ“š"""
    import random
    
    historical_data = {}
    for proxy in proxies:
        proxy_id = f"{proxy.host}:{proxy.port}"
        historical_data[proxy_id] = {
            'response_times': [random.randint(500, 3000) for _ in range(10)],
            'success_count': random.randint(5, 10),
            'total_requests': 10,
            'last_checked': datetime.now().isoformat()
        }
    
    return historical_data
