#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÂàÜÊûê API Á´ØÈªû

Êèê‰æõ‰ª£ÁêÜÂìÅË≥™ÂàÜÊûê„ÄÅÊÄßËÉΩÈ†êÊ∏¨ÂíåË∂®Âã¢ÂàÜÊûêÁ≠â API Êé•Âè£
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import asyncio
import logging

from .proxy_quality_analyzer import ProxyQualityAnalyzer, AnalysisReport, QualityScore
from ..proxy_manager.models import ProxyNode, ProxyStatus, ProxyAnonymity, ProxyProtocol

logger = logging.getLogger(__name__)

# ÂâµÂª∫ÂàÜÊûê API ÊáâÁî®Á®ãÂºè
analysis_api = FastAPI(
    title="Proxy Analysis API",
    description="‰ª£ÁêÜÂìÅË≥™ÂàÜÊûêÂíåÊÄßËÉΩÈ†êÊ∏¨ API",
    version="1.0.0"
)

# ÂàùÂßãÂåñÂàÜÊûêÂô®
analyzer = ProxyQualityAnalyzer()


@analysis_api.get("/health")
async def health_check() -> Dict[str, Any]:
    """ÂÅ•Â∫∑Ê™¢Êü•"""
    return {
        "status": "healthy",
        "service": "proxy-analysis-api",
        "timestamp": datetime.now().isoformat()
    }


@analysis_api.get("/quality/analyze")
async def analyze_proxy_quality(
    limit: int = 50,
    include_recommendations: bool = True
) -> Dict[str, Any]:
    """ÂàÜÊûê‰ª£ÁêÜÂìÅË≥™
    
    Args:
        limit: ÂàÜÊûê‰ª£ÁêÜÊï∏ÈáèÈôêÂà∂
        include_recommendations: ÊòØÂê¶ÂåÖÂê´ÊîπÈÄ≤Âª∫Ë≠∞
    """
    try:
        # Ê®°Êì¨‰ª£ÁêÜÊï∏Êìö (ÂØ¶ÈöõÊáâÁî®‰∏≠ÊáâË©≤ÂæûÊï∏ÊìöÂ∫´Áç≤Âèñ)
        sample_proxies = _generate_sample_proxies(limit)
        
        # Ê®°Êì¨Ê≠∑Âè≤Êï∏Êìö
        historical_data = _generate_sample_historical_data(sample_proxies)
        
        # Âü∑Ë°åÂàÜÊûê
        report = await analyzer.analyze_proxies(sample_proxies, historical_data)
        
        # Ê∫ñÂÇôÈüøÊáâÊï∏Êìö
        response_data = {
            "success": True,
            "analysis_report": {
                "total_proxies": report.total_proxies,
                "analyzed_proxies": report.analyzed_proxies,
                "average_score": report.average_score,
                "score_distribution": report.score_distribution,
                "generated_at": report.generated_at.isoformat()
            },
            "top_proxies": [
                {
                    "proxy_id": score.proxy_id,
                    "overall_score": score.overall_score,
                    "performance_score": score.performance_score,
                    "reliability_score": score.reliability_score,
                    "security_score": score.security_score,
                    "location_score": score.location_score,
                    "recommendations": score.recommendations if include_recommendations else []
                }
                for score in report.top_proxies
            ],
            "worst_proxies": [
                {
                    "proxy_id": score.proxy_id,
                    "overall_score": score.overall_score,
                    "performance_score": score.performance_score,
                    "reliability_score": score.reliability_score,
                    "security_score": score.security_score,
                    "location_score": score.location_score,
                    "recommendations": score.recommendations if include_recommendations else []
                }
                for score in report.worst_proxies
            ],
            "global_recommendations": report.recommendations if include_recommendations else []
        }
        
        return response_data
        
    except Exception as e:
        logger.error(f"‚ùå ‰ª£ÁêÜÂìÅË≥™ÂàÜÊûêÂ§±Êïó: {e}")
        raise HTTPException(status_code=500, detail=f"ÂàÜÊûêÂ§±Êïó: {str(e)}")


@analysis_api.get("/quality/trends")
async def get_quality_trends(days: int = 7) -> Dict[str, Any]:
    """Áç≤ÂèñÂìÅË≥™Ë∂®Âã¢Êï∏Êìö
    
    Args:
        days: Ë∂®Âã¢ÂàÜÊûêÂ§©Êï∏
    """
    try:
        trends = analyzer.get_quality_trends(days)
        return {
            "success": True,
            "trends": trends,
            "period_days": days
        }
        
    except Exception as e:
        logger.error(f"‚ùå Áç≤ÂèñÂìÅË≥™Ë∂®Âã¢Â§±Êïó: {e}")
        raise HTTPException(status_code=500, detail=f"Áç≤ÂèñË∂®Âã¢Â§±Êïó: {str(e)}")


@analysis_api.post("/quality/predict")
async def predict_proxy_performance(
    proxy_data: Dict[str, Any]
) -> Dict[str, Any]:
    """È†êÊ∏¨‰ª£ÁêÜÊÄßËÉΩ
    
    Args:
        proxy_data: ‰ª£ÁêÜÊï∏Êìö
            - host: ‰ª£ÁêÜ‰∏ªÊ©ü
            - port: ‰ª£ÁêÜÁ´ØÂè£
            - protocol: ‰ª£ÁêÜÂçîË≠∞
            - anonymity: ÂåøÂêçÁ≠âÁ¥ö
    """
    try:
        # ÂâµÂª∫‰ª£ÁêÜÂ∞çË±°
        proxy = ProxyNode(
            host=proxy_data.get("host", "127.0.0.1"),
            port=proxy_data.get("port", 8080),
            protocol=ProxyProtocol(proxy_data.get("protocol", "HTTP")),
            anonymity=ProxyAnonymity(proxy_data.get("anonymity", "ANONYMOUS")),
            country=proxy_data.get("country", "United States"),
            status=ProxyStatus.ACTIVE
        )
        
        # È†êÊ∏¨ÊÄßËÉΩ
        prediction = analyzer.predict_proxy_performance(proxy)
        
        return {
            "success": True,
            "proxy_id": f"{proxy.host}:{proxy.port}",
            "prediction": prediction,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå ‰ª£ÁêÜÊÄßËÉΩÈ†êÊ∏¨Â§±Êïó: {e}")
        raise HTTPException(status_code=500, detail=f"È†êÊ∏¨Â§±Êïó: {str(e)}")


@analysis_api.get("/quality/statistics")
async def get_quality_statistics() -> Dict[str, Any]:
    """Áç≤ÂèñÂìÅË≥™Áµ±Ë®àÊï∏Êìö"""
    try:
        # Ê®°Êì¨Áµ±Ë®àÊï∏Êìö
        stats = {
            "total_analyzed": 150,
            "average_score": 75.5,
            "score_distribution": {
                "ÂÑ™ÁßÄ (90-100)": 15,
                "ËâØÂ•Ω (80-89)": 35,
                "‰∏ÄËà¨ (70-79)": 45,
                "ËºÉÂ∑Æ (60-69)": 30,
                "ÂæàÂ∑Æ (0-59)": 25
            },
            "performance_metrics": {
                "average_response_time": 1200,
                "average_success_rate": 0.85,
                "average_uptime": 0.92
            },
            "top_countries": [
                {"country": "US", "count": 45, "avg_score": 78.5},
                {"country": "DE", "count": 32, "avg_score": 82.1},
                {"country": "GB", "count": 28, "avg_score": 79.8},
                {"country": "CA", "count": 25, "avg_score": 76.2},
                {"country": "FR", "count": 20, "avg_score": 74.9}
            ],
            "protocol_distribution": {
                "HTTP": 60,
                "HTTPS": 35,
                "SOCKS4": 3,
                "SOCKS5": 2
            },
            "anonymity_distribution": {
                "TRANSPARENT": 10,
                "ANONYMOUS": 70,
                "ELITE": 20
            }
        }
        
        return {
            "success": True,
            "statistics": stats,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Áç≤ÂèñÂìÅË≥™Áµ±Ë®àÂ§±Êïó: {e}")
        raise HTTPException(status_code=500, detail=f"Áç≤ÂèñÁµ±Ë®àÂ§±Êïó: {str(e)}")


@analysis_api.post("/quality/batch-analyze")
async def batch_analyze_proxies(
    proxy_list: List[Dict[str, Any]],
    background_tasks: BackgroundTasks
) -> Dict[str, Any]:
    """ÊâπÈáèÂàÜÊûê‰ª£ÁêÜÂìÅË≥™
    
    Args:
        proxy_list: ‰ª£ÁêÜÂàóË°®
        background_tasks: ÂæåÂè∞‰ªªÂãô
    """
    try:
        # ÂâµÂª∫‰ª£ÁêÜÂ∞çË±°ÂàóË°®
        proxies = []
        for proxy_data in proxy_list:
            proxy = ProxyNode(
                host=proxy_data.get("host", "127.0.0.1"),
                port=proxy_data.get("port", 8080),
                protocol=ProxyProtocol(proxy_data.get("protocol", "HTTP")),
                anonymity=ProxyAnonymity(proxy_data.get("anonymity", "ANONYMOUS")),
                country=proxy_data.get("country", "United States"),
                status=ProxyStatus.ACTIVE
            )
            proxies.append(proxy)
        
        # Ê∑ªÂä†ÂæåÂè∞‰ªªÂãô
        background_tasks.add_task(_run_batch_analysis, proxies)
        
        return {
            "success": True,
            "message": f"Â∑≤ÈñãÂßãÊâπÈáèÂàÜÊûê {len(proxies)} ÂÄã‰ª£ÁêÜ",
            "task_id": f"batch_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå ÊâπÈáèÂàÜÊûêÂ§±Êïó: {e}")
        raise HTTPException(status_code=500, detail=f"ÊâπÈáèÂàÜÊûêÂ§±Êïó: {str(e)}")


async def _run_batch_analysis(proxies: List[ProxyNode]):
    """Âü∑Ë°åÊâπÈáèÂàÜÊûê (ÂæåÂè∞‰ªªÂãô)"""
    try:
        logger.info(f"üîÑ ÈñãÂßãÂæåÂè∞ÊâπÈáèÂàÜÊûê {len(proxies)} ÂÄã‰ª£ÁêÜ")
        
        # Ê®°Êì¨Ê≠∑Âè≤Êï∏Êìö
        historical_data = _generate_sample_historical_data(proxies)
        
        # Âü∑Ë°åÂàÜÊûê
        report = await analyzer.analyze_proxies(proxies, historical_data)
        
        logger.info(f"‚úÖ ÂæåÂè∞ÊâπÈáèÂàÜÊûêÂÆåÊàêÔºåÂπ≥ÂùáË©ïÂàÜ: {report.average_score:.2f}")
        
    except Exception as e:
        logger.error(f"‚ùå ÂæåÂè∞ÊâπÈáèÂàÜÊûêÂ§±Êïó: {e}")


def _generate_sample_proxies(count: int) -> List[ProxyNode]:
    """ÁîüÊàêÊ®£Êú¨‰ª£ÁêÜÊï∏Êìö"""
    import random
    
    countries = ['US', 'DE', 'GB', 'CA', 'FR', 'JP', 'AU', 'NL', 'SG', 'HK']
    protocols = [ProxyProtocol.HTTP, ProxyProtocol.HTTPS, ProxyProtocol.SOCKS4, ProxyProtocol.SOCKS5]
    anonymity_levels = [ProxyAnonymity.TRANSPARENT, ProxyAnonymity.ANONYMOUS, ProxyAnonymity.ELITE]
    
    proxies = []
    for i in range(count):
        proxy = ProxyNode(
            host=f"192.168.1.{i % 255}",
            port=8080 + (i % 1000),
            protocol=random.choice(protocols),
            anonymity=random.choice(anonymity_levels),
            country=random.choice(countries),
            status=ProxyStatus.ACTIVE
        )
        proxies.append(proxy)
    
    return proxies


def _generate_sample_historical_data(proxies: List[ProxyNode]) -> Dict[str, Any]:
    """ÁîüÊàêÊ®£Êú¨Ê≠∑Âè≤Êï∏Êìö"""
    import random
    
    historical_data = {}
    for proxy in proxies:
        proxy_id = f"{proxy.host}:{proxy.port}"
        historical_data[proxy_id] = {
            'response_times': [random.randint(500, 3000) for _ in range(10)],
            'success_count': random.randint(5, 10),
            'total_requests': 10,
            'last_checked': datetime.now().isoformat()
        }
    
    return historical_data
