import time
from pathlib import Path
from typing import Dict, Any

import httpx

from ..config import PipelineOptions
from ..checksum import compute_checksum


class Url2ParquetPipeline:
    def __init__(self, options: PipelineOptions):
        self.options = options
        self.base = Path(options.work_dir)
        (self.base / "jobs").mkdir(parents=True, exist_ok=True)

    async def process_single(self, url: str) -> Dict[str, Any]:
        started = time.time()
        checksum = compute_checksum(url, vars(self.options)).replace("sha256:", "")

        # Fetch with redirect handling - ç¢ºä¿æ°¸é ä¸æœƒæ‹‹å‡ºhttpxéŒ¯èª¤
        html = None
        final_url = url
        
        try:
            async with httpx.AsyncClient(
                timeout=self.options.timeout_seconds,
                follow_redirects=False,  # ä¸è‡ªå‹•è·Ÿéš¨é‡å®šå‘
                max_redirects=0
            ) as client:
                resp = await client.get(url, headers={"User-Agent": self.options.user_agent})
                
                # æª¢æŸ¥é‡å®šå‘ç‹€æ…‹ç¢¼
                if resp.status_code in [301, 302, 303, 307, 308]:
                    redirect_url = resp.headers.get("location")
                    if redirect_url:
                        # ç¢ºä¿é‡å®šå‘URLæ˜¯å®Œæ•´çš„
                        if redirect_url.startswith('/'):
                            from urllib.parse import urljoin
                            redirect_url = urljoin(url, redirect_url)
                        
                        return {
                            "status": "redirected",
                            "original_url": url,
                            "final_url": redirect_url,
                            "message": f"URLå·²è·³è½‰è‡³ {redirect_url}ï¼Œæ˜¯å¦ç¹¼çºŒæ¸¬è©¦æ–°çš„URLï¼Ÿ"
                        }
                
                # åªæœ‰åœ¨éé‡å®šå‘ç‹€æ…‹ç¢¼æ™‚æ‰æª¢æŸ¥ç‹€æ…‹
                if resp.status_code not in [301, 302, 303, 307, 308]:
                    resp.raise_for_status()
                html = resp.text
                final_url = str(resp.url)
                
                # Check if URL was redirected (å³ä½¿ç‹€æ…‹ç¢¼ä¸æ˜¯é‡å®šå‘)
                if final_url != url:
                    return {
                        "status": "redirected",
                        "original_url": url,
                        "final_url": final_url,
                        "message": f"URLå·²è·³è½‰è‡³ {final_url}ï¼Œæ˜¯å¦ç¹¼çºŒæ¸¬è©¦æ–°çš„URLï¼Ÿ"
                    }
                        
        except Exception as e:
            # çµ±ä¸€è™•ç†æ‰€æœ‰éŒ¯èª¤ï¼ŒåŒ…æ‹¬httpxéŒ¯èª¤
            error_msg = str(e)
            print(f"ğŸ” è™•ç†URL {url} æ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé‡å®šå‘éŒ¯èª¤
            if ("Redirect response" in error_msg and "Redirect location:" in error_msg) or \
               ("301" in error_msg or "302" in error_msg or "303" in error_msg or "307" in error_msg or "308" in error_msg):
                # å¾éŒ¯èª¤è¨Šæ¯ä¸­æå–é‡å®šå‘URL
                import re
                match = re.search(r"Redirect location: '([^']+)'", error_msg)
                if match:
                    redirect_url = match.group(1)
                    return {
                        "status": "redirected",
                        "original_url": url,
                        "final_url": redirect_url,
                        "message": f"URLå·²è·³è½‰è‡³ {redirect_url}ï¼Œæ˜¯å¦ç¹¼çºŒæ¸¬è©¦æ–°çš„URLï¼Ÿ"
                    }
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºhttpx.HTTPStatusError
            if hasattr(e, 'response') and hasattr(e.response, 'status_code'):
                if e.response.status_code in [301, 302, 303, 307, 308]:
                    redirect_url = e.response.headers.get("location")
                    if redirect_url:
                        # ç¢ºä¿é‡å®šå‘URLæ˜¯å®Œæ•´çš„
                        if redirect_url.startswith('/'):
                            from urllib.parse import urljoin
                            redirect_url = urljoin(url, redirect_url)
                        
                        return {
                            "status": "redirected",
                            "original_url": url,
                            "final_url": redirect_url,
                            "message": f"URLå·²è·³è½‰è‡³ {redirect_url}ï¼Œæ˜¯å¦ç¹¼çºŒæ¸¬è©¦æ–°çš„URLï¼Ÿ"
                        }
            
            # å¦‚æœä¸æ˜¯é‡å®šå‘éŒ¯èª¤ï¼Œå‰‡æ‹‹å‡º
            raise
        
        # å¦‚æœæ²’æœ‰ç²å–åˆ°HTMLï¼Œå‰‡è¿”å›éŒ¯èª¤
        if html is None:
            raise ValueError(f"ç„¡æ³•ç²å–URLå…§å®¹: {url}")

        # Transform: trivial extract to markdown via markdownify if available; else plain text fallback
        try:
            from markdownify import markdownify

            markdown = markdownify(html)
        except Exception:
            # naive fallback
            markdown = html

        # Output: write minimal files according to formats
        files = []
        out_dir = self.base / "outputs"
        out_dir.mkdir(parents=True, exist_ok=True)

        if "md" in self.options.output_formats or "markdown" in self.options.output_formats:
            md_path = out_dir / f"{checksum[7:15]}_content.md"
            md_path.write_text(markdown, encoding="utf-8")
            files.append({"format": "md", "path": str(md_path), "size": md_path.stat().st_size})

        if "json" in self.options.output_formats:
            import json

            json_path = out_dir / f"{checksum[7:15]}_content.json"
            json_path.write_text(
                json.dumps({"url": url, "markdown": markdown}, ensure_ascii=False),
                encoding="utf-8",
            )
            files.append({"format": "json", "path": str(json_path), "size": json_path.stat().st_size})

        # CSV output (optional)
        if "csv" in self.options.output_formats:
            try:
                import pandas as pd  # type: ignore

                csv_path = out_dir / f"{checksum[7:15]}_content.csv"
                row = {
                    "url": url,
                    "markdown": markdown,
                    "engine": self.options.engine,
                    "checksum": checksum,
                    "fetched_at": time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime()),
                    "content_length": len(html or ""),
                }
                pd.DataFrame([row]).to_csv(csv_path, index=False)
                files.append({"format": "csv", "path": str(csv_path), "size": csv_path.stat().st_size})
            except Exception as e:
                print(f"CSV è¼¸å‡ºå¤±æ•—: {e}")

        # Parquet output (optional)
        if "parquet" in self.options.output_formats:
            try:
                import pandas as pd  # type: ignore

                parquet_path = out_dir / f"{checksum[7:15]}_content.parquet"
                row = {
                    "url": url,
                    "markdown": markdown,
                    "engine": self.options.engine,
                    "checksum": checksum,
                    "fetched_at": time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime()),
                    "content_length": len(html or ""),
                }
                # ä½¿ç”¨ pyarrow å¼•æ“å¯«å…¥ Parquet
                pd.DataFrame([row]).to_parquet(parquet_path, index=False, engine="pyarrow")
                files.append({"format": "parquet", "path": str(parquet_path), "size": parquet_path.stat().st_size})
            except Exception as e:
                print(f"Parquet è¼¸å‡ºå¤±æ•—: {e}")

        elapsed_ms = int((time.time() - started) * 1000)
        return {
            "url": url,
            "files": files,
            "checksum": checksum,
            "engine": self.options.engine,
            "processing_time_ms": elapsed_ms,
        }

