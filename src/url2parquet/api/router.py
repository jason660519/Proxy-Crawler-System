from fastapi import APIRouter, HTTPException
from fastapi.responses import FileResponse, StreamingResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import time
import os

from ..config import PipelineOptions
from ..types import JobStatus, JobResult
from ..core.pipeline import Url2ParquetPipeline
from ..core.cache import FileCache
from ..checksum import compute_checksum
import httpx
from pathlib import Path
import pathlib


router = APIRouter(prefix="/api/url2parquet", tags=["URL2Parquet"])

_jobs: Dict[str, JobResult] = {}


class CreateJobRequest(BaseModel):
    # æ–°å¾Œç«¯æ ¼å¼ï¼ˆæ¨è–¦ï¼‰
    urls: Optional[List[str]] = None
    engine: Optional[str] = "smart"
    output_formats: Optional[List[str]] = ["parquet"]
    obey_robots_txt: Optional[bool] = True
    timeout_seconds: Optional[int] = 20
    max_concurrency: Optional[int] = 4
    work_dir: Optional[str] = "data/url2parquet"

    # èˆŠå‰ç«¯ç›¸å®¹å±¤
    url: Optional[str] = None
    options: Optional[Dict[str, Any]] = None


@router.post("/jobs")
async def create_job(req: CreateJobRequest) -> Dict[str, Any]:
    job_id = f"job_{int(time.time()*1000)}"
    _jobs[job_id] = JobResult(job_id=job_id, status=JobStatus.queued)

    # ç›¸å®¹è™•ç†ï¼šæ”¯æ´ {url, options:{output_formats}} èˆŠæ ¼å¼
    urls_list: List[str] = []
    if req.urls:
        urls_list = req.urls
    elif req.url:
        urls_list = [req.url]

    # åˆä½µ options èˆ‡é ‚å±¤æ¬„ä½
    effective_formats = req.output_formats or []
    if req.options and isinstance(req.options, dict):
        if isinstance(req.options.get("output_formats"), list):
            effective_formats = req.options.get("output_formats")

    options = PipelineOptions(
        urls=urls_list,
        engine=req.engine or "smart",
        output_formats=effective_formats or ["parquet"],
        obey_robots_txt=bool(req.obey_robots_txt),
        timeout_seconds=int(req.timeout_seconds or 20),
        max_concurrency=int(req.max_concurrency or 4),
        work_dir=req.work_dir or "data/url2parquet",
    )

    cache = FileCache(options.work_dir)
    pipeline = Url2ParquetPipeline(options)

    # Process URLs and handle redirects
    _jobs[job_id].status = JobStatus.processing
    try:
        if not options.urls:
            raise ValueError("No URL provided")
        
        # Process all URLs
        results = []
        redirects = []
        
        for url in options.urls:
            try:
                checksum = compute_checksum(url, vars(options)).replace("sha256:", "")
                cached = cache.hit(checksum)
                
                if cached:
                    print(f"ğŸ“¦ ä½¿ç”¨å¿«å–çµæœ: {url}")
                    result = cached
                else:
                    print(f"ğŸŒ è™•ç†æ–°URL: {url}")
                    result = await pipeline.process_single(url)
                    print(f"ğŸ’¾ å„²å­˜çµæœåˆ°å¿«å–: {result}")
                    cache.save(checksum, result)
                
                # Check if URL was redirected
                print(f"ğŸ” æª¢æŸ¥URL {url} çš„çµæœ: {result}")
                if result.get("status") == "redirected":
                    print(f"ğŸ”„ æª¢æ¸¬åˆ°é‡å®šå‘: {result}")
                    redirects.append(result)
                else:
                    print(f"âœ… æ­£å¸¸çµæœ: {result}")
                    results.append(result)
            except httpx.HTTPStatusError as e:
                # å°ˆé–€è™•ç†httpx HTTPéŒ¯èª¤
                if e.response.status_code in [301, 302, 303, 307, 308]:
                    redirect_url = e.response.headers.get("location")
                    if redirect_url:
                        redirect_result = {
                            "url": url,
                            "status": "redirected",
                            "original_url": url,
                            "final_url": redirect_url,
                            "message": f"URLå·²è·³è½‰è‡³ {redirect_url}ï¼Œæ˜¯å¦ç¹¼çºŒæ¸¬è©¦æ–°çš„URLï¼Ÿ"
                        }
                        redirects.append(redirect_result)
                        print(f"ğŸ”„ æª¢æ¸¬åˆ°HTTPé‡å®šå‘ {url} -> {redirect_url}")
                        continue
                print(f"HTTPéŒ¯èª¤ {url}: {e}")
                continue
            except Exception as e:
                # æª¢æŸ¥éŒ¯èª¤æ˜¯å¦åŒ…å«é‡å®šå‘ä¿¡æ¯
                error_msg = str(e)
                if "Redirect response" in error_msg and "Redirect location:" in error_msg:
                    # å¾éŒ¯èª¤è¨Šæ¯ä¸­æå–é‡å®šå‘URL
                    import re
                    match = re.search(r"Redirect location: '([^']+)'", error_msg)
                    if match:
                        redirect_url = match.group(1)
                        redirect_result = {
                            "url": url,
                            "status": "redirected",
                            "original_url": url,
                            "final_url": redirect_url,
                            "message": f"URLå·²è·³è½‰è‡³ {redirect_url}ï¼Œæ˜¯å¦ç¹¼çºŒæ¸¬è©¦æ–°çš„URLï¼Ÿ"
                        }
                        redirects.append(redirect_result)
                        print(f"ğŸ”„ æª¢æ¸¬åˆ°é‡å®šå‘ {url} -> {redirect_url}")
                        continue
                
                # å¦‚æœå–®å€‹URLè™•ç†å¤±æ•—ï¼Œè¨˜éŒ„éŒ¯èª¤ä½†ç¹¼çºŒè™•ç†å…¶ä»–URL
                print(f"è™•ç†URL {url} æ™‚å‡ºéŒ¯: {e}")
                continue

        # If there are redirects, return redirect information
        if redirects:
            return {
                "job_id": job_id,
                "status": "redirected",
                "redirects": redirects,
                "message": "æª¢æ¸¬åˆ°URLé‡å®šå‘ï¼Œè«‹ç¢ºèªæ˜¯å¦ç¹¼çºŒ"
            }

        # Process successful results
        if results:
            result = results[0]  # Use first successful result
            _jobs[job_id].status = JobStatus.completed
            _jobs[job_id].url = result.get("url", options.urls[0])
            _jobs[job_id].checksum = result.get("checksum")
            _jobs[job_id].files = result.get("files", [])
            _jobs[job_id].engine = result.get("engine")
            _jobs[job_id].processing_time_ms = result.get("processing_time_ms")
            
            return {
                "job_id": job_id,
                "status": "completed",
                "result": {
                    "files": result.get("files", [])
                }
            }
        else:
            raise ValueError("No successful results")

    except Exception as e:
        _jobs[job_id].status = JobStatus.failed
        _jobs[job_id].error = str(e)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/jobs/{job_id}")
async def get_job(job_id: str) -> JobResult:
    job = _jobs.get(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job not found")
    return job


@router.get("/jobs/{job_id}/download")
async def download_info(job_id: str) -> Dict[str, Any]:
    job = _jobs.get(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job not found")
    return {"files": job.files}


@router.get("/jobs/{job_id}/files/{format}")
async def get_file_content(job_id: str, format: str) -> Dict[str, Any]:
    job = _jobs.get(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job not found")
    
    # æ‰¾åˆ°å°æ‡‰æ ¼å¼çš„æ–‡ä»¶
    file_info = None
    for file in job.files:
        if file.get("format") == format:
            file_info = file
            break
    
    if not file_info:
        raise HTTPException(status_code=404, detail=f"file with format {format} not found")
    
    try:
        # è®€å–æ–‡ä»¶å…§å®¹
        with open(file_info["path"], "r", encoding="utf-8") as f:
            content = f.read()
        
        return {
            "format": format,
            "content": content,
            "size": file_info.get("size", 0)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to read file: {str(e)}")


@router.get("/jobs/{job_id}/files/{format}/download")
async def download_file(job_id: str, format: str):
    """ä»¥æ­£ç¢ºçš„æª”æ¡ˆå‹æ…‹ä¸²æµä¸‹è¼‰ç”Ÿæˆçš„æª”æ¡ˆï¼ˆå« parquet äºŒé€²ä½ï¼‰ã€‚"""
    job = _jobs.get(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job not found")

    # æ‰¾åˆ°å°æ‡‰æ ¼å¼çš„æ–‡ä»¶
    file_info = None
    for file in job.files:
        if file.get("format") == format:
            file_info = file
            break

    if not file_info:
        raise HTTPException(status_code=404, detail=f"file with format {format} not found")

    path = file_info.get("path")
    if not path or not os.path.exists(path):
        raise HTTPException(status_code=404, detail="file not found on disk")

    filename = os.path.basename(path)

    # æ ¹æ“šæ ¼å¼æ±ºå®š MIME é¡å‹
    content_type_map = {
        "md": "text/markdown; charset=utf-8",
        "json": "application/json; charset=utf-8",
        "parquet": "application/octet-stream",
        "csv": "text/csv; charset=utf-8",
        "txt": "text/plain; charset=utf-8",
    }
    media_type = content_type_map.get(format.lower(), "application/octet-stream")

    try:
        return FileResponse(
            path,
            media_type=media_type,
            filename=filename
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to stream file: {str(e)}")


@router.post("/jobs/{job_id}/confirm-redirect")
async def confirm_redirect(job_id: str, redirect_urls: List[str]) -> Dict[str, Any]:
    """ç¢ºèªé‡å®šå‘ä¸¦ä½¿ç”¨æ–°çš„URLé‡æ–°è™•ç†"""
    job = _jobs.get(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job not found")
    
    if job.status != JobStatus.processing:
        raise HTTPException(status_code=400, detail="job not in processing state")
    
    try:
        # é‡æ–°è™•ç†é‡å®šå‘çš„URL
        options = PipelineOptions(
            urls=redirect_urls,
            engine="smart",
            output_formats=["md", "json", "parquet"],
            obey_robots_txt=True,
            timeout_seconds=30,
            max_concurrency=4,
            work_dir="data/url2parquet",
        )
        
        cache = FileCache(options.work_dir)
        pipeline = Url2ParquetPipeline(options)
        
        results = []
        for url in redirect_urls:
            checksum = compute_checksum(url, vars(options)).replace("sha256:", "")
            cached = cache.hit(checksum)
            
            if cached:
                result = cached
            else:
                result = await pipeline.process_single(url)
                cache.save(checksum, result)
            
            results.append(result)
        
        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        if results:
            result = results[0]
            _jobs[job_id].status = JobStatus.completed
            _jobs[job_id].url = result.get("url", redirect_urls[0])
            _jobs[job_id].checksum = result.get("checksum")
            _jobs[job_id].files = result.get("files", [])
            _jobs[job_id].engine = result.get("engine")
            _jobs[job_id].processing_time_ms = result.get("processing_time_ms")
            
            return {
                "job_id": job_id,
                "status": "completed",
                "result": {
                    "files": result.get("files", [])
                }
            }
        else:
            raise ValueError("No successful results")
            
    except Exception as e:
        _jobs[job_id].status = JobStatus.failed
        _jobs[job_id].error = str(e)
        raise HTTPException(status_code=500, detail=str(e))


# ============ æœ¬åœ°æ¨£æœ¬æª”ï¼ˆMarkdownï¼‰ç€è¦½ ============

def _outputs_dir(base_work_dir: Optional[str]) -> Path:
    root = Path(base_work_dir or "data/url2parquet").resolve()
    outputs = (root / "outputs").resolve()
    try:
        outputs.relative_to(root)
    except Exception:
        raise HTTPException(status_code=400, detail="invalid work_dir")
    outputs.mkdir(parents=True, exist_ok=True)
    return outputs


@router.get("/local-md")
async def list_local_markdown(work_dir: Optional[str] = None, limit: int = 50) -> Dict[str, Any]:
    """åˆ—å‡ºå·¥ä½œç›®éŒ„ä¸‹ outputs å…§çš„ .md æª”æ¡ˆï¼ˆä¾›å‰ç«¯æ‰‹å‹•è§£æï¼‰ã€‚"""
    outputs = _outputs_dir(work_dir)
    items = []
    try:
        md_files = sorted(outputs.rglob("*.md"), key=lambda p: p.stat().st_mtime, reverse=True)
        for p in md_files[: max(1, min(limit, 200))]:
            try:
                stat = p.stat()
                items.append({
                    "filename": str(p.relative_to(outputs).as_posix()),
                    "size": stat.st_size,
                    "modified": stat.st_mtime,
                })
            except Exception:
                continue
        return {"files": items}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list markdown files: {e}")


@router.get("/local-md/content")
async def read_local_markdown(filename: str, work_dir: Optional[str] = None) -> Dict[str, Any]:
    """è®€å– outputs å…§æŒ‡å®šçš„ .md æª”æ¡ˆå…§å®¹ã€‚"""
    outputs = _outputs_dir(work_dir)
    # é˜²ç›®éŒ„ç©¿è¶Šï¼Œåƒ…å…è¨±åœ¨ outputs å…§
    target = (outputs / filename).resolve()
    try:
        target.relative_to(outputs)
    except Exception:
        raise HTTPException(status_code=400, detail="invalid filename")
    if not target.is_file() or target.suffix.lower() != ".md":
        raise HTTPException(status_code=404, detail="file not found")
    try:
        content = target.read_text("utf-8", errors="ignore")
        return {"filename": filename, "content": content, "size": target.stat().st_size}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to read markdown: {e}")

