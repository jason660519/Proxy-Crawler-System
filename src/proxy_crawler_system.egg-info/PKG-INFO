Metadata-Version: 2.4
Name: proxy-crawler-system
Version: 1.0.0
Summary: 專業的代理伺服器爬取與管理系統
Author-email: Jason <jason@example.com>
Project-URL: Homepage, https://github.com/jason/proxy-crawler-system
Project-URL: Repository, https://github.com/jason/proxy-crawler-system.git
Project-URL: Issues, https://github.com/jason/proxy-crawler-system/issues
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.104.0
Requires-Dist: uvicorn[standard]>=0.24.0
Requires-Dist: python-multipart>=0.0.6
Requires-Dist: aiohttp>=3.8.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: requests>=2.31.0
Requires-Dist: beautifulsoup4>=4.12.0
Requires-Dist: lxml>=4.9.0
Requires-Dist: parsel>=1.8.0
Requires-Dist: markdownify>=0.11.6
Requires-Dist: trafilatura>=1.6.0
Requires-Dist: html2text>=2020.1.16
Requires-Dist: readability-lxml>=0.8.1
Requires-Dist: pydantic>=2.0.0
Requires-Dist: dataclasses-json>=0.6.0
Requires-Dist: pandas>=2.1.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: aiofiles>=23.0.0
Requires-Dist: sqlalchemy>=2.0.0
Requires-Dist: psycopg2-binary>=2.9.0
Requires-Dist: redis>=4.6.0
Requires-Dist: aiosqlite>=0.19.0
Requires-Dist: loguru>=0.7.0
Requires-Dist: structlog>=23.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: PyYAML>=6.0
Requires-Dist: tqdm>=4.66.0
Requires-Dist: rich>=13.0.0
Requires-Dist: click>=8.1.0
Requires-Dist: typer>=0.9.0
Requires-Dist: chardet>=5.0.0
Requires-Dist: python-dateutil>=2.8.0
Requires-Dist: textstat>=0.7.0
Requires-Dist: Pillow>=10.0.0
Requires-Dist: geopy>=2.4.0
Requires-Dist: geoip2>=4.8.0
Requires-Dist: jinja2>=3.1.6
Requires-Dist: asyncpg>=0.30.0
Requires-Dist: pydantic-settings>=2.10.1
Requires-Dist: aiohttp-cors>=0.8.1
Requires-Dist: prometheus-client>=0.22.1
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.0.280; extra == "dev"
Requires-Dist: mypy>=1.5.0; extra == "dev"
Provides-Extra: browser-automation
Requires-Dist: playwright>=1.40.0; extra == "browser-automation"
Requires-Dist: selenium>=4.15.0; extra == "browser-automation"
Requires-Dist: undetected-chromedriver>=3.5.5; extra == "browser-automation"
Requires-Dist: webdriver-manager>=4.0.1; extra == "browser-automation"
Provides-Extra: proxy-management
Requires-Dist: PySocks>=1.7.1; extra == "proxy-management"
Requires-Dist: fake-useragent>=1.4.0; extra == "proxy-management"
Provides-Extra: scrapy-framework
Requires-Dist: scrapy>=2.11.0; extra == "scrapy-framework"
Requires-Dist: scrapy-playwright>=0.0.28; extra == "scrapy-framework"
Provides-Extra: monitoring
Requires-Dist: prometheus-client>=0.19.0; extra == "monitoring"
Requires-Dist: tenacity>=8.2.3; extra == "monitoring"
Requires-Dist: retrying>=1.3.4; extra == "monitoring"

# JasonSpider - 代理爬蟲與管理系統

一個功能完整的代理爬蟲與管理系統，支援多來源代理獲取、HTML 轉 Markdown 服務，以及智能代理管理功能。

## 🚀 主要功能

### 1. HTML to Markdown 轉換服務

- 支援多種轉換引擎（markdownify、html2text、pandoc）
- RESTful API 介面
- 批量轉換支援
- 自動檔案儲存與時間戳記

### 2. 代理網站監控

- 多來源代理網站可用性檢查
- 支援 SSL Proxies、Geonode、GitHub 專案等
- 異步並發檢查機制
- 詳細的狀態報告

### 3. 智能代理管理系統 ✅

- **多來源代理獲取**：支援 ProxyScrape、GitHub、Shodan、Censys 等
- **智能代理池管理**：熱池、溫池、冷池、黑名單分類
- **代理驗證與評分**：多維度代理品質評估
- **多協議支援**：HTTP、HTTPS、SOCKS4、SOCKS5
- **地理位置分類**：基於 IP 的地理位置識別
- **自建代理探測器**：IP 範圍掃描、端口檢測、協議識別
- **API 配置管理**：安全的 API 金鑰管理系統

## 📁 專案結構

```
JasonSpider/
├── src/                                 # 源代碼目錄
│   ├── html_to_markdown/                # HTML to Markdown 轉換模組
│   ├── proxy_manager/                   # 代理管理模組
│   │   ├── manager.py                   # 核心代理管理器
│   │   ├── pools.py                     # 代理池管理
│   │   ├── fetchers.py                  # 代理獲取器
│   │   ├── advanced_fetchers.py         # 高級代理獲取器
│   │   ├── validators.py                # 代理驗證器
│   │   ├── scanner.py                   # 代理掃描器
│   │   ├── models.py                    # 數據模型
│   │   ├── config.py                    # 配置管理
│   │   └── api_config_manager.py        # API 配置管理
│   └── main.py                          # 主應用程序
├── JasonSpider-Dev/                     # 開發環境（自建探測器）
├── Docs/                                # 專案文檔
├── config/                              # 配置文件
├── data/                                # 資料目錄
├── docker/                              # Docker 相關檔案
├── check_proxy_websites.py              # 代理網站檢查工具
├── test_core_functionality.py           # 核心功能測試
├── test_censys_integration.py           # Censys 整合測試
├── setup_api_config.py                  # API 配置工具
└── requirements.txt                     # 依賴清單
```

## 🛠️ 安裝與使用

### 環境需求

- Python 3.11+
- 推薦使用 `uv` 進行依賴管理

### 快速開始

```bash
# 1. 克隆專案
git clone <repository-url>
cd JasonSpider

# 2. 設置虛擬環境（推薦使用 uv）
uv venv
uv shell

# 3. 安裝依賴
uv sync
# 或使用 pip
pip install -r requirements.txt

# 4. 配置 API 金鑰（可選）
python setup_api_config.py interactive

# 5. 測試核心功能
python test_core_functionality.py

# 6. 啟動服務（見下方速查指令與網址）
```

> 提示：現在只需在專案根目錄執行 `uv sync`，即可快速、可靠地建立與專案定義完全一致的 Python 開發環境，無需再關心複雜的 pip 與 venv 指令。這大幅簡化入門流程並保證環境一致性。

### Docker 部署

```bash
# 使用 Docker Compose 啟動所有服務（新插件語法）
docker compose up -d --build

# 停止並清理
docker compose down
```

## 🧭 啟動與存取網址速查

> 下列指令均在專案根目錄執行，除前端須先進入 `frontend`。

### 前端（Vite 開發伺服器）

```bash
cd frontend
npm ci
npm run dev
# 開啟瀏覽器存取
# http://127.0.0.1:5173
```

### 主後端 API（Port 8000）

```bash
uv run python run_server.py
# 健康檢查
# http://127.0.0.1:8000/health
# API 文件（Swagger）
# http://127.0.0.1:8000/docs
```

### ETL API（Port 8001）

> 注意：ETL API 的文件與健康檢查路徑與主後端不同。

```bash
uv run uvicorn src.etl.etl_api:etl_app --host 0.0.0.0 --port 8001 --log-level info
# 文件（Swagger）
# http://127.0.0.1:8001/etl/docs
# 健康檢查
# http://127.0.0.1:8001/api/etl/health
```

### 以 Docker 啟動（可選）

```bash
docker compose up -d --build
# 主後端: http://127.0.0.1:8000
# ETL API: http://127.0.0.1:8001 (文件與健康檢查同上)
# Redis: 6379（容器內部網路）
```

## 📖 API 使用

### HTML to Markdown 轉換

```bash
# 基本轉換
curl -X POST "http://localhost:8000/convert" \
     -H "Content-Type: application/json" \
     -d '{"html": "<h1>Hello World</h1>", "engine": "markdownify"}'

# 批量轉換
curl -X POST "http://localhost:8000/batch-convert" \
     -H "Content-Type: application/json" \
     -d '{"items": [{"html": "<h1>Title 1</h1>"}, {"html": "<h2>Title 2</h2>"}]}'
```

### 代理網站檢查

```bash
# 檢查所有代理網站狀態
python check_proxy_websites.py
```

## 🔧 配置

### 環境變數

複製 `.env.example` 到 `.env` 並根據需要修改配置：

```bash
cp .env.example .env
```

主要配置項目：

- `HOST`: 服務器主機地址（預設：0.0.0.0）
- `PORT`: 服務器端口（預設：8000）
- `LOG_LEVEL`: 日誌級別（預設：INFO）

## 📚 文檔

### 核心文檔

- [API 參考文檔](API_REFERENCE.md) - 完整的 API 接口說明
- [部署指南](DEPLOYMENT_GUIDE.md) - 詳細的部署和維護指南
- [API 配置指南](API_CONFIGURATION.md) - API 金鑰配置說明
- [專案依賴包說明（2025-09-08）](Docs/專案依賴包說明_2025-09-08.md) - 依賴套件用途與分類

### 專案文檔

- [專案總覽](Docs/PROJECT_OVERVIEW.md) - 完整的專案功能與架構說明
- [架構設計總覽](Docs/架構設計總覽.md) - 系統架構設計文檔
- [第一階段工作規劃](Docs/第一階段未完成工作細部規劃與注意事項.md) - 開發計劃和進度

### 技術文檔

- [自建探測器開發總結](JasonSpider-Dev/DEVELOPMENT_SUMMARY.md) - 自建代理探測器實現
- [ProxyScraper 整合分析](Docs/ProxyScraper整合分析與建議.md) - ProxyScraper 整合方案
- [GitHub 專案分析](Docs/三個GitHub代理專案分析與整合建議.md) - GitHub 代理專案整合建議

## 🤝 貢獻

歡迎提交 Issue 和 Pull Request！

### 開發規範

- 遵循 PEP 8 代碼風格
- 使用 `ruff` 進行代碼格式化和檢查
- 所有函數和類別必須包含中文文檔字符串
- 使用類型提示（Type Hints）

### 提交規範

使用 Conventional Commits 格式：

```
<type>(<scope>): <subject>

例如：
feat(crawler): add free-proxy-list.net crawler module
fix(api): resolve markdown conversion encoding issue
```

## 📄 授權

本專案採用 MIT 授權條款。詳見 [LICENSE](LICENSE) 檔案。

## 🔗 相關連結

- [FastAPI 文檔](https://fastapi.tiangolo.com/)
- [aiohttp 文檔](https://docs.aiohttp.org/)
- [Beautiful Soup 文檔](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

---

**注意**: 本專案仍在積極開發中，部分功能可能尚未完全實現。請參考文檔了解最新進度。
