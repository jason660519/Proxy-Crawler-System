#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETL API æ“´å±•æ¨¡çµ„

æ­¤æ¨¡çµ„ç‚ºä»£ç†ç®¡ç†ç³»çµ±æä¾› ETL ç›¸é—œçš„ API ç«¯é»ï¼ŒåŒ…æ‹¬ï¼š
- ETL æµç¨‹ç®¡ç†
- æ•¸æ“šå“è³ªç›£æ§
- æ‰¹é‡æ•¸æ“šè™•ç†
- ç›£æ§å„€è¡¨æ¿ API
- æ•¸æ“šé©—è­‰å’Œå ±å‘Š

ä½œè€…: Assistant
å‰µå»ºæ™‚é–“: 2024
"""

import asyncio
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from uuid import uuid4

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Query, Body
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel, Field, validator
from loguru import logger

# å°å…¥ ETL ç›¸é—œæ¨¡çµ„
from .proxy_etl_pipeline import (
    ProxyETLPipeline, ETLConfig, ETLMetrics, ETLResult,
    ETLStage, DataSource
)
from .data_validator import (
    ProxyDataValidator, ValidationLevel, ValidationResult,
    ValidationReport, ValidationIssue
)
from .monitoring_dashboard import (
    MonitoringDashboard, SystemMetrics, AlertRule, Alert
)
from .database_schema import DatabaseSchemaManager
from ..proxy_manager.models import ProxyNode, ProxyFilter
from database_config import db_config

def get_db_config():
    """ç²å–æ•¸æ“šåº«é…ç½®"""
    return db_config


# ===== Pydantic æ¨¡å‹å®šç¾© =====

class ETLJobRequest(BaseModel):
    """ETL ä½œæ¥­è«‹æ±‚æ¨¡å‹"""
    job_name: str = Field(..., description="ä½œæ¥­åç¨±")
    data_source: str = Field(..., description="æ•¸æ“šä¾†æº")
    batch_size: int = Field(1000, ge=1, le=10000, description="æ‰¹æ¬¡å¤§å°")
    validation_level: str = Field("STANDARD", description="é©—è­‰ç´šåˆ¥")
    enable_monitoring: bool = Field(True, description="å•Ÿç”¨ç›£æ§")
    config_overrides: Optional[Dict[str, Any]] = Field(None, description="é…ç½®è¦†è“‹")
    
    @validator('validation_level')
    def validate_level(cls, v):
        valid_levels = ['BASIC', 'STANDARD', 'STRICT']
        if v.upper() not in valid_levels:
            raise ValueError(f"é©—è­‰ç´šåˆ¥å¿…é ˆæ˜¯ {valid_levels} ä¹‹ä¸€")
        return v.upper()


class ETLJobResponse(BaseModel):
    """ETL ä½œæ¥­å›æ‡‰æ¨¡å‹"""
    job_id: str
    job_name: str
    status: str
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    progress: float = 0.0
    metrics: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None


class ValidationRequest(BaseModel):
    """æ•¸æ“šé©—è­‰è«‹æ±‚æ¨¡å‹"""
    data_source: str = Field(..., description="æ•¸æ“šä¾†æº")
    validation_level: str = Field("STANDARD", description="é©—è­‰ç´šåˆ¥")
    sample_size: Optional[int] = Field(None, ge=1, le=10000, description="æ¨£æœ¬å¤§å°")
    include_details: bool = Field(True, description="åŒ…å«è©³ç´°ä¿¡æ¯")


class ValidationResponse(BaseModel):
    """æ•¸æ“šé©—è­‰å›æ‡‰æ¨¡å‹"""
    validation_id: str
    status: str
    overall_score: float
    total_records: int
    valid_records: int
    invalid_records: int
    issues: List[Dict[str, Any]]
    recommendations: List[str]
    created_at: datetime


class MonitoringMetricsResponse(BaseModel):
    """ç›£æ§æŒ‡æ¨™å›æ‡‰æ¨¡å‹"""
    timestamp: datetime
    system_metrics: Dict[str, Any]
    etl_metrics: Dict[str, Any]
    database_metrics: Dict[str, Any]
    alerts: List[Dict[str, Any]]


class DataQueryRequest(BaseModel):
    """æ•¸æ“šæŸ¥è©¢è«‹æ±‚æ¨¡å‹"""
    query_type: str = Field(..., description="æŸ¥è©¢é¡å‹")
    filters: Optional[Dict[str, Any]] = Field(None, description="ç¯©é¸æ¢ä»¶")
    date_range: Optional[Dict[str, str]] = Field(None, description="æ—¥æœŸç¯„åœ")
    aggregation: Optional[str] = Field(None, description="èšåˆæ–¹å¼")
    limit: int = Field(100, ge=1, le=10000, description="çµæœé™åˆ¶")
    offset: int = Field(0, ge=0, description="åç§»é‡")


class DataExportRequest(BaseModel):
    """æ•¸æ“šå°å‡ºè«‹æ±‚æ¨¡å‹"""
    export_type: str = Field(..., description="å°å‡ºé¡å‹")
    format: str = Field("json", description="å°å‡ºæ ¼å¼")
    filters: Optional[Dict[str, Any]] = Field(None, description="ç¯©é¸æ¢ä»¶")
    date_range: Optional[Dict[str, str]] = Field(None, description="æ—¥æœŸç¯„åœ")
    include_metadata: bool = Field(True, description="åŒ…å«å…ƒæ•¸æ“š")


# ===== ETL API é¡åˆ¥ =====

class ETLAPIManager:
    """ETL API ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ– ETL API ç®¡ç†å™¨"""
        self.etl_pipeline = None
        self.data_validator = None
        self.monitoring_dashboard = None
        self.schema_manager = None
        self.active_jobs: Dict[str, Dict[str, Any]] = {}
        self.job_history: List[Dict[str, Any]] = []
        
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶"""
        try:
            # åˆå§‹åŒ–æ•¸æ“šåº«é…ç½®
            db_config = get_db_config()
            
            # åˆå§‹åŒ– ETL ç®¡é“
            etl_config = ETLConfig(
                batch_size=1000,
                validation_level=ValidationLevel.STANDARD,
                enable_monitoring=True
            )
            self.etl_pipeline = ProxyETLPipeline(etl_config)
            
            # åˆå§‹åŒ–æ•¸æ“šé©—è­‰å™¨
            self.data_validator = ProxyDataValidator()
            
            # åˆå§‹åŒ–ç›£æ§å„€è¡¨æ¿
            self.monitoring_dashboard = MonitoringDashboard()
            await self.monitoring_dashboard.initialize()
            
            # åˆå§‹åŒ–æ•¸æ“šåº«æ¶æ§‹ç®¡ç†å™¨
            self.schema_manager = DatabaseSchemaManager(db_config)
            
            logger.info("âœ… ETL API ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ETL API ç®¡ç†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def create_etl_job(self, request: ETLJobRequest) -> str:
        """å‰µå»º ETL ä½œæ¥­"""
        job_id = str(uuid4())
        
        job_info = {
            "job_id": job_id,
            "job_name": request.job_name,
            "data_source": request.data_source,
            "status": "PENDING",
            "created_at": datetime.now(),
            "config": {
                "batch_size": request.batch_size,
                "validation_level": request.validation_level,
                "enable_monitoring": request.enable_monitoring,
                "config_overrides": request.config_overrides or {}
            },
            "progress": 0.0,
            "metrics": None,
            "error_message": None
        }
        
        self.active_jobs[job_id] = job_info
        logger.info(f"ğŸ“‹ å‰µå»º ETL ä½œæ¥­: {job_id} - {request.job_name}")
        
        return job_id
    
    async def start_etl_job(self, job_id: str) -> bool:
        """å•Ÿå‹• ETL ä½œæ¥­"""
        if job_id not in self.active_jobs:
            raise ValueError(f"ä½œæ¥­ {job_id} ä¸å­˜åœ¨")
        
        job_info = self.active_jobs[job_id]
        
        try:
            job_info["status"] = "RUNNING"
            job_info["started_at"] = datetime.now()
            
            # é…ç½® ETL ç®¡é“
            config = ETLConfig(
                batch_size=job_info["config"]["batch_size"],
                validation_level=ValidationLevel[job_info["config"]["validation_level"]],
                enable_monitoring=job_info["config"]["enable_monitoring"]
            )
            
            # åŸ·è¡Œ ETL æµç¨‹
            result = await self.etl_pipeline.process_data(
                data_source=DataSource[job_info["data_source"].upper()],
                config=config
            )
            
            # æ›´æ–°ä½œæ¥­ç‹€æ…‹
            job_info["status"] = "COMPLETED" if result.success else "FAILED"
            job_info["completed_at"] = datetime.now()
            job_info["progress"] = 100.0
            job_info["metrics"] = result.metrics.to_dict() if result.metrics else None
            
            if not result.success:
                job_info["error_message"] = result.error_message
            
            # ç§»å‹•åˆ°æ­·å²è¨˜éŒ„
            self.job_history.append(job_info.copy())
            del self.active_jobs[job_id]
            
            logger.info(f"âœ… ETL ä½œæ¥­å®Œæˆ: {job_id}")
            return True
            
        except Exception as e:
            job_info["status"] = "FAILED"
            job_info["error_message"] = str(e)
            job_info["completed_at"] = datetime.now()
            
            logger.error(f"âŒ ETL ä½œæ¥­å¤±æ•—: {job_id} - {e}")
            return False
    
    async def validate_data(self, request: ValidationRequest) -> ValidationReport:
        """åŸ·è¡Œæ•¸æ“šé©—è­‰"""
        try:
            # æ¨¡æ“¬ç²å–æ•¸æ“šé€²è¡Œé©—è­‰
            # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒå¾æŒ‡å®šçš„æ•¸æ“šæºç²å–æ•¸æ“š
            sample_data = []  # é€™è£¡æ‡‰è©²å¾æ•¸æ“šæºç²å–å¯¦éš›æ•¸æ“š
            
            validation_level = ValidationLevel[request.validation_level]
            
            # åŸ·è¡Œé©—è­‰
            report = await self.data_validator.validate_batch(
                sample_data,
                validation_level
            )
            
            logger.info(f"ğŸ“Š æ•¸æ“šé©—è­‰å®Œæˆ: {request.data_source}")
            return report
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
            raise
    
    async def get_monitoring_metrics(self) -> Dict[str, Any]:
        """ç²å–ç›£æ§æŒ‡æ¨™"""
        try:
            metrics = await self.monitoring_dashboard.collect_metrics()
            alerts = await self.monitoring_dashboard.check_alerts()
            
            return {
                "timestamp": datetime.now(),
                "system_metrics": metrics.to_dict() if metrics else {},
                "etl_metrics": self._get_etl_metrics(),
                "database_metrics": await self._get_database_metrics(),
                "alerts": [alert.to_dict() for alert in alerts]
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ç›£æ§æŒ‡æ¨™å¤±æ•—: {e}")
            raise
    
    def _get_etl_metrics(self) -> Dict[str, Any]:
        """ç²å– ETL æŒ‡æ¨™"""
        active_jobs_count = len(self.active_jobs)
        completed_jobs_count = len([job for job in self.job_history if job["status"] == "COMPLETED"])
        failed_jobs_count = len([job for job in self.job_history if job["status"] == "FAILED"])
        
        return {
            "active_jobs": active_jobs_count,
            "completed_jobs": completed_jobs_count,
            "failed_jobs": failed_jobs_count,
            "success_rate": completed_jobs_count / max(completed_jobs_count + failed_jobs_count, 1) * 100
        }
    
    async def _get_database_metrics(self) -> Dict[str, Any]:
        """ç²å–æ•¸æ“šåº«æŒ‡æ¨™"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾å¯¦éš›çš„æ•¸æ“šåº«æŒ‡æ¨™æ”¶é›†
        return {
            "connection_pool_size": 10,
            "active_connections": 5,
            "query_performance": "good",
            "storage_usage": "75%"
        }


# ===== å…¨å±€å¯¦ä¾‹ =====
etl_api_manager = ETLAPIManager()


# ===== ä¾è³´æ³¨å…¥å‡½æ•¸ =====

async def get_etl_manager() -> ETLAPIManager:
    """ç²å– ETL API ç®¡ç†å™¨å¯¦ä¾‹"""
    if etl_api_manager.etl_pipeline is None:
        await etl_api_manager.initialize()
    return etl_api_manager


# ===== FastAPI æ‡‰ç”¨å¯¦ä¾‹ =====

etl_app = FastAPI(
    title="ä»£ç†ç®¡ç†ç³»çµ± ETL API",
    description="æä¾› ETL æµç¨‹ç®¡ç†ã€æ•¸æ“šé©—è­‰å’Œç›£æ§åŠŸèƒ½çš„ API",
    version="1.0.0",
    docs_url="/etl/docs",
    redoc_url="/etl/redoc"
)


# ===== API ç«¯é»å®šç¾© =====

@etl_app.post("/api/etl/jobs", response_model=ETLJobResponse, summary="å‰µå»º ETL ä½œæ¥­")
async def create_etl_job(
    request: ETLJobRequest,
    background_tasks: BackgroundTasks,
    manager: ETLAPIManager = Depends(get_etl_manager)
):
    """å‰µå»ºä¸¦å•Ÿå‹•æ–°çš„ ETL ä½œæ¥­"""
    try:
        job_id = await manager.create_etl_job(request)
        
        # åœ¨èƒŒæ™¯å•Ÿå‹•ä½œæ¥­
        background_tasks.add_task(manager.start_etl_job, job_id)
        
        job_info = manager.active_jobs[job_id]
        
        return ETLJobResponse(
            job_id=job_id,
            job_name=job_info["job_name"],
            status=job_info["status"],
            created_at=job_info["created_at"],
            progress=job_info["progress"]
        )
        
    except Exception as e:
        logger.error(f"âŒ å‰µå»º ETL ä½œæ¥­å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"å‰µå»ºä½œæ¥­å¤±æ•—: {e}")


@etl_app.get("/api/etl/jobs/{job_id}", response_model=ETLJobResponse, summary="ç²å– ETL ä½œæ¥­ç‹€æ…‹")
async def get_etl_job(
    job_id: str,
    manager: ETLAPIManager = Depends(get_etl_manager)
):
    """ç²å–æŒ‡å®š ETL ä½œæ¥­çš„ç‹€æ…‹å’Œé€²åº¦"""
    try:
        # æª¢æŸ¥æ´»èºä½œæ¥­
        if job_id in manager.active_jobs:
            job_info = manager.active_jobs[job_id]
        else:
            # æª¢æŸ¥æ­·å²è¨˜éŒ„
            job_info = next(
                (job for job in manager.job_history if job["job_id"] == job_id),
                None
            )
            
        if not job_info:
            raise HTTPException(status_code=404, detail="ä½œæ¥­ä¸å­˜åœ¨")
        
        return ETLJobResponse(
            job_id=job_info["job_id"],
            job_name=job_info["job_name"],
            status=job_info["status"],
            created_at=job_info["created_at"],
            started_at=job_info.get("started_at"),
            completed_at=job_info.get("completed_at"),
            progress=job_info["progress"],
            metrics=job_info["metrics"],
            error_message=job_info.get("error_message")
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ç²å–ä½œæ¥­ç‹€æ…‹å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–ä½œæ¥­ç‹€æ…‹å¤±æ•—: {e}")


@etl_app.get("/api/etl/jobs", response_model=List[ETLJobResponse], summary="ç²å–æ‰€æœ‰ ETL ä½œæ¥­")
async def list_etl_jobs(
    status: Optional[str] = Query(None, description="ç¯©é¸ç‹€æ…‹"),
    limit: int = Query(50, ge=1, le=200, description="çµæœé™åˆ¶"),
    manager: ETLAPIManager = Depends(get_etl_manager)
):
    """ç²å–æ‰€æœ‰ ETL ä½œæ¥­åˆ—è¡¨"""
    try:
        all_jobs = list(manager.active_jobs.values()) + manager.job_history
        
        # ç‹€æ…‹ç¯©é¸
        if status:
            all_jobs = [job for job in all_jobs if job["status"].upper() == status.upper()]
        
        # æŒ‰å‰µå»ºæ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        all_jobs.sort(key=lambda x: x["created_at"], reverse=True)
        
        # é™åˆ¶çµæœæ•¸é‡
        all_jobs = all_jobs[:limit]
        
        return [
            ETLJobResponse(
                job_id=job["job_id"],
                job_name=job["job_name"],
                status=job["status"],
                created_at=job["created_at"],
                started_at=job.get("started_at"),
                completed_at=job.get("completed_at"),
                progress=job["progress"],
                metrics=job["metrics"],
                error_message=job.get("error_message")
            )
            for job in all_jobs
        ]
        
    except Exception as e:
        logger.error(f"âŒ ç²å–ä½œæ¥­åˆ—è¡¨å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–ä½œæ¥­åˆ—è¡¨å¤±æ•—: {e}")


@etl_app.post("/api/etl/validate", response_model=ValidationResponse, summary="åŸ·è¡Œæ•¸æ“šé©—è­‰")
async def validate_data(
    request: ValidationRequest,
    manager: ETLAPIManager = Depends(get_etl_manager)
):
    """å°æŒ‡å®šæ•¸æ“šæºåŸ·è¡Œæ•¸æ“šå“è³ªé©—è­‰"""
    try:
        validation_id = str(uuid4())
        
        # åŸ·è¡Œé©—è­‰
        report = await manager.validate_data(request)
        
        return ValidationResponse(
            validation_id=validation_id,
            status="COMPLETED",
            overall_score=report.overall_score,
            total_records=report.total_records,
            valid_records=report.valid_records,
            invalid_records=report.invalid_records,
            issues=[issue.to_dict() for issue in report.issues],
            recommendations=report.recommendations,
            created_at=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æ•¸æ“šé©—è­‰å¤±æ•—: {e}")


@etl_app.get("/api/etl/monitoring/metrics", response_model=MonitoringMetricsResponse, summary="ç²å–ç›£æ§æŒ‡æ¨™")
async def get_monitoring_metrics(
    manager: ETLAPIManager = Depends(get_etl_manager)
):
    """ç²å–ç³»çµ±ç›£æ§æŒ‡æ¨™å’Œè­¦å ±ä¿¡æ¯"""
    try:
        metrics = await manager.get_monitoring_metrics()
        
        return MonitoringMetricsResponse(
            timestamp=metrics["timestamp"],
            system_metrics=metrics["system_metrics"],
            etl_metrics=metrics["etl_metrics"],
            database_metrics=metrics["database_metrics"],
            alerts=metrics["alerts"]
        )
        
    except Exception as e:
        logger.error(f"âŒ ç²å–ç›£æ§æŒ‡æ¨™å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–ç›£æ§æŒ‡æ¨™å¤±æ•—: {e}")


@etl_app.post("/api/etl/data/query", summary="æŸ¥è©¢æ•¸æ“š")
async def query_data(
    request: DataQueryRequest,
    manager: ETLAPIManager = Depends(get_etl_manager)
):
    """åŸ·è¡Œæ•¸æ“šæŸ¥è©¢æ“ä½œ"""
    try:
        # é€™è£¡æ‡‰è©²å¯¦ç¾å¯¦éš›çš„æ•¸æ“šæŸ¥è©¢é‚è¼¯
        # æ ¹æ“šæŸ¥è©¢é¡å‹å’Œç¯©é¸æ¢ä»¶å¾æ•¸æ“šåº«ç²å–æ•¸æ“š
        
        result = {
            "query_id": str(uuid4()),
            "query_type": request.query_type,
            "total_count": 0,
            "data": [],
            "metadata": {
                "filters_applied": request.filters or {},
                "date_range": request.date_range,
                "aggregation": request.aggregation,
                "limit": request.limit,
                "offset": request.offset
            },
            "execution_time_ms": 0,
            "timestamp": datetime.now().isoformat()
        }
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šæŸ¥è©¢å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æ•¸æ“šæŸ¥è©¢å¤±æ•—: {e}")


@etl_app.post("/api/etl/data/export", summary="å°å‡ºæ•¸æ“š")
async def export_data(
    request: DataExportRequest,
    background_tasks: BackgroundTasks,
    manager: ETLAPIManager = Depends(get_etl_manager)
):
    """å°å‡ºæ•¸æ“šåˆ°æ–‡ä»¶"""
    try:
        export_id = str(uuid4())
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"export_{request.export_type}_{timestamp}.{request.format}"
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œå°å‡ºä»»å‹™
        # background_tasks.add_task(perform_data_export, request, filename)
        
        return {
            "export_id": export_id,
            "filename": filename,
            "status": "PROCESSING",
            "download_url": f"/api/etl/data/download/{filename}",
            "estimated_completion": (datetime.now() + timedelta(minutes=5)).isoformat(),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šå°å‡ºå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æ•¸æ“šå°å‡ºå¤±æ•—: {e}")


@etl_app.get("/api/etl/data/download/{filename}", summary="ä¸‹è¼‰å°å‡ºæ–‡ä»¶")
async def download_export_file(filename: str):
    """ä¸‹è¼‰å°å‡ºçš„æ•¸æ“šæ–‡ä»¶"""
    try:
        file_path = Path("data/exports") / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        return FileResponse(
            path=str(file_path),
            filename=filename,
            media_type='application/octet-stream'
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶ä¸‹è¼‰å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸‹è¼‰å¤±æ•—: {e}")


@etl_app.get("/api/etl/health", summary="å¥åº·æª¢æŸ¥")
async def health_check(
    manager: ETLAPIManager = Depends(get_etl_manager)
):
    """ETL ç³»çµ±å¥åº·æª¢æŸ¥"""
    try:
        health_status = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "components": {
                "etl_pipeline": "operational" if manager.etl_pipeline else "unavailable",
                "data_validator": "operational" if manager.data_validator else "unavailable",
                "monitoring_dashboard": "operational" if manager.monitoring_dashboard else "unavailable",
                "schema_manager": "operational" if manager.schema_manager else "unavailable"
            },
            "active_jobs": len(manager.active_jobs),
            "total_jobs_processed": len(manager.job_history)
        }
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•çµ„ä»¶ä¸å¯ç”¨
        if any(status == "unavailable" for status in health_status["components"].values()):
            health_status["status"] = "degraded"
        
        return health_status
        
    except Exception as e:
        logger.error(f"âŒ å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


# ===== éŒ¯èª¤è™•ç† =====

@etl_app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """HTTP ç•°å¸¸è™•ç†å™¨"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "timestamp": datetime.now().isoformat(),
            "path": str(request.url)
        }
    )


@etl_app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """é€šç”¨ç•°å¸¸è™•ç†å™¨"""
    logger.error(f"âŒ æœªè™•ç†çš„ç•°å¸¸: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "å…§éƒ¨æœå‹™å™¨éŒ¯èª¤",
            "timestamp": datetime.now().isoformat(),
            "path": str(request.url)
        }
    )


if __name__ == "__main__":
    import uvicorn
    
    # å•Ÿå‹• ETL API æœå‹™å™¨
    uvicorn.run(
        "src.etl.etl_api:etl_app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )